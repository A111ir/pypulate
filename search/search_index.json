{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pypulate Documentation","text":"<p>Pypulate framework</p> <p>High performance financial and business analytics framework</p> <p> </p> <p>Welcome to Pypulate, a high-performance Python framework for financial analysis and business metrics. Pypulate offers powerful classes designed to handle different aspects of financial and business analytics (more to come):</p>"},{"location":"#core-components","title":"Core Components","text":""},{"location":"#parray-pypulate-array","title":"Parray (Pypulate Array)","text":"<p>A specialized array class for financial time series analysis with built-in technical analysis capabilities, preprocessing functions, and method chaining:</p> <pre><code>from pypulate import Parray\nimport numpy as np\n\n# Create a price array\nprices = Parray([10, 11, 12, 11, 10, 9, 10, 11, 12, 13, 15, 11, 8, 10, 14, 16])\n\n# Technical Analysis\nsma = prices.sma(5)                   \nrsi = prices.rsi(14)                  \n\n# Data Preprocessing\nnormalized = prices.normalize()        # L2 normalization  \nstandardized = prices.standardize()    # Z-score standardization\ncleaned = prices.remove_outliers()     # Remove outliers\ntransformed = prices.log_transform()   # Log transformation\n\n# Signal Detection with Method Chaining\nchain = (prices\n    .fill_missing(method='forward')    # Fill missing values\n    .standardize()                     # Standardize data\n    .sma(5)                            # Calculate 5-period SMA\n)\n\n# Volatility Analysis\nvolatility = prices.historical_volatility(7)\n</code></pre>"},{"location":"#kpi-key-performance-indicators","title":"KPI (Key Performance Indicators)","text":"<p>A comprehensive class for calculating and tracking business metrics:</p> <pre><code>from pypulate import KPI\n\nkpi = KPI()\n\n# Customer Metrics\nchurn = kpi.churn_rate(customers_start=1000, customers_end=950, new_customers=50)\nretention = kpi.retention_rate(customers_start=1000, customers_end=950, new_customers=50)\n\n# Financial Metrics\nclv = kpi.customer_lifetime_value(\n    avg_revenue_per_customer=100,\n    gross_margin=70,\n    churn_rate_value=5\n)\n\n# Health Assessment\nhealth = kpi.health  # Returns overall business health score and component analysis\n</code></pre>"},{"location":"#portfolio","title":"Portfolio","text":"<p>A class for portfolio analysis and risk management:</p> <pre><code>from pypulate import Portfolio\nimport numpy as np\n\nportfolio = Portfolio()\n\n# Sample data\nstart = [100, 102, 105, 103, 107, 108, 107, 110, 112, 111]\nend = [110, 95, 111, 103, 130, 89, 99, 104, 102, 100]\n\ncash_flows = [0, -1000, 0, 500, 0, -2000, 0, 1000, 0, 0]\nrisk_free_rate = 0.02\n\n# Return Metrics\nreturns = portfolio.simple_return(end, start)\nlog_ret = portfolio.log_return(end, start)\n\n# Risk Metrics\nsharpe = portfolio.sharpe_ratio(returns, risk_free_rate)\nvar = portfolio.value_at_risk(returns, confidence_level=0.95)\ndd = portfolio.drawdown(returns)\n\n# Portfolio Health\nhealth = portfolio.health  # Returns portfolio health analysis\n</code></pre>"},{"location":"#allocation","title":"Allocation","text":"<p>A comprehensive class for portfolio optimization and asset allocation:</p> <pre><code>from pypulate import Allocation\nimport numpy as np\n\nallocation = Allocation()\n\n# Sample returns data (252 days, 5 assets)\nreturns = np.random.normal(0.0001, 0.02, (252, 5))\nrisk_free_rate = 0.04\n\n# Mean-Variance Optimization\nweights, ret, risk = allocation.mean_variance(returns, risk_free_rate=risk_free_rate)\n\n# Risk Parity Portfolio\nweights, ret, risk = allocation.risk_parity(returns)\n\n# Kelly Criterion (with half-Kelly for conservative sizing)\nweights, ret, risk = allocation.kelly_criterion(returns, kelly_fraction=0.5)\n\n# Black-Litterman with views\nviews = {0: 0.15, 1: 0.12}  # Views on first two assets\nview_confidences = {0: 0.8, 1: 0.7}\nmarket_caps = np.array([1000, 800, 600, 400, 200])\nweights, ret, risk = allocation.black_litterman(\n    returns, market_caps, views, view_confidences\n)\n\n# Hierarchical Risk Parity\nweights, ret, risk = allocation.hierarchical_risk_parity(returns)\n</code></pre>"},{"location":"#servicepricing","title":"ServicePricing","text":"<p>A unified interface for calculating and managing various service pricing models:</p> <pre><code>from pypulate import ServicePricing\n\npricing = ServicePricing()\n\n# Tiered Pricing\nprice = pricing.calculate_tiered_price(\n    usage_units=1500,\n    tiers={\n        \"0-1000\": 0.10,    # $0.10 per unit for first 1000 units\n        \"1001-2000\": 0.08, # $0.08 per unit for next 500 units\n        \"2001+\": 0.05      # $0.05 per unit for remaining units\n    }\n)\n\n# Subscription with Features\nsub_price = pricing.calculate_subscription_price(\n    base_price=99.99,\n    features=['premium', 'api_access'],\n    feature_prices={'premium': 49.99, 'api_access': 29.99},\n    duration_months=12,\n    discount_rate=0.10\n)\n\n# Dynamic Pricing\ndynamic_price = pricing.apply_dynamic_pricing(\n    base_price=100.0,\n    demand_factor=1.2,\n    competition_factor=0.9,\n    seasonality_factor=1.1\n)\n\n# Track Pricing History\npricing.save_current_pricing()\nhistory = pricing.get_pricing_history()\n</code></pre>"},{"location":"#creditscoring","title":"CreditScoring","text":"<p>A comprehensive class for credit risk assessment, scoring, and loan analysis:</p> <pre><code>from pypulate.dtypes import CreditScoring\n\ncredit = CreditScoring()\n\n# Corporate Bankruptcy Risk Assessment\nz_score = credit.altman_z_score(\n    working_capital=1200000,\n    retained_earnings=1500000,\n    ebit=800000,\n    market_value_equity=5000000,\n    sales=4500000,\n    total_assets=6000000,\n    total_liabilities=2500000\n)\n\n# Default Probability Using Structural Model\npd_result = credit.merton_model(\n    asset_value=10000000,\n    debt_face_value=5000000,\n    asset_volatility=0.25,\n    risk_free_rate=0.03,\n    time_to_maturity=1.0\n)\n\n# Retail Credit Scoring\nscorecard_result = credit.create_scorecard(\n    features={\"age\": 35, \"income\": 75000, \"years_employed\": 5},\n    weights={\"age\": 2.5, \"income\": 3.2, \"years_employed\": 4.0},\n    scaling_factor=20,\n    base_score=600\n)\n\n# Financial Ratio Analysis\nratios = credit.financial_ratios(\n    current_assets=2000000,\n    current_liabilities=1200000,\n    total_assets=8000000,\n    total_liabilities=4000000,\n    ebit=1200000,\n    interest_expense=300000,\n    net_income=700000,\n    total_equity=4000000,\n    sales=6000000\n)\n\n# Risk-Based Loan Pricing\npricing = credit.loan_pricing(\n    loan_amount=250000,\n    term=5,\n    pd=0.03,\n    lgd=0.35,\n    funding_cost=0.04,\n    operating_cost=0.01,\n    capital_requirement=0.08,\n    target_roe=0.15\n)\n\n# Expected Credit Loss Calculation\necl = credit.expected_credit_loss(\n    pd=0.05,\n    lgd=0.4,\n    ead=100000\n)\n\n# Model Usage History\nhistory = credit.get_history()\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pypulate\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Parray: </li> <li>Technical indicators (30+ implementations)</li> <li>Signal detection and pattern recognition</li> <li>Time series transformations</li> <li> <p>Built-in filtering methods</p> </li> <li> <p>KPI:</p> </li> <li>Customer metrics (churn, retention, LTV)</li> <li>Financial metrics (ROI, CAC, ARR)</li> <li>Engagement metrics (NPS, CSAT)</li> <li> <p>Health scoring system</p> </li> <li> <p>Portfolio:</p> </li> <li>Return calculations</li> <li>Risk metrics</li> <li>Performance attribution</li> <li> <p>Health assessment</p> </li> <li> <p>Allocation:</p> </li> <li>Portfolio optimization</li> <li>Asset allocation</li> <li> <p>Risk management</p> </li> <li> <p>ServicePricing:</p> </li> <li>Tiered pricing models</li> <li>Subscription pricing with features</li> <li>Usage-based pricing</li> <li>Dynamic pricing adjustments</li> <li>Volume discounts</li> <li>Custom pricing rules</li> <li> <p>Pricing history tracking</p> </li> <li> <p>CreditScoring:</p> </li> <li>Bankruptcy prediction models</li> <li>Default probability estimation</li> <li>Credit scorecard development</li> <li>Financial ratio analysis</li> <li>Expected credit loss calculation</li> <li>Risk-based loan pricing</li> <li>Credit model validation</li> <li>Loss given default estimation</li> <li>Exposure at default calculation</li> </ul>"},{"location":"#user-guide","title":"User Guide","text":"<ul> <li>Getting Started</li> <li>Parray Guide</li> <li>KPI Guide</li> <li>Portfolio Guide</li> <li>Service Pricing Guide</li> <li>Credit Scoring Guide</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please feel free to submit a Pull Request.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#030","title":"[0.3.0]","text":"<ul> <li>Added feature rich preprocessing methods for Parray.</li> <li>Added feature rich statistics methods for Parray</li> <li>Added complete test of methods.</li> <li>Improved docs.</li> </ul>"},{"location":"changelog/#022","title":"[0.2.2]","text":"<ul> <li>Various credit and loan models added.</li> </ul>"},{"location":"changelog/#021","title":"[0.2.1]","text":"<ul> <li>Added more pricing models.</li> <li>Minor improvements in code and docs.</li> </ul>"},{"location":"changelog/#020","title":"[0.2.0]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>New Class for Allocation portfolio</li> <li>mean_variance_optimization,</li> <li>minimum_variance_portfolio,</li> <li>maximum_sharpe_ratio,</li> <li>risk_parity_portfolio,</li> <li>maximum_diversification_portfolio,</li> <li>equal_weight_portfolio,</li> <li>market_cap_weight_portfolio,</li> <li>hierarchical_risk_parity,</li> <li>black_litterman,</li> <li>kelly_criterion_optimization</li> </ul>"},{"location":"changelog/#010-2025-03-04","title":"[0.1.0] - 2025-03-04","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Initial release </li> </ul>"},{"location":"contributing/","title":"Contributing to Pypulate","text":"<p>Thank you for considering contributing to Pypulate! This document provides guidelines and instructions for contributing to the project.</p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be respectful and considerate of others when contributing to this project. We aim to foster an inclusive and welcoming community.</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":"<p>There are many ways to contribute to Pypulate:</p> <ol> <li> <p>Report bugs: If you find a bug, please create an issue on GitHub with a detailed description of the problem, including steps to reproduce it.</p> </li> <li> <p>Suggest features: If you have an idea for a new feature or improvement, please create an issue on GitHub to discuss it.</p> </li> <li> <p>Contribute code: If you want to contribute code, please follow the steps below.</p> </li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Fork the repository on GitHub.</p> </li> <li> <p>Clone your fork locally:    <pre><code>git clone https://github.com/yourusername/pypulate.git\ncd pypulate\n</code></pre></p> </li> <li> <p>Create a virtual environment and install development dependencies:    <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows, use: .venv\\Scripts\\activate\npip install -e \".[dev]\"\n</code></pre></p> </li> <li> <p>Create a branch for your changes:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> </ol>"},{"location":"contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We follow the PEP 8 style guide for Python code. We use the following tools to enforce code style:</p> <ul> <li>Black: For code formatting</li> <li>isort: For import sorting</li> <li>flake8: For linting</li> </ul> <p>You can run these tools with: <pre><code>black src tests\nisort src tests\nflake8 src tests\n</code></pre></p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>All functions, classes, and modules should have docstrings following the NumPy docstring format.</li> <li>Update the documentation when adding or modifying features.</li> <li>Run the documentation locally to check your changes:   <pre><code>mkdocs serve\n</code></pre></li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for all new features and bug fixes.</li> <li>Make sure all tests pass before submitting a pull request:   <pre><code>pytest\n</code></pre></li> </ul>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Update the documentation with details of changes to the interface, if applicable.</li> <li>Update the tests to cover your changes.</li> <li>Make sure all tests pass.</li> <li>Submit a pull request to the <code>main</code> branch.</li> <li>The pull request will be reviewed by maintainers, who may request changes or improvements.</li> <li>Once approved, your pull request will be merged.</li> </ol>"},{"location":"contributing/#adding-new-kpis-or-moving-averages","title":"Adding New KPIs or Moving Averages","text":"<p>If you want to add a new KPI or moving average function:</p> <ol> <li>Add the function to the appropriate module (<code>kpi/business_kpi.py</code> for KPIs, <code>moving_averages/movingaverages.py</code> for moving averages).</li> <li>Write comprehensive docstrings with parameters, return values, and examples.</li> <li>Add tests for the new function.</li> <li>Update the documentation to include the new function.</li> <li>Add the function to the appropriate <code>__init__.py</code> file to expose it.</li> </ol>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing to Pypulate, you agree that your contributions will be licensed under the project's MIT License. </p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Pypulate requires:</p> <ul> <li>Python 3.7 or higher</li> <li>NumPy 1.20.0 or higher</li> </ul>"},{"location":"installation/#installing-from-pypi","title":"Installing from PyPI","text":"<p>The easiest way to install Pypulate is using pip:</p> <pre><code>pip install pypulate\n</code></pre> <p>This will install Pypulate and all its dependencies.</p>"},{"location":"installation/#installing-from-source","title":"Installing from Source","text":"<p>If you want to install the latest development version, you can install directly from the GitHub repository:</p> <pre><code>pip install git+https://github.com/yourusername/pypulate.git\n</code></pre>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>For development purposes, you can clone the repository and install in development mode:</p> <pre><code>git clone https://github.com/yourusername/pypulate.git\ncd pypulate\npip install -e .\n</code></pre> <p>This will install the package in development mode, allowing you to modify the code and see the changes immediately without reinstalling.</p>"},{"location":"installation/#verifying-installation","title":"Verifying Installation","text":"<p>You can verify that Pypulate is installed correctly by importing it in Python:</p> <pre><code>import pypulate\nprint(pypulate.__version__)\n</code></pre> <p>This should print the version number of Pypulate. </p>"},{"location":"license/","title":"License","text":"<p>Pypulate is released under the MIT License, which is a permissive open-source license that allows for free use, modification, and distribution of the software.</p>"},{"location":"license/#mit-license","title":"MIT License","text":"<pre><code>MIT License\n\nCopyright (c) 2025 Amir Rezaei\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"license/#what-this-means","title":"What This Means","text":"<p>The MIT License is one of the most permissive open-source licenses available. It allows you to:</p> <ul> <li>Use the code for commercial purposes</li> <li>Modify the code</li> <li>Distribute the code</li> <li>Use the code in private projects</li> <li>Sublicense the code</li> </ul> <p>The only requirement is that you include the original copyright notice and license text in any copy of the software/source.</p>"},{"location":"license/#third-party-libraries","title":"Third-Party Libraries","text":"<p>Pypulate depends on several third-party libraries, each with their own licenses:</p> <ul> <li>NumPy: BSD 3-Clause License</li> <li>MkDocs (for documentation): BSD 3-Clause License</li> <li>MkDocs Material Theme: MIT License</li> </ul> <p>Please refer to each library's documentation for more details on their licenses. </p>"},{"location":"api/allocation/allocation/","title":"Allocation API Reference","text":"<p>Portfolio Allocation Module</p> <p>This module provides a class for portfolio allocation optimization, including various methods like Mean-Variance Optimization, Minimum Variance, Maximum Sharpe Ratio, Hierarchical Risk Parity, Black-Litterman, Kelly Criterion, and other common portfolio optimization techniques.</p>"},{"location":"api/allocation/allocation/#pypulate.dtypes.allocation.Allocation","title":"<code>Allocation</code>","text":"<p>A class for portfolio allocation optimization.</p> <p>This class provides methods for various portfolio optimization techniques including Mean-Variance Optimization, Minimum Variance Portfolio, Maximum Sharpe Ratio, and other common portfolio optimization methods.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pypulate.dtypes import Allocation\n&gt;&gt;&gt; allocation = Allocation()\n&gt;&gt;&gt; weights, ret, risk = allocation.mean_variance(returns_matrix)\n&gt;&gt;&gt; weights = allocation.equal_weight(returns_matrix)\n</code></pre>"},{"location":"api/allocation/allocation/#pypulate.dtypes.allocation.Allocation.method","title":"<code>method</code>  <code>property</code>","text":"<p>Get the current optimization method used.</p>"},{"location":"api/allocation/allocation/#pypulate.dtypes.allocation.Allocation.portfolio_return","title":"<code>portfolio_return</code>  <code>property</code>","text":"<p>Get the current portfolio return.</p>"},{"location":"api/allocation/allocation/#pypulate.dtypes.allocation.Allocation.portfolio_risk","title":"<code>portfolio_risk</code>  <code>property</code>","text":"<p>Get the current portfolio risk.</p>"},{"location":"api/allocation/allocation/#pypulate.dtypes.allocation.Allocation.weights","title":"<code>weights</code>  <code>property</code>","text":"<p>Get the current portfolio weights.</p>"},{"location":"api/allocation/allocation/#pypulate.dtypes.allocation.Allocation.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Allocation class with empty state.</p>"},{"location":"api/allocation/allocation/#pypulate.dtypes.allocation.Allocation.black_litterman","title":"<code>black_litterman(returns, market_caps, views, view_confidences, tau=0.05, risk_free_rate=0.0)</code>","text":"<p>Implement Black-Litterman portfolio optimization.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>ndarray</code> <p>Matrix of asset returns where each column represents an asset</p> required <code>market_caps</code> <code>ndarray</code> <p>Array of market capitalizations for each asset</p> required <code>views</code> <code>dict</code> <p>Dictionary mapping asset indices to expected returns</p> required <code>view_confidences</code> <code>dict</code> <p>Dictionary mapping asset indices to confidence levels (0-1)</p> required <code>tau</code> <code>float</code> <p>Uncertainty in the prior distribution</p> <code>0.05</code> <code>risk_free_rate</code> <code>float</code> <p>Risk-free rate</p> <code>0.0</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(optimal_weights, portfolio_return, portfolio_risk)</p>"},{"location":"api/allocation/allocation/#pypulate.dtypes.allocation.Allocation.equal_weight","title":"<code>equal_weight(returns)</code>","text":"<p>Create an equal-weighted portfolio.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>ndarray</code> <p>Matrix of asset returns where each column represents an asset</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(weights, portfolio_return, portfolio_risk)</p>"},{"location":"api/allocation/allocation/#pypulate.dtypes.allocation.Allocation.get_portfolio_metrics","title":"<code>get_portfolio_metrics()</code>","text":"<p>Get the current portfolio metrics.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing: - weights: Current portfolio weights - portfolio_return: Current portfolio return - portfolio_risk: Current portfolio risk - method: Current optimization method - constraints: Current optimization constraints</p>"},{"location":"api/allocation/allocation/#pypulate.dtypes.allocation.Allocation.hierarchical_risk_parity","title":"<code>hierarchical_risk_parity(returns, linkage_method='single', distance_metric='euclidean')</code>","text":"<p>Implement Hierarchical Risk Parity (HRP) portfolio optimization.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>ndarray</code> <p>Matrix of asset returns where each column represents an asset</p> required <code>linkage_method</code> <code>str</code> <p>Linkage method for hierarchical clustering ('single', 'complete', 'average', 'ward')</p> <code>'single'</code> <code>distance_metric</code> <code>str</code> <p>Distance metric for clustering ('euclidean', 'correlation')</p> <code>'euclidean'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(optimal_weights, portfolio_return, portfolio_risk)</p>"},{"location":"api/allocation/allocation/#pypulate.dtypes.allocation.Allocation.kelly_criterion","title":"<code>kelly_criterion(returns, risk_free_rate=0.0, constraints=None, kelly_fraction=1.0)</code>","text":"<p>Implement Kelly Criterion portfolio optimization.</p> <p>The Kelly Criterion determines the optimal fraction of capital to allocate to each investment to maximize long-term growth rate.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>ndarray</code> <p>Matrix of asset returns where each column represents an asset</p> required <code>risk_free_rate</code> <code>float</code> <p>Risk-free rate</p> <code>0.0</code> <code>constraints</code> <code>list of dict</code> <p>List of constraints for the optimization problem</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(optimal_weights, portfolio_return, portfolio_risk)</p> Notes <p>The Kelly Criterion maximizes the expected logarithmic growth rate of wealth. The full Kelly Criterion can be aggressive, so practitioners often use a fraction of the Kelly Criterion (e.g., half-Kelly) for more conservative position sizing.</p>"},{"location":"api/allocation/allocation/#pypulate.dtypes.allocation.Allocation.market_cap_weight","title":"<code>market_cap_weight(market_caps)</code>","text":"<p>Create a market-cap weighted portfolio.</p> <p>Parameters:</p> Name Type Description Default <code>market_caps</code> <code>ndarray</code> <p>Array of market capitalizations for each asset</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Market-cap weighted portfolio weights</p>"},{"location":"api/allocation/allocation/#pypulate.dtypes.allocation.Allocation.maximum_diversification","title":"<code>maximum_diversification(returns, constraints=None)</code>","text":"<p>Find the portfolio that maximizes diversification ratio.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>ndarray</code> <p>Matrix of asset returns where each column represents an asset</p> required <code>constraints</code> <code>list of dict</code> <p>List of constraints for the optimization problem</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(optimal_weights, portfolio_return, portfolio_risk)</p>"},{"location":"api/allocation/allocation/#pypulate.dtypes.allocation.Allocation.maximum_sharpe","title":"<code>maximum_sharpe(returns, risk_free_rate=0.0, constraints=None)</code>","text":"<p>Find the portfolio with maximum Sharpe ratio.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>ndarray</code> <p>Matrix of asset returns where each column represents an asset</p> required <code>risk_free_rate</code> <code>float</code> <p>Risk-free rate for Sharpe ratio calculation</p> <code>0.0</code> <code>constraints</code> <code>list of dict</code> <p>List of constraints for the optimization problem</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(optimal_weights, portfolio_return, portfolio_risk)</p>"},{"location":"api/allocation/allocation/#pypulate.dtypes.allocation.Allocation.mean_variance","title":"<code>mean_variance(returns, target_return=None, risk_free_rate=0.0, constraints=None)</code>","text":"<p>Perform Mean-Variance Optimization to find optimal portfolio weights.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>ndarray</code> <p>Matrix of asset returns where each column represents an asset</p> required <code>target_return</code> <code>float</code> <p>Target portfolio return. If None, maximizes Sharpe ratio</p> <code>None</code> <code>risk_free_rate</code> <code>float</code> <p>Risk-free rate for Sharpe ratio calculation</p> <code>0.0</code> <code>constraints</code> <code>list of dict</code> <p>List of constraints for the optimization problem</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(optimal_weights, portfolio_return, portfolio_risk)</p>"},{"location":"api/allocation/allocation/#pypulate.dtypes.allocation.Allocation.minimum_variance","title":"<code>minimum_variance(returns, constraints=None)</code>","text":"<p>Find the portfolio with minimum variance.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>ndarray</code> <p>Matrix of asset returns where each column represents an asset</p> required <code>constraints</code> <code>list of dict</code> <p>List of constraints for the optimization problem</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(optimal_weights, portfolio_return, portfolio_risk)</p>"},{"location":"api/allocation/allocation/#pypulate.dtypes.allocation.Allocation.risk_parity","title":"<code>risk_parity(returns, constraints=None)</code>","text":"<p>Find the portfolio where risk is equally distributed across assets.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>ndarray</code> <p>Matrix of asset returns where each column represents an asset</p> required <code>constraints</code> <code>list of dict</code> <p>List of constraints for the optimization problem</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(optimal_weights, portfolio_return, portfolio_risk)</p>"},{"location":"api/allocation/optimization/","title":"Optimizations Reference","text":"<p>Portfolio Optimization Module</p> <p>This module provides various portfolio optimization methods including Mean-Variance Optimization, Minimum Variance Portfolio, Maximum Sharpe Ratio, Hierarchical Risk Parity, Black-Litterman, Kelly Criterion, and other common portfolio optimization techniques.</p>"},{"location":"api/allocation/optimization/#pypulate.allocation.optimization.black_litterman","title":"<code>black_litterman(returns, market_caps, views, view_confidences, tau=0.05, risk_free_rate=0.0)</code>","text":"<p>Implement Black-Litterman portfolio optimization.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Matrix of asset returns where each column represents an asset</p> required <code>market_caps</code> <code>array - like</code> <p>Array of market capitalizations for each asset</p> required <code>views</code> <code>dict</code> <p>Dictionary mapping asset indices to expected returns</p> required <code>view_confidences</code> <code>dict</code> <p>Dictionary mapping asset indices to confidence levels (0-1)</p> required <code>tau</code> <code>float</code> <p>Uncertainty in the prior distribution</p> <code>0.05</code> <code>risk_free_rate</code> <code>float</code> <p>Risk-free rate</p> <code>0.0</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(optimal_weights, portfolio_return, portfolio_risk)</p>"},{"location":"api/allocation/optimization/#pypulate.allocation.optimization.equal_weight_portfolio","title":"<code>equal_weight_portfolio(returns)</code>","text":"<p>Create an equal-weighted portfolio.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Matrix of asset returns where each column represents an asset</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(weights, portfolio_return, portfolio_risk)</p>"},{"location":"api/allocation/optimization/#pypulate.allocation.optimization.hierarchical_risk_parity","title":"<code>hierarchical_risk_parity(returns, linkage_method='single', distance_metric='euclidean')</code>","text":"<p>Implement Hierarchical Risk Parity (HRP) portfolio optimization.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Matrix of asset returns where each column represents an asset</p> required <code>linkage_method</code> <code>str</code> <p>Linkage method for hierarchical clustering ('single', 'complete', 'average', 'ward')</p> <code>'single'</code> <code>distance_metric</code> <code>str</code> <p>Distance metric for clustering ('euclidean', 'correlation')</p> <code>'euclidean'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(optimal_weights, portfolio_return, portfolio_risk)</p>"},{"location":"api/allocation/optimization/#pypulate.allocation.optimization.kelly_criterion_optimization","title":"<code>kelly_criterion_optimization(returns, risk_free_rate=0.0, constraints=None, kelly_fraction=1.0)</code>","text":"<p>Implement Kelly Criterion portfolio optimization.</p> <p>The Kelly Criterion determines the optimal fraction of capital to allocate to each investment to maximize long-term growth rate.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Matrix of asset returns where each column represents an asset</p> required <code>risk_free_rate</code> <code>float</code> <p>Risk-free rate</p> <code>0.0</code> <code>constraints</code> <code>list of dict</code> <p>List of constraints for the optimization problem</p> <code>None</code> <code>kelly_fraction</code> <code>float</code> <p>Fraction of Kelly Criterion to use (e.g., 0.5 for half-Kelly)</p> <code>1.0</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(optimal_weights, portfolio_return, portfolio_risk)</p> Notes <p>The Kelly Criterion maximizes the expected logarithmic growth rate of wealth. The full Kelly Criterion can be aggressive, so practitioners often use a fraction of the Kelly Criterion (e.g., half-Kelly) for more conservative position sizing.</p>"},{"location":"api/allocation/optimization/#pypulate.allocation.optimization.market_cap_weight_portfolio","title":"<code>market_cap_weight_portfolio(market_caps)</code>","text":"<p>Create a market-cap weighted portfolio.</p> <p>Parameters:</p> Name Type Description Default <code>market_caps</code> <code>array - like</code> <p>Array of market capitalizations for each asset</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Market-cap weighted portfolio weights</p>"},{"location":"api/allocation/optimization/#pypulate.allocation.optimization.maximum_diversification_portfolio","title":"<code>maximum_diversification_portfolio(returns, constraints=None)</code>","text":"<p>Find the portfolio that maximizes diversification ratio.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Matrix of asset returns where each column represents an asset</p> required <code>constraints</code> <code>list of dict</code> <p>List of constraints for the optimization problem</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(optimal_weights, portfolio_return, portfolio_risk)</p>"},{"location":"api/allocation/optimization/#pypulate.allocation.optimization.maximum_sharpe_ratio","title":"<code>maximum_sharpe_ratio(returns, risk_free_rate=0.0, constraints=None)</code>","text":"<p>Find the portfolio with maximum Sharpe ratio.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Matrix of asset returns where each column represents an asset</p> required <code>risk_free_rate</code> <code>float</code> <p>Risk-free rate for Sharpe ratio calculation</p> <code>0.0</code> <code>constraints</code> <code>list of dict</code> <p>List of constraints for the optimization problem</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(optimal_weights, portfolio_return, portfolio_risk)</p>"},{"location":"api/allocation/optimization/#pypulate.allocation.optimization.mean_variance_optimization","title":"<code>mean_variance_optimization(returns, target_return=None, risk_free_rate=0.0, constraints=None)</code>","text":"<p>Perform Mean-Variance Optimization to find optimal portfolio weights.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Matrix of asset returns where each column represents an asset</p> required <code>target_return</code> <code>float</code> <p>Target portfolio return. If None, maximizes Sharpe ratio</p> <code>None</code> <code>risk_free_rate</code> <code>float</code> <p>Risk-free rate for Sharpe ratio calculation</p> <code>0.0</code> <code>constraints</code> <code>list of dict</code> <p>List of constraints for the optimization problem</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(optimal_weights, portfolio_return, portfolio_risk)</p>"},{"location":"api/allocation/optimization/#pypulate.allocation.optimization.minimum_variance_portfolio","title":"<code>minimum_variance_portfolio(returns, constraints=None)</code>","text":"<p>Find the portfolio with minimum variance.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Matrix of asset returns where each column represents an asset</p> required <code>constraints</code> <code>list of dict</code> <p>List of constraints for the optimization problem</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(optimal_weights, portfolio_return, portfolio_risk)</p>"},{"location":"api/allocation/optimization/#pypulate.allocation.optimization.risk_parity_portfolio","title":"<code>risk_parity_portfolio(returns, constraints=None)</code>","text":"<p>Find the portfolio where risk is equally distributed across assets.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Matrix of asset returns where each column represents an asset</p> required <code>constraints</code> <code>list of dict</code> <p>List of constraints for the optimization problem</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(optimal_weights, portfolio_return, portfolio_risk)</p>"},{"location":"api/asset/apt/","title":"Arbitrage Pricing Theory (APT) API","text":"<p>Arbitrage Pricing Theory (APT) model implementation.</p>"},{"location":"api/asset/apt/#pypulate.asset.apt.apt","title":"<code>apt(risk_free_rate, factor_betas, factor_risk_premiums)</code>","text":"<p>Calculate expected return using the Arbitrage Pricing Theory (APT) model.</p> <p>APT formula: E(R) = Rf + \u03b2\u2081(RP\u2081) + \u03b2\u2082(RP\u2082) + ... + \u03b2\u2099(RP\u2099)</p> <p>Parameters:</p> Name Type Description Default <code>risk_free_rate</code> <code>float</code> <p>Risk-free rate of return (e.g., 0.03 for 3%)</p> required <code>factor_betas</code> <code>list of float</code> <p>Beta coefficients for each factor</p> required <code>factor_risk_premiums</code> <code>list of float</code> <p>Risk premiums for each factor</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Expected return and components</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If number of factor betas does not match number of factor risk premiums If no factors are provided</p> <code>-------</code>"},{"location":"api/asset/binomial_tree/","title":"Binomial Tree Option Pricing API","text":"<p>Binomial tree option pricing model implementation.</p>"},{"location":"api/asset/binomial_tree/#pypulate.asset.binomial_tree.binomial_tree","title":"<code>binomial_tree(option_type, underlying_price, strike_price, time_to_expiry, risk_free_rate, volatility, steps=100, dividend_yield=0.0)</code>","text":"<p>Calculate option price using the binomial tree model.</p> <p>Parameters:</p> Name Type Description Default <code>option_type</code> <code>str</code> <p>Type of option ('european_call', 'european_put', 'american_call', 'american_put')</p> required <code>underlying_price</code> <code>float</code> <p>Current price of the underlying asset</p> required <code>strike_price</code> <code>float</code> <p>Strike price of the option</p> required <code>time_to_expiry</code> <code>float</code> <p>Time to expiration in years</p> required <code>risk_free_rate</code> <code>float</code> <p>Risk-free interest rate (annualized)</p> required <code>volatility</code> <code>float</code> <p>Volatility of the underlying asset (annualized)</p> required <code>steps</code> <code>int</code> <p>Number of time steps in the binomial tree, by default 100</p> <code>100</code> <code>dividend_yield</code> <code>float</code> <p>Continuous dividend yield, by default 0.0</p> <code>0.0</code> <p>Returns:</p> Type Description <code>dict</code> <p>Option price and details</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If option_type is not one of 'european_call', 'european_put', 'american_call', 'american_put' If underlying_price or strike_price is not positive If time_to_expiry is not positive If volatility is not positive If steps is not a positive integer</p>"},{"location":"api/asset/black_scholes/","title":"Black-Scholes Option Pricing API","text":"<p>Black-Scholes option pricing model implementation.</p>"},{"location":"api/asset/black_scholes/#pypulate.asset.black_scholes.black_scholes","title":"<code>black_scholes(option_type, underlying_price, strike_price, time_to_expiry, risk_free_rate, volatility, dividend_yield=0.0)</code>","text":"<p>Calculate option price using the Black-Scholes model.</p> <p>Parameters:</p> Name Type Description Default <code>option_type</code> <code>str</code> <p>Type of option ('call' or 'put')</p> required <code>underlying_price</code> <code>float</code> <p>Current price of the underlying asset</p> required <code>strike_price</code> <code>float</code> <p>Strike price of the option</p> required <code>time_to_expiry</code> <code>float</code> <p>Time to expiration in years</p> required <code>risk_free_rate</code> <code>float</code> <p>Risk-free interest rate (annualized)</p> required <code>volatility</code> <code>float</code> <p>Volatility of the underlying asset (annualized)</p> required <code>dividend_yield</code> <code>float</code> <p>Continuous dividend yield, by default 0.0</p> <code>0.0</code> <p>Returns:</p> Type Description <code>dict</code> <p>Option price and Greeks</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If option_type is not 'call' or 'put' If underlying_price or strike_price is not positive If time_to_expiry is not positive If volatility is not positive</p>"},{"location":"api/asset/black_scholes/#pypulate.asset.black_scholes.implied_volatility","title":"<code>implied_volatility(option_type, market_price, underlying_price, strike_price, time_to_expiry, risk_free_rate, dividend_yield=0.0, precision=0.0001, max_iterations=100, initial_vol=0.2)</code>","text":"<p>Calculate implied volatility from option market price using the Black-Scholes model.</p> <p>Parameters:</p> Name Type Description Default <code>option_type</code> <code>str</code> <p>Type of option ('call' or 'put')</p> required <code>market_price</code> <code>float</code> <p>Market price of the option</p> required <code>underlying_price</code> <code>float</code> <p>Current price of the underlying asset</p> required <code>strike_price</code> <code>float</code> <p>Strike price of the option</p> required <code>time_to_expiry</code> <code>float</code> <p>Time to expiration in years</p> required <code>risk_free_rate</code> <code>float</code> <p>Risk-free interest rate (annualized)</p> required <code>dividend_yield</code> <code>float</code> <p>Continuous dividend yield, by default 0.0</p> <code>0.0</code> <code>precision</code> <code>float</code> <p>Desired precision for implied volatility, by default 0.0001</p> <code>0.0001</code> <code>max_iterations</code> <code>int</code> <p>Maximum number of iterations, by default 100</p> <code>100</code> <code>initial_vol</code> <code>float</code> <p>Initial volatility guess, by default 0.2</p> <code>0.2</code> <p>Returns:</p> Type Description <code>dict</code> <p>Implied volatility and option details</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pypulate.asset import implied_volatility\n&gt;&gt;&gt; result = implied_volatility(\n...     option_type='call',\n...     market_price=10.5,\n...     underlying_price=100,\n...     strike_price=100,\n...     time_to_expiry=1.0,\n...     risk_free_rate=0.05\n... )\n&gt;&gt;&gt; print(f\"Implied Volatility: {result['implied_volatility']:.2%}\")\nImplied Volatility: 20.12%\n</code></pre>"},{"location":"api/asset/bond_pricing/","title":"Bond Pricing API","text":"<p>Bond pricing and fixed income analysis functions.</p>"},{"location":"api/asset/bond_pricing/#pypulate.asset.bond_pricing.duration_convexity","title":"<code>duration_convexity(face_value, coupon_rate, years_to_maturity, yield_to_maturity, frequency=2)</code>","text":"<p>Calculate the Macaulay duration, modified duration, and convexity of a bond.</p> <p>Parameters:</p> Name Type Description Default <code>face_value</code> <code>float</code> <p>Face value (par value) of the bond</p> required <code>coupon_rate</code> <code>float</code> <p>Annual coupon rate as a decimal (e.g., 0.05 for 5%)</p> required <code>years_to_maturity</code> <code>float</code> <p>Years until the bond matures</p> required <code>yield_to_maturity</code> <code>float</code> <p>Annual yield to maturity as a decimal (e.g., 0.06 for 6%)</p> required <code>frequency</code> <code>int</code> <p>Number of coupon payments per year, by default 2 (semi-annual)</p> <code>2</code> <p>Returns:</p> Type Description <code>dict</code> <p>Duration, modified duration, convexity, and bond details</p>"},{"location":"api/asset/bond_pricing/#pypulate.asset.bond_pricing.price_bond","title":"<code>price_bond(face_value, coupon_rate, years_to_maturity, yield_to_maturity, frequency=2)</code>","text":"<p>Calculate the price of a bond using discounted cash flow analysis.</p> <p>Parameters:</p> Name Type Description Default <code>face_value</code> <code>float</code> <p>Face value (par value) of the bond</p> required <code>coupon_rate</code> <code>float</code> <p>Annual coupon rate as a decimal (e.g., 0.05 for 5%)</p> required <code>years_to_maturity</code> <code>float</code> <p>Years until the bond matures</p> required <code>yield_to_maturity</code> <code>float</code> <p>Annual yield to maturity as a decimal (e.g., 0.06 for 6%)</p> required <code>frequency</code> <code>int</code> <p>Number of coupon payments per year, by default 2 (semi-annual)</p> <code>2</code> <p>Returns:</p> Type Description <code>dict</code> <p>Bond price and details</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the input parameters are invalid</p>"},{"location":"api/asset/bond_pricing/#pypulate.asset.bond_pricing.yield_to_maturity","title":"<code>yield_to_maturity(price, face_value, coupon_rate, years_to_maturity, frequency=2, initial_guess=0.05, precision=1e-10, max_iterations=100)</code>","text":"<p>Calculate the yield to maturity of a bond.</p> <p>Parameters:</p> Name Type Description Default <code>price</code> <code>float</code> <p>Current market price of the bond</p> required <code>face_value</code> <code>float</code> <p>Face value (par value) of the bond</p> required <code>coupon_rate</code> <code>float</code> <p>Annual coupon rate as a decimal (e.g., 0.05 for 5%)</p> required <code>years_to_maturity</code> <code>float</code> <p>Years until the bond matures</p> required <code>frequency</code> <code>int</code> <p>Number of coupon payments per year, by default 2 (semi-annual)</p> <code>2</code> <code>initial_guess</code> <code>float</code> <p>Initial guess for YTM, by default 0.05 (5%)</p> <code>0.05</code> <code>precision</code> <code>float</code> <p>Desired precision for YTM calculation, by default 1e-10</p> <code>1e-10</code> <code>max_iterations</code> <code>int</code> <p>Maximum number of iterations, by default 100</p> <code>100</code> <p>Returns:</p> Type Description <code>dict</code> <p>Yield to maturity and bond details</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the input parameters are invalid</p>"},{"location":"api/asset/capm/","title":"Capital Asset Pricing Model (CAPM) API","text":"<p>Capital Asset Pricing Model (CAPM) implementation.</p>"},{"location":"api/asset/capm/#pypulate.asset.capm.capm","title":"<code>capm(risk_free_rate, beta, market_return)</code>","text":"<p>Calculate expected return using the Capital Asset Pricing Model (CAPM).</p> <p>CAPM formula: E(R) = Rf + \u03b2 \u00d7 (Rm - Rf)</p> <p>This implementation supports both scalar and vector inputs for batch calculations.</p> <p>Parameters:</p> Name Type Description Default <code>risk_free_rate</code> <code>float or array - like</code> <p>Risk-free rate of return (e.g., 0.03 for 3%)</p> required <code>beta</code> <code>float or array - like</code> <p>Beta of the asset(s) (measure of systematic risk)</p> required <code>market_return</code> <code>float or array - like</code> <p>Expected market return (e.g., 0.08 for 8%)</p> required <p>Returns:</p> Type Description <code>dict or list of dict</code> <p>Expected return and components for each asset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the input parameters are invalid</p>"},{"location":"api/asset/fama_french/","title":"Fama-French Models API","text":"<p>Fama-French factor models implementation.</p>"},{"location":"api/asset/fama_french/#pypulate.asset.fama_french.fama_french_five_factor","title":"<code>fama_french_five_factor(risk_free_rate, market_beta, size_beta, value_beta, profitability_beta, investment_beta, market_premium, size_premium, value_premium, profitability_premium, investment_premium)</code>","text":"<p>Calculate expected return using the Fama-French Five-Factor model.</p> <p>Formula: E(R) = Rf + \u03b2_m(Rm-Rf) + \u03b2_s(SMB) + \u03b2_v(HML) + \u03b2_p(RMW) + \u03b2_i(CMA)</p> <p>Parameters:</p> Name Type Description Default <code>risk_free_rate</code> <code>float or array - like</code> <p>Risk-free rate of return (e.g., 0.03 for 3%)</p> required <code>market_beta</code> <code>float or array - like</code> <p>Beta coefficient for market risk factor</p> required <code>size_beta</code> <code>float or array - like</code> <p>Beta coefficient for size factor (SMB - Small Minus Big)</p> required <code>value_beta</code> <code>float or array - like</code> <p>Beta coefficient for value factor (HML - High Minus Low)</p> required <code>profitability_beta</code> <code>float or array - like</code> <p>Beta coefficient for profitability factor (RMW - Robust Minus Weak)</p> required <code>investment_beta</code> <code>float or array - like</code> <p>Beta coefficient for investment factor (CMA - Conservative Minus Aggressive)</p> required <code>market_premium</code> <code>float or array - like</code> <p>Market risk premium (Rm - Rf)</p> required <code>size_premium</code> <code>float or array - like</code> <p>Size premium (SMB)</p> required <code>value_premium</code> <code>float or array - like</code> <p>Value premium (HML)</p> required <code>profitability_premium</code> <code>float or array - like</code> <p>Profitability premium (RMW)</p> required <code>investment_premium</code> <code>float or array - like</code> <p>Investment premium (CMA)</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Expected return and factor contributions</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the input arrays are not of the same shape</p>"},{"location":"api/asset/fama_french/#pypulate.asset.fama_french.fama_french_three_factor","title":"<code>fama_french_three_factor(risk_free_rate, market_beta, size_beta, value_beta, market_premium, size_premium, value_premium)</code>","text":"<p>Calculate expected return using the Fama-French Three-Factor model.</p> <p>Formula: E(R) = Rf + \u03b2_m(Rm-Rf) + \u03b2_s(SMB) + \u03b2_v(HML)</p> <p>Parameters:</p> Name Type Description Default <code>risk_free_rate</code> <code>float or array - like</code> <p>Risk-free rate of return (e.g., 0.03 for 3%)</p> required <code>market_beta</code> <code>float or array - like</code> <p>Beta coefficient for market risk factor</p> required <code>size_beta</code> <code>float or array - like</code> <p>Beta coefficient for size factor (SMB - Small Minus Big)</p> required <code>value_beta</code> <code>float or array - like</code> <p>Beta coefficient for value factor (HML - High Minus Low)</p> required <code>market_premium</code> <code>float or array - like</code> <p>Market risk premium (Rm - Rf)</p> required <code>size_premium</code> <code>float or array - like</code> <p>Size premium (SMB)</p> required <code>value_premium</code> <code>float or array - like</code> <p>Value premium (HML)</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Expected return and factor contributions</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the input arrays are not of the same shape</p>"},{"location":"api/asset/mean_inversion/","title":"Mean Inversion Models API","text":"<p>Mean inversion pricing model for asset pricing.</p> <p>This module implements the Ornstein-Uhlenbeck process for mean-reverting assets such as commodities, interest rates, and volatility.</p>"},{"location":"api/asset/mean_inversion/#pypulate.asset.mean_inversion.analytical_mean_inversion_option","title":"<code>analytical_mean_inversion_option(current_price, long_term_mean, mean_reversion_rate, volatility, time_to_expiry, risk_free_rate, strike_price, option_type='call')</code>","text":"<p>Price European options on mean-reverting assets using an analytical approximation.</p> <p>This function implements an analytical approximation for pricing European options on mean-reverting assets based on the Ornstein-Uhlenbeck process.</p> <p>Parameters:</p> Name Type Description Default <code>current_price</code> <code>float</code> <p>Current price of the underlying asset</p> required <code>long_term_mean</code> <code>float</code> <p>Long-term mean level that the asset price reverts to</p> required <code>mean_reversion_rate</code> <code>float</code> <p>Speed at which the asset price reverts to the long-term mean (annualized)</p> required <code>volatility</code> <code>float</code> <p>Volatility of the asset price (annualized)</p> required <code>time_to_expiry</code> <code>float</code> <p>Time to option expiration in years</p> required <code>risk_free_rate</code> <code>float</code> <p>Risk-free interest rate (annualized)</p> required <code>strike_price</code> <code>float</code> <p>Strike price of the option</p> required <code>option_type</code> <code>str</code> <p>Type of option ('call' or 'put'), by default 'call'</p> <code>'call'</code> <p>Returns:</p> Type Description <code>dict</code> <p>Option price and details</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If Current price, volatility, time to expiry, mean reversion rate, or strike price are not positive. If option type is not 'call' or 'put'.</p>"},{"location":"api/asset/mean_inversion/#pypulate.asset.mean_inversion.mean_inversion_pricing","title":"<code>mean_inversion_pricing(current_price, long_term_mean, mean_reversion_rate, volatility, time_to_expiry, risk_free_rate, strike_price, option_type='call', simulations=10000, time_steps=252, seed=None)</code>","text":"<p>Price options on mean-reverting assets using Monte Carlo simulation.</p> <p>This function implements the Ornstein-Uhlenbeck process to model mean-reverting assets and prices options using Monte Carlo simulation.</p> <p>Parameters:</p> Name Type Description Default <code>current_price</code> <code>float</code> <p>Current price of the underlying asset</p> required <code>long_term_mean</code> <code>float</code> <p>Long-term mean level that the asset price reverts to</p> required <code>mean_reversion_rate</code> <code>float</code> <p>Speed at which the asset price reverts to the long-term mean (annualized)</p> required <code>volatility</code> <code>float</code> <p>Volatility of the asset price (annualized)</p> required <code>time_to_expiry</code> <code>float</code> <p>Time to option expiration in years</p> required <code>risk_free_rate</code> <code>float</code> <p>Risk-free interest rate (annualized)</p> required <code>strike_price</code> <code>float</code> <p>Strike price of the option</p> required <code>option_type</code> <code>str</code> <p>Type of option ('call' or 'put'), by default 'call'</p> <code>'call'</code> <code>simulations</code> <code>int</code> <p>Number of Monte Carlo simulations, by default 10000</p> <code>10000</code> <code>time_steps</code> <code>int</code> <p>Number of time steps in each simulation, by default 252</p> <code>252</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Option price and details</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If Current price, volatility, time to expiry, mean reversion rate, or simulations are not positive. If option type is not 'call' or 'put'. If time steps is not a positive integer.</p>"},{"location":"api/asset/monte_carlo/","title":"Monte Carlo Simulation API","text":"<p>Monte Carlo option pricing model implementation.</p>"},{"location":"api/asset/monte_carlo/#pypulate.asset.monte_carlo.hybrid_price_action_monte_carlo","title":"<code>hybrid_price_action_monte_carlo(option_type, underlying_price, strike_price, time_to_expiry, risk_free_rate, volatility, support_levels, resistance_levels, mean_reversion_params=None, jump_params=None, price_action_weight=0.4, mean_reversion_weight=0.3, jump_diffusion_weight=0.3, respect_level_strength=0.7, volatility_near_levels=1.3, simulations=10000, time_steps=252, dividend_yield=0.0, antithetic=True, seed=None)</code>","text":"<p>A hybrid option pricing model that combines price action Monte Carlo with mean reversion and jump diffusion models.</p> <p>This function creates a weighted average of three pricing models: 1. Price action Monte Carlo (respects support/resistance) 2. Mean reversion (if parameters provided) 3. Jump diffusion (if parameters provided)</p> <p>Parameters:</p> Name Type Description Default <code>option_type</code> <code>str</code> <p>Type of option ('european_call', 'european_put', 'asian_call', 'asian_put',  'lookback_call', 'lookback_put')</p> required <code>underlying_price</code> <code>float</code> <p>Current price of the underlying asset</p> required <code>strike_price</code> <code>float</code> <p>Strike price of the option</p> required <code>time_to_expiry</code> <code>float</code> <p>Time to expiration in years</p> required <code>risk_free_rate</code> <code>float</code> <p>Risk-free interest rate (annualized)</p> required <code>volatility</code> <code>float</code> <p>Base volatility of the underlying asset (annualized)</p> required <code>support_levels</code> <code>List[float]</code> <p>List of price levels that act as support</p> required <code>resistance_levels</code> <code>List[float]</code> <p>List of price levels that act as resistance</p> required <code>mean_reversion_params</code> <code>Dict</code> <p>Parameters for mean reversion model: {'long_term_mean': float, 'mean_reversion_rate': float}</p> <code>None</code> <code>jump_params</code> <code>Dict</code> <p>Parameters for jump diffusion: {'jump_intensity': float, 'jump_mean': float, 'jump_std': float}</p> <code>None</code> <code>price_action_weight</code> <code>float</code> <p>Weight for the price action model (0-1)</p> <code>0.4</code> <code>mean_reversion_weight</code> <code>float</code> <p>Weight for the mean reversion model (0-1)</p> <code>0.3</code> <code>jump_diffusion_weight</code> <code>float</code> <p>Weight for the jump diffusion model (0-1)</p> <code>0.3</code> <code>respect_level_strength</code> <code>float</code> <p>How strongly the price respects support/resistance levels (0-1)</p> <code>0.7</code> <code>volatility_near_levels</code> <code>float</code> <p>Volatility multiplier when price is near support/resistance levels</p> <code>1.3</code> <code>simulations</code> <code>int</code> <p>Number of Monte Carlo simulations</p> <code>10000</code> <code>time_steps</code> <code>int</code> <p>Number of time steps in each simulation</p> <code>252</code> <code>dividend_yield</code> <code>float</code> <p>Continuous dividend yield</p> <code>0.0</code> <code>antithetic</code> <code>bool</code> <p>Whether to use antithetic variates for variance reduction</p> <code>True</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the option price, standard error, and other information</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If weights do not sum to 1.0, or missing required parameters</p>"},{"location":"api/asset/monte_carlo/#pypulate.asset.monte_carlo.monte_carlo_option_pricing","title":"<code>monte_carlo_option_pricing(option_type, underlying_price, strike_price, time_to_expiry, risk_free_rate, volatility, simulations=10000, time_steps=252, dividend_yield=0.0, antithetic=True, jump_intensity=0.0, jump_mean=0.0, jump_std=0.0, seed=None)</code>","text":"<p>Price options using Monte Carlo simulation.</p> <p>This function implements Monte Carlo simulation for pricing various types of options, including European, Asian, and lookback options. It also supports jump diffusion for modeling assets with sudden price jumps like cryptocurrencies.</p> <p>Parameters:</p> Name Type Description Default <code>option_type</code> <code>str</code> <p>Type of option ('european_call', 'european_put', 'asian_call', 'asian_put', 'lookback_call', 'lookback_put')</p> required <code>underlying_price</code> <code>float</code> <p>Current price of the underlying asset</p> required <code>strike_price</code> <code>float</code> <p>Strike price of the option</p> required <code>time_to_expiry</code> <code>float</code> <p>Time to expiration in years</p> required <code>risk_free_rate</code> <code>float</code> <p>Risk-free interest rate (annualized)</p> required <code>volatility</code> <code>float</code> <p>Volatility of the underlying asset (annualized)</p> required <code>simulations</code> <code>int</code> <p>Number of Monte Carlo simulations, by default 10000</p> <code>10000</code> <code>time_steps</code> <code>int</code> <p>Number of time steps in each simulation, by default 252</p> <code>252</code> <code>dividend_yield</code> <code>float</code> <p>Continuous dividend yield, by default 0.0</p> <code>0.0</code> <code>antithetic</code> <code>bool</code> <p>Whether to use antithetic variates for variance reduction, by default True</p> <code>True</code> <code>jump_intensity</code> <code>float</code> <p>Expected number of jumps per year (lambda in Poisson process), by default 0.0</p> <code>0.0</code> <code>jump_mean</code> <code>float</code> <p>Mean of the jump size distribution, by default 0.0</p> <code>0.0</code> <code>jump_std</code> <code>float</code> <p>Standard deviation of the jump size distribution, by default 0.0</p> <code>0.0</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Option price and details</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If option_type is not valid, or prices are non-positive, or time to expiry is non-positive, or volatility is non-positive, or number of simulations or time steps are non-positive, or jump intensity or jump standard deviation are negative</p>"},{"location":"api/asset/monte_carlo/#pypulate.asset.monte_carlo.price_action_monte_carlo","title":"<code>price_action_monte_carlo(option_type, underlying_price, strike_price, time_to_expiry, risk_free_rate, volatility, support_levels, resistance_levels, respect_level_strength=0.7, volatility_near_levels=1.3, simulations=10000, time_steps=252, dividend_yield=0.0, jump_intensity=0.0, jump_mean=0.0, jump_std=0.0, antithetic=True, seed=None)</code>","text":"<p>Price options using Monte Carlo simulation with price action considerations.</p> <p>This function extends the standard Monte Carlo simulation by incorporating technical analysis elements such as support and resistance levels. The price paths are adjusted to respect these levels, with increased volatility near levels and potential bounces or breakouts.</p> <p>Parameters:</p> Name Type Description Default <code>option_type</code> <code>str</code> <p>Type of option ('european_call', 'european_put', 'asian_call', 'asian_put',  'lookback_call', 'lookback_put')</p> required <code>underlying_price</code> <code>float</code> <p>Current price of the underlying asset</p> required <code>strike_price</code> <code>float</code> <p>Strike price of the option</p> required <code>time_to_expiry</code> <code>float</code> <p>Time to expiration in years</p> required <code>risk_free_rate</code> <code>float</code> <p>Risk-free interest rate (annualized)</p> required <code>volatility</code> <code>float</code> <p>Base volatility of the underlying asset (annualized)</p> required <code>support_levels</code> <code>List[float]</code> <p>List of price levels that act as support (in ascending order)</p> required <code>resistance_levels</code> <code>List[float]</code> <p>List of price levels that act as resistance (in ascending order)</p> required <code>respect_level_strength</code> <code>float</code> <p>How strongly the price respects support/resistance levels (0-1) 0 = no respect (standard Monte Carlo), 1 = strong respect</p> <code>0.7</code> <code>volatility_near_levels</code> <code>float</code> <p>Volatility multiplier when price is near support/resistance levels</p> <code>1.3</code> <code>simulations</code> <code>int</code> <p>Number of Monte Carlo simulations</p> <code>10000</code> <code>time_steps</code> <code>int</code> <p>Number of time steps in each simulation</p> <code>252</code> <code>dividend_yield</code> <code>float</code> <p>Continuous dividend yield</p> <code>0.0</code> <code>jump_intensity</code> <code>float</code> <p>Expected number of jumps per year (lambda in Poisson process)</p> <code>0.0</code> <code>jump_mean</code> <code>float</code> <p>Mean of the jump size distribution</p> <code>0.0</code> <code>jump_std</code> <code>float</code> <p>Standard deviation of the jump size distribution</p> <code>0.0</code> <code>antithetic</code> <code>bool</code> <p>Whether to use antithetic variates for variance reduction</p> <code>True</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the option price, standard error, and other information</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If prices are non-positive, or time to expiry is non-positive, or volatility is non-positive, or number of simulations or time steps are non-positive, or jump intensity or jump standard deviation are negative, or respect level strength is not between 0 and 1, or volatility multiplier is non-positive</p>"},{"location":"api/asset/risk_neutral/","title":"Risk Neutral Pricing API","text":"<p>Risk-neutral valuation for derivatives pricing.</p>"},{"location":"api/asset/risk_neutral/#pypulate.asset.risk_neutral.risk_neutral_valuation","title":"<code>risk_neutral_valuation(payoff_function, underlying_price, risk_free_rate, volatility, time_to_expiry, steps=100, simulations=1000, dividend_yield=0.0, seed=None)</code>","text":"<p>Price a derivative using risk-neutral valuation with optimized NumPy vectorization.</p> <p>Parameters:</p> Name Type Description Default <code>payoff_function</code> <code>callable</code> <p>Function that takes the final underlying price and returns the payoff</p> required <code>underlying_price</code> <code>float</code> <p>Current price of the underlying asset</p> required <code>risk_free_rate</code> <code>float</code> <p>Risk-free interest rate (annualized)</p> required <code>volatility</code> <code>float</code> <p>Volatility of the underlying asset (annualized)</p> required <code>time_to_expiry</code> <code>float</code> <p>Time to expiration in years</p> required <code>steps</code> <code>int</code> <p>Number of time steps in the simulation, by default 100</p> <code>100</code> <code>simulations</code> <code>int</code> <p>Number of Monte Carlo simulations, by default 1000</p> <code>1000</code> <code>dividend_yield</code> <code>float</code> <p>Continuous dividend yield, by default 0.0</p> <code>0.0</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Derivative price and details</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the payoff function is not callable, underlying price is not positive, time to expiry is not positive, volatility is not positive, steps is not a positive integer, or simulations is not a positive integer.</p>"},{"location":"api/asset/term_structure/","title":"Term Structure Models API","text":"<p>Term structure models for yield curve fitting.</p>"},{"location":"api/asset/term_structure/#pypulate.asset.term_structure.nelson_siegel","title":"<code>nelson_siegel(maturities, rates, initial_params=None)</code>","text":"<p>Fit the Nelson-Siegel model to yield curve data.</p> <p>The Nelson-Siegel model is defined as: r(t) = \u03b2\u2080 + \u03b2\u2081 * (1 - exp(-t/\u03c4))/(t/\u03c4) + \u03b2\u2082 * ((1 - exp(-t/\u03c4))/(t/\u03c4) - exp(-t/\u03c4))</p> <p>Parameters:</p> Name Type Description Default <code>maturities</code> <code>list of float</code> <p>Maturities in years for the observed rates</p> required <code>rates</code> <code>list of float</code> <p>Observed interest rates (as decimals)</p> required <code>initial_params</code> <code>list of float</code> <p>Initial parameters [\u03b2\u2080, \u03b2\u2081, \u03b2\u2082, \u03c4], by default [0.03, -0.02, -0.01, 1.5]</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Fitted parameters and model details</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pypulate.asset import nelson_siegel\n&gt;&gt;&gt; result = nelson_siegel(\n...     maturities=[0.25, 0.5, 1, 2, 3, 5, 7, 10, 20, 30],\n...     rates=[0.01, 0.015, 0.02, 0.025, 0.028, 0.03, 0.031, 0.032, 0.033, 0.034]\n... )\n&gt;&gt;&gt; # Get the fitted parameters\n&gt;&gt;&gt; beta0, beta1, beta2, tau = result['parameters']\n&gt;&gt;&gt; print(f\"Long-term rate (\u03b2\u2080): {beta0:.2%}\")\nLong-term rate (\u03b2\u2080): 3.40%\n&gt;&gt;&gt; # Predict rate at a specific maturity\n&gt;&gt;&gt; rate_4y = result['predict_func'](4)\n&gt;&gt;&gt; print(f\"4-year rate: {rate_4y:.2%}\")\n4-year rate: 2.93%\n</code></pre>"},{"location":"api/asset/term_structure/#pypulate.asset.term_structure.svensson","title":"<code>svensson(maturities, rates, initial_params=None)</code>","text":"<p>Fit the Svensson model to yield curve data.</p> <p>The Svensson model is defined as: r(t) = \u03b2\u2080 + \u03b2\u2081 * (1 - exp(-t/\u03c4\u2081))/(t/\u03c4\u2081) +         \u03b2\u2082 * ((1 - exp(-t/\u03c4\u2081))/(t/\u03c4\u2081) - exp(-t/\u03c4\u2081)) +        \u03b2\u2083 * ((1 - exp(-t/\u03c4\u2082))/(t/\u03c4\u2082) - exp(-t/\u03c4\u2082))</p> <p>Parameters:</p> Name Type Description Default <code>maturities</code> <code>list of float</code> <p>Maturities in years for the observed rates</p> required <code>rates</code> <code>list of float</code> <p>Observed interest rates (as decimals)</p> required <code>initial_params</code> <code>list of float</code> <p>Initial parameters [\u03b2\u2080, \u03b2\u2081, \u03b2\u2082, \u03b2\u2083, \u03c4\u2081, \u03c4\u2082], by default [0.03, -0.02, -0.01, 0.01, 1.5, 10]</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Fitted parameters and model details</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pypulate.asset import svensson\n&gt;&gt;&gt; result = svensson(\n...     maturities=[0.25, 0.5, 1, 2, 3, 5, 7, 10, 20, 30],\n...     rates=[0.01, 0.015, 0.02, 0.025, 0.028, 0.03, 0.031, 0.032, 0.033, 0.034]\n... )\n&gt;&gt;&gt; # Get the fitted parameters\n&gt;&gt;&gt; beta0, beta1, beta2, beta3, tau1, tau2 = result['parameters']\n&gt;&gt;&gt; print(f\"Long-term rate (\u03b2\u2080): {beta0:.2%}\")\nLong-term rate (\u03b2\u2080): 3.40%\n&gt;&gt;&gt; # Predict rate at a specific maturity\n&gt;&gt;&gt; rate_4y = result['predict_func'](4)\n&gt;&gt;&gt; print(f\"4-year rate: {rate_4y:.2%}\")\n4-year rate: 2.93%\n</code></pre>"},{"location":"api/asset/yield_curve/","title":"Yield Curve API","text":"<p>Yield curve construction and interpolation functions.</p>"},{"location":"api/asset/yield_curve/#pypulate.asset.yield_curve.construct_yield_curve","title":"<code>construct_yield_curve(maturities, rates, interpolation_method='cubic', extrapolate=False)</code>","text":"<p>Construct a yield curve from observed market rates.</p> <p>Parameters:</p> Name Type Description Default <code>maturities</code> <code>list of float</code> <p>Maturities in years for the observed rates</p> required <code>rates</code> <code>list of float</code> <p>Observed interest rates (as decimals)</p> required <code>interpolation_method</code> <code>str</code> <p>Method for interpolation ('linear', 'cubic', 'monotonic'), by default 'cubic'</p> <code>'cubic'</code> <code>extrapolate</code> <code>bool</code> <p>Whether to allow extrapolation beyond observed maturities, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Yield curve object and details</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs are invalid or maturities are not in ascending order If interpolation method is not valid</p>"},{"location":"api/asset/yield_curve/#pypulate.asset.yield_curve.interpolate_rate","title":"<code>interpolate_rate(yield_curve, maturity)</code>","text":"<p>Interpolate interest rate at a specific maturity from a yield curve.</p> <p>Parameters:</p> Name Type Description Default <code>yield_curve</code> <code>dict</code> <p>Yield curve object from construct_yield_curve</p> required <code>maturity</code> <code>float</code> <p>Maturity in years for which to interpolate the rate</p> required <p>Returns:</p> Type Description <code>float</code> <p>Interpolated interest rate</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If yield_curve is not a valid yield curve object If maturity is not positive</p>"},{"location":"api/credit/altman_z_score/","title":"Altman Z-Score API Reference","text":"<p>This page documents the API for the Altman Z-Score function in Pypulate.</p> <p>Altman Z-Score for bankruptcy prediction.</p>"},{"location":"api/credit/altman_z_score/#pypulate.credit.altman_z_score.altman_z_score","title":"<code>altman_z_score(working_capital, retained_earnings, ebit, market_value_equity, sales, total_assets, total_liabilities)</code>","text":"<p>Calculate Altman Z-Score for predicting bankruptcy risk.</p> <p>Z-Score = 1.2X1 + 1.4X2 + 3.3X3 + 0.6X4 + 0.999*X5</p> <p>Parameters:</p> Name Type Description Default <code>working_capital</code> <code>float</code> <p>Working capital</p> required <code>retained_earnings</code> <code>float</code> <p>Retained earnings</p> required <code>ebit</code> <code>float</code> <p>Earnings before interest and taxes</p> required <code>market_value_equity</code> <code>float</code> <p>Market value of equity</p> required <code>sales</code> <code>float</code> <p>Sales</p> required <code>total_assets</code> <code>float</code> <p>Total assets</p> required <code>total_liabilities</code> <code>float</code> <p>Total liabilities</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Z-score value and risk interpretation</p>"},{"location":"api/credit/create_scorecard/","title":"Create Scorecard API Reference","text":"<p>This page documents the API for the Create Scorecard function in Pypulate.</p> <p>Scorecard creation for credit scoring.</p>"},{"location":"api/credit/create_scorecard/#pypulate.credit.create_scorecard.create_scorecard","title":"<code>create_scorecard(features, weights, offsets=None, scaling_factor=100.0, base_score=600)</code>","text":"<p>Create a points-based scorecard for credit scoring.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>dict</code> <p>Dictionary of feature names and their values (e.g., {\"income\": 75000, \"age\": 35}).</p> required <code>weights</code> <code>dict</code> <p>Dictionary of feature names and their weights (e.g., {\"income\": 0.3, \"age\": 0.5}).</p> required <code>offsets</code> <code>dict</code> <p>Dictionary of feature names and offset values (default is 0 for each feature if None).</p> <code>None</code> <code>scaling_factor</code> <code>float</code> <p>Scaling factor to divide the points, controlling the score range (default is 100.0).</p> <code>100.0</code> <code>base_score</code> <code>float</code> <p>Base score to which feature points are added (default is 600).</p> <code>600</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the total score, points breakdown, and risk category.</p>"},{"location":"api/credit/debt_service_coverage_ratio/","title":"Debt Service Coverage Ratio API Reference","text":"<p>This page documents the API for the Debt Service Coverage Ratio function in Pypulate.</p> <p>Debt Service Coverage Ratio (DSCR) calculation.</p>"},{"location":"api/credit/debt_service_coverage_ratio/#pypulate.credit.debt_service_coverage_ratio.debt_service_coverage_ratio","title":"<code>debt_service_coverage_ratio(net_operating_income, total_debt_service)</code>","text":"<p>Calculate Debt Service Coverage Ratio (DSCR).</p> <p>DSCR = Net Operating Income / Total Debt Service</p> <p>Parameters:</p> Name Type Description Default <code>net_operating_income</code> <code>float</code> <p>Net operating income</p> required <code>total_debt_service</code> <code>float</code> <p>Total debt service</p> required <p>Returns:</p> Type Description <code>dict</code> <p>DSCR value and interpretation</p>"},{"location":"api/credit/expected_credit_loss/","title":"Expected Credit Loss API Reference","text":"<p>This page documents the API for the Expected Credit Loss function in Pypulate.</p> <p>Expected Credit Loss (ECL) calculation.</p>"},{"location":"api/credit/expected_credit_loss/#pypulate.credit.expected_credit_loss.expected_credit_loss","title":"<code>expected_credit_loss(pd, lgd, ead, time_horizon=1.0, discount_rate=0.0)</code>","text":"<p>Calculate expected credit loss.</p> <p>ECL = PD \u00d7 LGD \u00d7 EAD \u00d7 Discount Factor</p> <p>Parameters:</p> Name Type Description Default <code>pd</code> <code>float</code> <p>Probability of default</p> required <code>lgd</code> <code>float</code> <p>Loss given default (as a decimal)</p> required <code>ead</code> <code>float</code> <p>Exposure at default</p> required <code>time_horizon</code> <code>float</code> <p>Time horizon in years</p> <code>1.0</code> <code>discount_rate</code> <code>float</code> <p>Discount rate for future losses</p> <code>0.0</code> <p>Returns:</p> Type Description <code>dict</code> <p>ECL and components</p>"},{"location":"api/credit/exposure_at_default/","title":"Exposure at Default API Reference","text":"<p>This page documents the API for the Exposure at Default function in Pypulate.</p> <p>Exposure at Default (EAD) calculation.</p>"},{"location":"api/credit/exposure_at_default/#pypulate.credit.exposure_at_default.exposure_at_default","title":"<code>exposure_at_default(current_balance, undrawn_amount, credit_conversion_factor=0.5)</code>","text":"<p>Calculate exposure at default for credit facilities.</p> <p>Parameters:</p> Name Type Description Default <code>current_balance</code> <code>float</code> <p>Current drawn balance</p> required <code>undrawn_amount</code> <code>float</code> <p>Undrawn commitment</p> required <code>credit_conversion_factor</code> <code>float</code> <p>Factor to convert undrawn amounts to exposure</p> <code>0.5</code> <p>Returns:</p> Type Description <code>dict</code> <p>EAD and components</p>"},{"location":"api/credit/financial_ratios/","title":"Financial Ratios API Reference","text":"<p>This page documents the API for the Financial Ratios function in Pypulate.</p> <p>Financial ratios calculation for credit assessment.</p>"},{"location":"api/credit/financial_ratios/#pypulate.credit.financial_ratios.financial_ratios","title":"<code>financial_ratios(current_assets, current_liabilities, total_assets, total_liabilities, ebit, interest_expense, net_income, total_equity, sales)</code>","text":"<p>Calculate key financial ratios for credit assessment.</p> <p>Parameters:</p> Name Type Description Default <code>current_assets</code> <code>float</code> <p>Current assets</p> required <code>current_liabilities</code> <code>float</code> <p>Current liabilities</p> required <code>total_assets</code> <code>float</code> <p>Total assets</p> required <code>total_liabilities</code> <code>float</code> <p>Total liabilities</p> required <code>ebit</code> <code>float</code> <p>Earnings before interest and taxes</p> required <code>interest_expense</code> <code>float</code> <p>Interest expense</p> required <code>net_income</code> <code>float</code> <p>Net income</p> required <code>total_equity</code> <code>float</code> <p>Total equity</p> required <code>sales</code> <code>float</code> <p>Sales</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Financial ratios and assessments</p>"},{"location":"api/credit/loan_pricing/","title":"Loan Pricing API Reference","text":"<p>This page documents the API for the Risk-Based Loan Pricing function in Pypulate.</p> <p>Risk-based loan pricing model.</p>"},{"location":"api/credit/loan_pricing/#pypulate.credit.loan_pricing.loan_pricing","title":"<code>loan_pricing(loan_amount, term, pd, lgd, funding_cost, operating_cost, capital_requirement, target_roe)</code>","text":"<p>Calculate risk-based loan pricing.</p> <p>Parameters:</p> Name Type Description Default <code>loan_amount</code> <code>float</code> <p>Loan amount</p> required <code>term</code> <code>float</code> <p>Loan term in years</p> required <code>pd</code> <code>float</code> <p>Probability of default (annual)</p> required <code>lgd</code> <code>float</code> <p>Loss given default (as a decimal)</p> required <code>funding_cost</code> <code>float</code> <p>Cost of funds (annual rate)</p> required <code>operating_cost</code> <code>float</code> <p>Operating costs (as percentage of loan amount)</p> required <code>capital_requirement</code> <code>float</code> <p>Capital requirement as percentage of loan amount</p> required <code>target_roe</code> <code>float</code> <p>Target return on equity (annual rate)</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Recommended interest rate and components</p>"},{"location":"api/credit/logistic_regression_score/","title":"Logistic Regression Score API Reference","text":"<p>This page documents the API for the Logistic Regression Score function in Pypulate.</p> <p>Logistic regression scoring for credit risk assessment.</p>"},{"location":"api/credit/logistic_regression_score/#pypulate.credit.logistic_regression_score.logistic_regression_score","title":"<code>logistic_regression_score(coefficients, features, intercept=0)</code>","text":"<p>Calculate credit score using logistic regression coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>coefficients</code> <code>array_like</code> <p>Coefficients for each feature</p> required <code>features</code> <code>array_like</code> <p>Feature values</p> required <code>intercept</code> <code>float</code> <p>Intercept term</p> <code>0</code> <p>Returns:</p> Type Description <code>dict</code> <p>Probability of default and score</p>"},{"location":"api/credit/loss_given_default/","title":"Loss Given Default API Reference","text":"<p>This page documents the API for the Loss Given Default function in Pypulate.</p> <p>Loss Given Default (LGD) estimation.</p>"},{"location":"api/credit/loss_given_default/#pypulate.credit.loss_given_default.loss_given_default","title":"<code>loss_given_default(collateral_value, loan_amount, recovery_rate=None, liquidation_costs=0.1, time_to_recovery=1.0)</code>","text":"<p>Estimate the loss given default for a loan.</p> <p>Parameters:</p> Name Type Description Default <code>collateral_value</code> <code>float</code> <p>Value of collateral</p> required <code>loan_amount</code> <code>float</code> <p>Outstanding loan amount</p> required <code>recovery_rate</code> <code>float</code> <p>Historical recovery rate for similar loans</p> <code>None</code> <code>liquidation_costs</code> <code>float</code> <p>Costs associated with liquidating collateral</p> <code>0.1</code> <code>time_to_recovery</code> <code>float</code> <p>Expected time to recovery in years</p> <code>1.0</code> <p>Returns:</p> Type Description <code>dict</code> <p>LGD estimate and components</p>"},{"location":"api/credit/merton_model/","title":"Merton Model API Reference","text":"<p>This page documents the API for the Merton Model function in Pypulate.</p> <p>Merton model for default probability calculation.</p>"},{"location":"api/credit/merton_model/#pypulate.credit.merton_model.merton_model","title":"<code>merton_model(asset_value, debt_face_value, asset_volatility, risk_free_rate, time_to_maturity)</code>","text":"<p>Calculate default probability using the Merton model.</p> <p>Parameters:</p> Name Type Description Default <code>asset_value</code> <code>float</code> <p>Market value of assets</p> required <code>debt_face_value</code> <code>float</code> <p>Face value of debt</p> required <code>asset_volatility</code> <code>float</code> <p>Volatility of assets (annualized)</p> required <code>risk_free_rate</code> <code>float</code> <p>Risk-free interest rate</p> required <code>time_to_maturity</code> <code>float</code> <p>Time to maturity in years</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Default probability and distance to default</p>"},{"location":"api/credit/scoring_model_validation/","title":"Scoring Model Validation API Reference","text":"<p>This page documents the API for the Scoring Model Validation function in Pypulate.</p> <p>Credit scoring model validation.</p>"},{"location":"api/credit/scoring_model_validation/#pypulate.credit.scoring_model_validation.scoring_model_validation","title":"<code>scoring_model_validation(predicted_scores, actual_defaults, score_bins=10)</code>","text":"<p>Validate credit scoring model performance.</p> <p>Parameters:</p> Name Type Description Default <code>predicted_scores</code> <code>array_like</code> <p>Predicted credit scores</p> required <code>actual_defaults</code> <code>array_like</code> <p>Actual default outcomes (0/1)</p> required <code>score_bins</code> <code>int</code> <p>Number of score bins for analysis</p> <code>10</code> <p>Returns:</p> Type Description <code>dict</code> <p>Validation metrics (Gini, KS, AUC, etc.)</p>"},{"location":"api/credit/transition_matrix/","title":"Transition Matrix API Reference","text":"<p>This page documents the API for the Credit Rating Transition Matrix function in Pypulate.</p> <p>Credit rating transition matrix calculation.</p>"},{"location":"api/credit/transition_matrix/#pypulate.credit.transition_matrix.transition_matrix","title":"<code>transition_matrix(ratings_t0, ratings_t1)</code>","text":"<p>Calculate credit rating transition matrix.</p> <p>Parameters:</p> Name Type Description Default <code>ratings_t0</code> <code>array_like</code> <p>Ratings at time 0</p> required <code>ratings_t1</code> <code>array_like</code> <p>Ratings at time 1</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Transition matrix and probabilities</p>"},{"location":"api/credit/weight_of_evidence/","title":"Weight of Evidence API Reference","text":"<p>This page documents the API for the Weight of Evidence function in Pypulate.</p> <p>Weight of Evidence (WOE) and Information Value (IV) calculation.</p>"},{"location":"api/credit/weight_of_evidence/#pypulate.credit.weight_of_evidence.weight_of_evidence","title":"<code>weight_of_evidence(good_count, bad_count, min_samples=0.01, adjustment=0.5)</code>","text":"<p>Calculate Weight of Evidence (WOE) and Information Value (IV).</p> <p>WOE = ln(Distribution of Good / Distribution of Bad)</p> <p>Parameters:</p> Name Type Description Default <code>good_count</code> <code>array_like</code> <p>Count of good cases in each bin</p> required <code>bad_count</code> <code>array_like</code> <p>Count of bad cases in each bin</p> required <code>min_samples</code> <code>float</code> <p>Minimum percentage of samples required in a bin</p> <code>0.01</code> <code>adjustment</code> <code>float</code> <p>Adjustment factor for zero counts</p> <code>0.5</code> <p>Returns:</p> Type Description <code>dict</code> <p>WOE values, IV, and distributions</p>"},{"location":"api/kpi/business_kpi/","title":"business kpi API Reference","text":"<p>Business KPIs Module</p> <p>This module provides functions for calculating various business metrics commonly used in SaaS and subscription-based businesses.</p>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.annual_recurring_revenue","title":"<code>annual_recurring_revenue(paying_customers, avg_revenue_per_customer)</code>","text":"<p>Calculate Annual Recurring Revenue (ARR).</p> <p>ARR is the value of the recurring revenue of a business's term subscriptions normalized for a single calendar year.</p> <p>Parameters:</p> Name Type Description Default <code>paying_customers</code> <code>array - like</code> <p>Number of paying customers</p> required <code>avg_revenue_per_customer</code> <code>array - like</code> <p>Average revenue per customer per month</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Annual Recurring Revenue</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; annual_recurring_revenue(100, 50)\n60000.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.average_revenue_per_paying_user","title":"<code>average_revenue_per_paying_user(total_revenue, paying_users)</code>","text":"<p>Calculate Average Revenue Per Paying User (ARPPU).</p> <p>ARPPU measures the average revenue generated per paying user or customer.</p> <p>Parameters:</p> Name Type Description Default <code>total_revenue</code> <code>array - like</code> <p>Total revenue for the period</p> required <code>paying_users</code> <code>array - like</code> <p>Number of paying users or customers</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Average Revenue Per Paying User</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; average_revenue_per_paying_user(10000, 200)\n50.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.average_revenue_per_user","title":"<code>average_revenue_per_user(total_revenue, total_users)</code>","text":"<p>Calculate Average Revenue Per User (ARPU).</p> <p>ARPU measures the average revenue generated per user or customer.</p> <p>Parameters:</p> Name Type Description Default <code>total_revenue</code> <code>array - like</code> <p>Total revenue for the period</p> required <code>total_users</code> <code>array - like</code> <p>Total number of users or customers</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Average Revenue Per User</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; average_revenue_per_user(10000, 500)\n20.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.burn_rate","title":"<code>burn_rate(starting_capital, ending_capital, months)</code>","text":"<p>Calculate Monthly Burn Rate.</p> <p>Burn Rate is the rate at which a company is losing money.</p> <p>Parameters:</p> Name Type Description Default <code>starting_capital</code> <code>array - like</code> <p>Capital at the start of the period</p> required <code>ending_capital</code> <code>array - like</code> <p>Capital at the end of the period</p> required <code>months</code> <code>array - like</code> <p>Number of months in the period</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Monthly Burn Rate</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; burn_rate(100000, 70000, 6)\n5000.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.churn_rate","title":"<code>churn_rate(customers_start, customers_end, new_customers)</code>","text":"<p>Calculate customer churn rate.</p> <p>Churn rate is the percentage of customers who stop using your product or service during a given time period.</p> <p>Parameters:</p> Name Type Description Default <code>customers_start</code> <code>array - like</code> <p>Number of customers at the start of the period</p> required <code>customers_end</code> <code>array - like</code> <p>Number of customers at the end of the period</p> required <code>new_customers</code> <code>array - like</code> <p>Number of new customers acquired during the period</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Churn rate as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; churn_rate(100, 90, 10)\n20.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.conversion_rate","title":"<code>conversion_rate(conversions, total_visitors)</code>","text":"<p>Calculate Conversion Rate.</p> <p>Conversion Rate is the percentage of visitors who take a desired action.</p> <p>Parameters:</p> Name Type Description Default <code>conversions</code> <code>array - like</code> <p>Number of conversions (desired actions taken)</p> required <code>total_visitors</code> <code>array - like</code> <p>Total number of visitors or users</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Conversion Rate as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; conversion_rate(50, 1000)\n5.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.customer_acquisition_cost","title":"<code>customer_acquisition_cost(marketing_costs, sales_costs, new_customers)</code>","text":"<p>Calculate Customer Acquisition Cost (CAC).</p> <p>CAC is the cost of convincing a potential customer to buy a product or service.</p> <p>Parameters:</p> Name Type Description Default <code>marketing_costs</code> <code>array - like</code> <p>Total marketing costs for the period</p> required <code>sales_costs</code> <code>array - like</code> <p>Total sales costs for the period</p> required <code>new_customers</code> <code>array - like</code> <p>Number of new customers acquired during the period</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Customer Acquisition Cost</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; customer_acquisition_cost(5000, 3000, 100)\n80.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.customer_effort_score","title":"<code>customer_effort_score(effort_ratings, max_rating=7)</code>","text":"<p>Calculate Customer Effort Score (CES).</p> <p>CES measures how much effort a customer has to exert to use a product or service. Lower scores are better.</p> <p>Parameters:</p> Name Type Description Default <code>effort_ratings</code> <code>array - like</code> <p>Array of customer effort ratings</p> required <code>max_rating</code> <code>array - like</code> <p>Maximum possible rating value</p> <code>7</code> <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Customer Effort Score (average)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; customer_effort_score([2, 3, 1, 2, 4])\n2.4\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.customer_engagement_score","title":"<code>customer_engagement_score(active_days, total_days)</code>","text":"<p>Calculate Customer Engagement Score.</p> <p>Customer Engagement Score measures how actively customers are using a product or service.</p> <p>Parameters:</p> Name Type Description Default <code>active_days</code> <code>array - like</code> <p>Number of days the customer was active</p> required <code>total_days</code> <code>array - like</code> <p>Total number of days in the period</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Customer Engagement Score as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; customer_engagement_score(15, 30)\n50.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.customer_lifetime_value","title":"<code>customer_lifetime_value(avg_revenue_per_customer, gross_margin, churn_rate_value, discount_rate=10.0)</code>","text":"<p>Calculate Customer Lifetime Value (CLV).</p> <p>CLV is the total worth to a business of a customer over the whole period of their relationship.</p> <p>Parameters:</p> Name Type Description Default <code>avg_revenue_per_customer</code> <code>array - like</code> <p>Average revenue per customer per period (e.g., monthly)</p> required <code>gross_margin</code> <code>array - like</code> <p>Gross margin percentage (0-100)</p> required <code>churn_rate_value</code> <code>array - like</code> <p>Churn rate percentage (0-100)</p> required <code>discount_rate</code> <code>array - like</code> <p>Annual discount rate for future cash flows (0-100)</p> <code>10.0</code> <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Customer Lifetime Value</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; customer_lifetime_value(100, 70, 5, 10)\n466.67\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.customer_satisfaction_score","title":"<code>customer_satisfaction_score(satisfaction_ratings, max_rating=5)</code>","text":"<p>Calculate Customer Satisfaction Score (CSAT).</p> <p>CSAT measures how satisfied customers are with a product, service, or interaction.</p> <p>Parameters:</p> Name Type Description Default <code>satisfaction_ratings</code> <code>array - like</code> <p>Array of customer satisfaction ratings</p> required <code>max_rating</code> <code>array - like</code> <p>Maximum possible rating value</p> <code>5</code> <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Customer Satisfaction Score as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; customer_satisfaction_score([4, 5, 3, 5, 4])\n84.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.daily_active_users_ratio","title":"<code>daily_active_users_ratio(daily_active_users, total_users)</code>","text":"<p>Calculate Daily Active Users (DAU) Ratio.</p> <p>DAU Ratio measures the percentage of total users who are active on a daily basis.</p> <p>Parameters:</p> Name Type Description Default <code>daily_active_users</code> <code>array - like</code> <p>Number of daily active users</p> required <code>total_users</code> <code>array - like</code> <p>Total number of users</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Daily Active Users Ratio as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; daily_active_users_ratio(500, 2000)\n25.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.expansion_revenue_rate","title":"<code>expansion_revenue_rate(upsell_revenue, cross_sell_revenue, revenue_start)</code>","text":"<p>Calculate Expansion Revenue Rate.</p> <p>Expansion Revenue Rate is the percentage of additional revenue generated from existing customers.</p> <p>Parameters:</p> Name Type Description Default <code>upsell_revenue</code> <code>array - like</code> <p>Revenue from upselling to existing customers</p> required <code>cross_sell_revenue</code> <code>array - like</code> <p>Revenue from cross-selling to existing customers</p> required <code>revenue_start</code> <code>array - like</code> <p>Revenue at the start of the period</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Expansion Revenue Rate as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; expansion_revenue_rate(1000, 500, 10000)\n15.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.feature_adoption_rate","title":"<code>feature_adoption_rate(users_adopting_feature, total_users)</code>","text":"<p>Calculate Feature Adoption Rate.</p> <p>Feature Adoption Rate measures the percentage of users who adopt a specific feature.</p> <p>Parameters:</p> Name Type Description Default <code>users_adopting_feature</code> <code>array - like</code> <p>Number of users who adopted the feature</p> required <code>total_users</code> <code>array - like</code> <p>Total number of users</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Feature Adoption Rate as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; feature_adoption_rate(300, 1000)\n30.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.gross_margin","title":"<code>gross_margin(revenue, cost_of_goods_sold)</code>","text":"<p>Calculate Gross Margin.</p> <p>Gross Margin is the percentage of revenue that exceeds the cost of goods sold.</p> <p>Parameters:</p> Name Type Description Default <code>revenue</code> <code>array - like</code> <p>Total revenue</p> required <code>cost_of_goods_sold</code> <code>array - like</code> <p>Cost of goods sold</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Gross Margin as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gross_margin(10000, 3000)\n70.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.ltv_cac_ratio","title":"<code>ltv_cac_ratio(ltv, cac)</code>","text":"<p>Calculate LTV:CAC Ratio.</p> <p>LTV:CAC Ratio is a metric that compares the lifetime value of a customer to the cost of acquiring that customer.</p> <p>Parameters:</p> Name Type Description Default <code>ltv</code> <code>array - like</code> <p>Customer Lifetime Value</p> required <code>cac</code> <code>array - like</code> <p>Customer Acquisition Cost</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>LTV:CAC Ratio</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ltv_cac_ratio(1000, 200)\n5.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.monthly_active_users_ratio","title":"<code>monthly_active_users_ratio(monthly_active_users, total_users)</code>","text":"<p>Calculate Monthly Active Users (MAU) Ratio.</p> <p>MAU Ratio measures the percentage of total users who are active on a monthly basis.</p> <p>Parameters:</p> Name Type Description Default <code>monthly_active_users</code> <code>array - like</code> <p>Number of monthly active users</p> required <code>total_users</code> <code>array - like</code> <p>Total number of users</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Monthly Active Users Ratio as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; monthly_active_users_ratio(1500, 2000)\n75.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.monthly_recurring_revenue","title":"<code>monthly_recurring_revenue(paying_customers, avg_revenue_per_customer)</code>","text":"<p>Calculate Monthly Recurring Revenue (MRR).</p> <p>MRR is the predictable total revenue generated by all the active subscriptions in a month.</p> <p>Parameters:</p> Name Type Description Default <code>paying_customers</code> <code>array - like</code> <p>Number of paying customers</p> required <code>avg_revenue_per_customer</code> <code>array - like</code> <p>Average revenue per customer per month</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Monthly Recurring Revenue</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; monthly_recurring_revenue(100, 50)\n5000.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.net_promoter_score","title":"<code>net_promoter_score(promoters, detractors, total_respondents)</code>","text":"<p>Calculate Net Promoter Score (NPS).</p> <p>NPS measures customer experience and predicts business growth.</p> <p>Parameters:</p> Name Type Description Default <code>promoters</code> <code>array - like</code> <p>Number of promoters (customers who rated 9-10)</p> required <code>detractors</code> <code>array - like</code> <p>Number of detractors (customers who rated 0-6)</p> required <code>total_respondents</code> <code>array - like</code> <p>Total number of survey respondents</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Net Promoter Score (ranges from -100 to 100)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; net_promoter_score(70, 10, 100)\n60.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.payback_period","title":"<code>payback_period(cac, avg_monthly_revenue, gross_margin)</code>","text":"<p>Calculate CAC Payback Period in months.</p> <p>CAC Payback Period is the number of months it takes to recover the cost of acquiring a customer.</p> <p>Parameters:</p> Name Type Description Default <code>cac</code> <code>array - like</code> <p>Customer Acquisition Cost</p> required <code>avg_monthly_revenue</code> <code>array - like</code> <p>Average monthly revenue per customer</p> required <code>gross_margin</code> <code>array - like</code> <p>Gross margin percentage (0-100)</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>CAC Payback Period in months</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; payback_period(1000, 100, 70)\n14.29\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.retention_rate","title":"<code>retention_rate(customers_start, customers_end, new_customers)</code>","text":"<p>Calculate customer retention rate.</p> <p>Retention rate is the percentage of customers who remain with your product or service over a given time period.</p> <p>Parameters:</p> Name Type Description Default <code>customers_start</code> <code>array - like</code> <p>Number of customers at the start of the period</p> required <code>customers_end</code> <code>array - like</code> <p>Number of customers at the end of the period</p> required <code>new_customers</code> <code>array - like</code> <p>Number of new customers acquired during the period</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Retention rate as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; retention_rate(100, 90, 10)\n80.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.revenue_churn_rate","title":"<code>revenue_churn_rate(revenue_start, revenue_end, new_revenue)</code>","text":"<p>Calculate Revenue Churn Rate.</p> <p>Revenue Churn Rate is the percentage of revenue lost from existing customers in a given period.</p> <p>Parameters:</p> Name Type Description Default <code>revenue_start</code> <code>array - like</code> <p>Revenue at the start of the period</p> required <code>revenue_end</code> <code>array - like</code> <p>Revenue at the end of the period</p> required <code>new_revenue</code> <code>array - like</code> <p>New revenue acquired during the period</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Revenue Churn Rate as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; revenue_churn_rate(10000, 9500, 1000)\n15.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.roi","title":"<code>roi(revenue, costs)</code>","text":"<p>Calculate Return on Investment (ROI).</p> <p>ROI measures the return on an investment relative to its cost.</p> <p>Parameters:</p> Name Type Description Default <code>revenue</code> <code>array - like</code> <p>Revenue or return from the investment</p> required <code>costs</code> <code>array - like</code> <p>Cost of the investment</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Return on Investment as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; roi(150, 100)\n50.0\n&gt;&gt;&gt; roi([150, 200, 250], [100, 120, 150])\narray([50., 66.67, 66.67])\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.runway","title":"<code>runway(current_capital, monthly_burn_rate)</code>","text":"<p>Calculate Runway in months.</p> <p>Runway is the amount of time a company has before it runs out of money.</p> <p>Parameters:</p> Name Type Description Default <code>current_capital</code> <code>array - like</code> <p>Current capital</p> required <code>monthly_burn_rate</code> <code>array - like</code> <p>Monthly burn rate</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Runway in months</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; runway(100000, 5000)\n20.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.stickiness_ratio","title":"<code>stickiness_ratio(daily_active_users, monthly_active_users)</code>","text":"<p>Calculate Stickiness Ratio (DAU/MAU).</p> <p>Stickiness Ratio measures how frequently active users engage with a product.</p> <p>Parameters:</p> Name Type Description Default <code>daily_active_users</code> <code>array - like</code> <p>Number of daily active users</p> required <code>monthly_active_users</code> <code>array - like</code> <p>Number of monthly active users</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Stickiness Ratio as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; stickiness_ratio(500, 1500)\n33.33\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.time_to_value","title":"<code>time_to_value(onboarding_time, setup_time, learning_time)</code>","text":"<p>Calculate Time to Value (TTV).</p> <p>Time to Value is the amount of time it takes for a customer to realize value from a product.</p> <p>Parameters:</p> Name Type Description Default <code>onboarding_time</code> <code>array - like</code> <p>Time spent on onboarding</p> required <code>setup_time</code> <code>array - like</code> <p>Time spent on setup</p> required <code>learning_time</code> <code>array - like</code> <p>Time spent on learning</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Time to Value</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; time_to_value(2, 3, 5)\n10.0\n</code></pre>"},{"location":"api/kpi/business_kpi/#pypulate.kpi.business_kpi.virality_coefficient","title":"<code>virality_coefficient(new_users, invites_sent, total_users)</code>","text":"<p>Calculate Virality Coefficient (K-factor).</p> <p>Virality Coefficient measures how many new users each existing user brings in.</p> <p>Parameters:</p> Name Type Description Default <code>new_users</code> <code>array - like</code> <p>Number of new users from invites</p> required <code>invites_sent</code> <code>array - like</code> <p>Number of invites sent</p> required <code>total_users</code> <code>array - like</code> <p>Total number of users</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Virality Coefficient</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; virality_coefficient(100, 500, 1000)\n0.1\n</code></pre>"},{"location":"api/kpi/kpi/","title":"business kpi API Reference","text":"<p>Business KPIs Module</p> <p>This module provides functions for calculating various business metrics commonly used in SaaS and subscription-based businesses.</p>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.annual_recurring_revenue","title":"<code>annual_recurring_revenue(paying_customers, avg_revenue_per_customer)</code>","text":"<p>Calculate Annual Recurring Revenue (ARR).</p> <p>ARR is the value of the recurring revenue of a business's term subscriptions normalized for a single calendar year.</p> <p>Parameters:</p> Name Type Description Default <code>paying_customers</code> <code>array - like</code> <p>Number of paying customers</p> required <code>avg_revenue_per_customer</code> <code>array - like</code> <p>Average revenue per customer per month</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Annual Recurring Revenue</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; annual_recurring_revenue(100, 50)\n60000.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.average_revenue_per_paying_user","title":"<code>average_revenue_per_paying_user(total_revenue, paying_users)</code>","text":"<p>Calculate Average Revenue Per Paying User (ARPPU).</p> <p>ARPPU measures the average revenue generated per paying user or customer.</p> <p>Parameters:</p> Name Type Description Default <code>total_revenue</code> <code>array - like</code> <p>Total revenue for the period</p> required <code>paying_users</code> <code>array - like</code> <p>Number of paying users or customers</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Average Revenue Per Paying User</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; average_revenue_per_paying_user(10000, 200)\n50.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.average_revenue_per_user","title":"<code>average_revenue_per_user(total_revenue, total_users)</code>","text":"<p>Calculate Average Revenue Per User (ARPU).</p> <p>ARPU measures the average revenue generated per user or customer.</p> <p>Parameters:</p> Name Type Description Default <code>total_revenue</code> <code>array - like</code> <p>Total revenue for the period</p> required <code>total_users</code> <code>array - like</code> <p>Total number of users or customers</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Average Revenue Per User</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; average_revenue_per_user(10000, 500)\n20.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.burn_rate","title":"<code>burn_rate(starting_capital, ending_capital, months)</code>","text":"<p>Calculate Monthly Burn Rate.</p> <p>Burn Rate is the rate at which a company is losing money.</p> <p>Parameters:</p> Name Type Description Default <code>starting_capital</code> <code>array - like</code> <p>Capital at the start of the period</p> required <code>ending_capital</code> <code>array - like</code> <p>Capital at the end of the period</p> required <code>months</code> <code>array - like</code> <p>Number of months in the period</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Monthly Burn Rate</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; burn_rate(100000, 70000, 6)\n5000.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.churn_rate","title":"<code>churn_rate(customers_start, customers_end, new_customers)</code>","text":"<p>Calculate customer churn rate.</p> <p>Churn rate is the percentage of customers who stop using your product or service during a given time period.</p> <p>Parameters:</p> Name Type Description Default <code>customers_start</code> <code>array - like</code> <p>Number of customers at the start of the period</p> required <code>customers_end</code> <code>array - like</code> <p>Number of customers at the end of the period</p> required <code>new_customers</code> <code>array - like</code> <p>Number of new customers acquired during the period</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Churn rate as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; churn_rate(100, 90, 10)\n20.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.conversion_rate","title":"<code>conversion_rate(conversions, total_visitors)</code>","text":"<p>Calculate Conversion Rate.</p> <p>Conversion Rate is the percentage of visitors who take a desired action.</p> <p>Parameters:</p> Name Type Description Default <code>conversions</code> <code>array - like</code> <p>Number of conversions (desired actions taken)</p> required <code>total_visitors</code> <code>array - like</code> <p>Total number of visitors or users</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Conversion Rate as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; conversion_rate(50, 1000)\n5.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.customer_acquisition_cost","title":"<code>customer_acquisition_cost(marketing_costs, sales_costs, new_customers)</code>","text":"<p>Calculate Customer Acquisition Cost (CAC).</p> <p>CAC is the cost of convincing a potential customer to buy a product or service.</p> <p>Parameters:</p> Name Type Description Default <code>marketing_costs</code> <code>array - like</code> <p>Total marketing costs for the period</p> required <code>sales_costs</code> <code>array - like</code> <p>Total sales costs for the period</p> required <code>new_customers</code> <code>array - like</code> <p>Number of new customers acquired during the period</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Customer Acquisition Cost</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; customer_acquisition_cost(5000, 3000, 100)\n80.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.customer_effort_score","title":"<code>customer_effort_score(effort_ratings, max_rating=7)</code>","text":"<p>Calculate Customer Effort Score (CES).</p> <p>CES measures how much effort a customer has to exert to use a product or service. Lower scores are better.</p> <p>Parameters:</p> Name Type Description Default <code>effort_ratings</code> <code>array - like</code> <p>Array of customer effort ratings</p> required <code>max_rating</code> <code>array - like</code> <p>Maximum possible rating value</p> <code>7</code> <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Customer Effort Score (average)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; customer_effort_score([2, 3, 1, 2, 4])\n2.4\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.customer_engagement_score","title":"<code>customer_engagement_score(active_days, total_days)</code>","text":"<p>Calculate Customer Engagement Score.</p> <p>Customer Engagement Score measures how actively customers are using a product or service.</p> <p>Parameters:</p> Name Type Description Default <code>active_days</code> <code>array - like</code> <p>Number of days the customer was active</p> required <code>total_days</code> <code>array - like</code> <p>Total number of days in the period</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Customer Engagement Score as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; customer_engagement_score(15, 30)\n50.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.customer_lifetime_value","title":"<code>customer_lifetime_value(avg_revenue_per_customer, gross_margin, churn_rate_value, discount_rate=10.0)</code>","text":"<p>Calculate Customer Lifetime Value (CLV).</p> <p>CLV is the total worth to a business of a customer over the whole period of their relationship.</p> <p>Parameters:</p> Name Type Description Default <code>avg_revenue_per_customer</code> <code>array - like</code> <p>Average revenue per customer per period (e.g., monthly)</p> required <code>gross_margin</code> <code>array - like</code> <p>Gross margin percentage (0-100)</p> required <code>churn_rate_value</code> <code>array - like</code> <p>Churn rate percentage (0-100)</p> required <code>discount_rate</code> <code>array - like</code> <p>Annual discount rate for future cash flows (0-100)</p> <code>10.0</code> <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Customer Lifetime Value</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; customer_lifetime_value(100, 70, 5, 10)\n466.67\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.customer_satisfaction_score","title":"<code>customer_satisfaction_score(satisfaction_ratings, max_rating=5)</code>","text":"<p>Calculate Customer Satisfaction Score (CSAT).</p> <p>CSAT measures how satisfied customers are with a product, service, or interaction.</p> <p>Parameters:</p> Name Type Description Default <code>satisfaction_ratings</code> <code>array - like</code> <p>Array of customer satisfaction ratings</p> required <code>max_rating</code> <code>array - like</code> <p>Maximum possible rating value</p> <code>5</code> <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Customer Satisfaction Score as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; customer_satisfaction_score([4, 5, 3, 5, 4])\n84.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.daily_active_users_ratio","title":"<code>daily_active_users_ratio(daily_active_users, total_users)</code>","text":"<p>Calculate Daily Active Users (DAU) Ratio.</p> <p>DAU Ratio measures the percentage of total users who are active on a daily basis.</p> <p>Parameters:</p> Name Type Description Default <code>daily_active_users</code> <code>array - like</code> <p>Number of daily active users</p> required <code>total_users</code> <code>array - like</code> <p>Total number of users</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Daily Active Users Ratio as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; daily_active_users_ratio(500, 2000)\n25.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.expansion_revenue_rate","title":"<code>expansion_revenue_rate(upsell_revenue, cross_sell_revenue, revenue_start)</code>","text":"<p>Calculate Expansion Revenue Rate.</p> <p>Expansion Revenue Rate is the percentage of additional revenue generated from existing customers.</p> <p>Parameters:</p> Name Type Description Default <code>upsell_revenue</code> <code>array - like</code> <p>Revenue from upselling to existing customers</p> required <code>cross_sell_revenue</code> <code>array - like</code> <p>Revenue from cross-selling to existing customers</p> required <code>revenue_start</code> <code>array - like</code> <p>Revenue at the start of the period</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Expansion Revenue Rate as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; expansion_revenue_rate(1000, 500, 10000)\n15.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.feature_adoption_rate","title":"<code>feature_adoption_rate(users_adopting_feature, total_users)</code>","text":"<p>Calculate Feature Adoption Rate.</p> <p>Feature Adoption Rate measures the percentage of users who adopt a specific feature.</p> <p>Parameters:</p> Name Type Description Default <code>users_adopting_feature</code> <code>array - like</code> <p>Number of users who adopted the feature</p> required <code>total_users</code> <code>array - like</code> <p>Total number of users</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Feature Adoption Rate as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; feature_adoption_rate(300, 1000)\n30.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.gross_margin","title":"<code>gross_margin(revenue, cost_of_goods_sold)</code>","text":"<p>Calculate Gross Margin.</p> <p>Gross Margin is the percentage of revenue that exceeds the cost of goods sold.</p> <p>Parameters:</p> Name Type Description Default <code>revenue</code> <code>array - like</code> <p>Total revenue</p> required <code>cost_of_goods_sold</code> <code>array - like</code> <p>Cost of goods sold</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Gross Margin as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gross_margin(10000, 3000)\n70.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.ltv_cac_ratio","title":"<code>ltv_cac_ratio(ltv, cac)</code>","text":"<p>Calculate LTV:CAC Ratio.</p> <p>LTV:CAC Ratio is a metric that compares the lifetime value of a customer to the cost of acquiring that customer.</p> <p>Parameters:</p> Name Type Description Default <code>ltv</code> <code>array - like</code> <p>Customer Lifetime Value</p> required <code>cac</code> <code>array - like</code> <p>Customer Acquisition Cost</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>LTV:CAC Ratio</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ltv_cac_ratio(1000, 200)\n5.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.monthly_active_users_ratio","title":"<code>monthly_active_users_ratio(monthly_active_users, total_users)</code>","text":"<p>Calculate Monthly Active Users (MAU) Ratio.</p> <p>MAU Ratio measures the percentage of total users who are active on a monthly basis.</p> <p>Parameters:</p> Name Type Description Default <code>monthly_active_users</code> <code>array - like</code> <p>Number of monthly active users</p> required <code>total_users</code> <code>array - like</code> <p>Total number of users</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Monthly Active Users Ratio as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; monthly_active_users_ratio(1500, 2000)\n75.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.monthly_recurring_revenue","title":"<code>monthly_recurring_revenue(paying_customers, avg_revenue_per_customer)</code>","text":"<p>Calculate Monthly Recurring Revenue (MRR).</p> <p>MRR is the predictable total revenue generated by all the active subscriptions in a month.</p> <p>Parameters:</p> Name Type Description Default <code>paying_customers</code> <code>array - like</code> <p>Number of paying customers</p> required <code>avg_revenue_per_customer</code> <code>array - like</code> <p>Average revenue per customer per month</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Monthly Recurring Revenue</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; monthly_recurring_revenue(100, 50)\n5000.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.net_promoter_score","title":"<code>net_promoter_score(promoters, detractors, total_respondents)</code>","text":"<p>Calculate Net Promoter Score (NPS).</p> <p>NPS measures customer experience and predicts business growth.</p> <p>Parameters:</p> Name Type Description Default <code>promoters</code> <code>array - like</code> <p>Number of promoters (customers who rated 9-10)</p> required <code>detractors</code> <code>array - like</code> <p>Number of detractors (customers who rated 0-6)</p> required <code>total_respondents</code> <code>array - like</code> <p>Total number of survey respondents</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Net Promoter Score (ranges from -100 to 100)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; net_promoter_score(70, 10, 100)\n60.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.payback_period","title":"<code>payback_period(cac, avg_monthly_revenue, gross_margin)</code>","text":"<p>Calculate CAC Payback Period in months.</p> <p>CAC Payback Period is the number of months it takes to recover the cost of acquiring a customer.</p> <p>Parameters:</p> Name Type Description Default <code>cac</code> <code>array - like</code> <p>Customer Acquisition Cost</p> required <code>avg_monthly_revenue</code> <code>array - like</code> <p>Average monthly revenue per customer</p> required <code>gross_margin</code> <code>array - like</code> <p>Gross margin percentage (0-100)</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>CAC Payback Period in months</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; payback_period(1000, 100, 70)\n14.29\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.retention_rate","title":"<code>retention_rate(customers_start, customers_end, new_customers)</code>","text":"<p>Calculate customer retention rate.</p> <p>Retention rate is the percentage of customers who remain with your product or service over a given time period.</p> <p>Parameters:</p> Name Type Description Default <code>customers_start</code> <code>array - like</code> <p>Number of customers at the start of the period</p> required <code>customers_end</code> <code>array - like</code> <p>Number of customers at the end of the period</p> required <code>new_customers</code> <code>array - like</code> <p>Number of new customers acquired during the period</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Retention rate as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; retention_rate(100, 90, 10)\n80.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.revenue_churn_rate","title":"<code>revenue_churn_rate(revenue_start, revenue_end, new_revenue)</code>","text":"<p>Calculate Revenue Churn Rate.</p> <p>Revenue Churn Rate is the percentage of revenue lost from existing customers in a given period.</p> <p>Parameters:</p> Name Type Description Default <code>revenue_start</code> <code>array - like</code> <p>Revenue at the start of the period</p> required <code>revenue_end</code> <code>array - like</code> <p>Revenue at the end of the period</p> required <code>new_revenue</code> <code>array - like</code> <p>New revenue acquired during the period</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Revenue Churn Rate as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; revenue_churn_rate(10000, 9500, 1000)\n15.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.roi","title":"<code>roi(revenue, costs)</code>","text":"<p>Calculate Return on Investment (ROI).</p> <p>ROI measures the return on an investment relative to its cost.</p> <p>Parameters:</p> Name Type Description Default <code>revenue</code> <code>array - like</code> <p>Revenue or return from the investment</p> required <code>costs</code> <code>array - like</code> <p>Cost of the investment</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Return on Investment as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; roi(150, 100)\n50.0\n&gt;&gt;&gt; roi([150, 200, 250], [100, 120, 150])\narray([50., 66.67, 66.67])\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.runway","title":"<code>runway(current_capital, monthly_burn_rate)</code>","text":"<p>Calculate Runway in months.</p> <p>Runway is the amount of time a company has before it runs out of money.</p> <p>Parameters:</p> Name Type Description Default <code>current_capital</code> <code>array - like</code> <p>Current capital</p> required <code>monthly_burn_rate</code> <code>array - like</code> <p>Monthly burn rate</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Runway in months</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; runway(100000, 5000)\n20.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.stickiness_ratio","title":"<code>stickiness_ratio(daily_active_users, monthly_active_users)</code>","text":"<p>Calculate Stickiness Ratio (DAU/MAU).</p> <p>Stickiness Ratio measures how frequently active users engage with a product.</p> <p>Parameters:</p> Name Type Description Default <code>daily_active_users</code> <code>array - like</code> <p>Number of daily active users</p> required <code>monthly_active_users</code> <code>array - like</code> <p>Number of monthly active users</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Stickiness Ratio as a percentage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; stickiness_ratio(500, 1500)\n33.33\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.time_to_value","title":"<code>time_to_value(onboarding_time, setup_time, learning_time)</code>","text":"<p>Calculate Time to Value (TTV).</p> <p>Time to Value is the amount of time it takes for a customer to realize value from a product.</p> <p>Parameters:</p> Name Type Description Default <code>onboarding_time</code> <code>array - like</code> <p>Time spent on onboarding</p> required <code>setup_time</code> <code>array - like</code> <p>Time spent on setup</p> required <code>learning_time</code> <code>array - like</code> <p>Time spent on learning</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Time to Value</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; time_to_value(2, 3, 5)\n10.0\n</code></pre>"},{"location":"api/kpi/kpi/#pypulate.kpi.business_kpi.virality_coefficient","title":"<code>virality_coefficient(new_users, invites_sent, total_users)</code>","text":"<p>Calculate Virality Coefficient (K-factor).</p> <p>Virality Coefficient measures how many new users each existing user brings in.</p> <p>Parameters:</p> Name Type Description Default <code>new_users</code> <code>array - like</code> <p>Number of new users from invites</p> required <code>invites_sent</code> <code>array - like</code> <p>Number of invites sent</p> required <code>total_users</code> <code>array - like</code> <p>Total number of users</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Virality Coefficient</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; virality_coefficient(100, 500, 1000)\n0.1\n</code></pre>"},{"location":"api/parray/filters/","title":"Filters API Reference","text":"<p>This page documents the API for the filters module in Pypulate.</p>"},{"location":"api/parray/filters/#kalman-filters","title":"Kalman Filters","text":"<p>Apply a standard Kalman filter to a time series.</p> <p>The Kalman filter is an optimal estimator that infers parameters of interest from indirect, inaccurate and uncertain observations. It's recursive so new measurements can be processed as they arrive.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Input time series data</p> required <code>process_variance</code> <code>float</code> <p>Process noise variance (Q)</p> <code>1e-5</code> <code>measurement_variance</code> <code>float</code> <p>Measurement noise variance (R)</p> <code>1e-3</code> <code>initial_state</code> <code>float</code> <p>Initial state estimate. If None, the first data point is used</p> <code>None</code> <code>initial_covariance</code> <code>float</code> <p>Initial estimate covariance</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Filtered time series</p> <p>Apply an Extended Kalman Filter (EKF) to a time series with non-linear dynamics.</p> <p>The EKF is a nonlinear version of the Kalman filter that linearizes about the current mean and covariance. It's used when the state transition or observation models are non-linear.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Input time series data (observations)</p> required <code>state_transition_func</code> <code>callable</code> <p>Function that computes the state transition (f)</p> required <code>observation_func</code> <code>callable</code> <p>Function that computes the observation from state (h)</p> required <code>process_jacobian_func</code> <code>callable</code> <p>Function that computes the Jacobian of the state transition function</p> required <code>observation_jacobian_func</code> <code>callable</code> <p>Function that computes the Jacobian of the observation function</p> required <code>process_covariance</code> <code>ndarray</code> <p>Process noise covariance matrix (Q)</p> required <code>observation_covariance</code> <code>ndarray</code> <p>Observation noise covariance matrix (R)</p> required <code>initial_state</code> <code>ndarray</code> <p>Initial state estimate. If None, zeros are used</p> <code>None</code> <code>initial_covariance</code> <code>ndarray</code> <p>Initial estimate covariance matrix. If None, identity is used</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Filtered time series (state estimates)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.filters import extended_kalman_filter\n&gt;&gt;&gt; # Define non-linear system\n&gt;&gt;&gt; def state_transition(x):\n...     # Non-linear state transition function\n...     return np.array([x[0] + x[1], 0.5 * x[1]])\n&gt;&gt;&gt; def observation(x):\n...     # Non-linear observation function\n...     return np.array([np.sin(x[0])])\n&gt;&gt;&gt; def process_jacobian(x):\n...     # Jacobian of state transition function\n...     return np.array([[1, 1], [0, 0.5]])\n&gt;&gt;&gt; def observation_jacobian(x):\n...     # Jacobian of observation function\n...     return np.array([[np.cos(x[0]), 0]])\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; n = 100\n&gt;&gt;&gt; true_states = np.zeros((n, 2))\n&gt;&gt;&gt; true_states[0] = [0, 1]\n&gt;&gt;&gt; for i in range(1, n):\n...     true_states[i] = state_transition(true_states[i-1])\n&gt;&gt;&gt; observations = np.array([observation(x)[0] for x in true_states])\n&gt;&gt;&gt; observations += np.random.normal(0, 0.1, n)\n&gt;&gt;&gt; # Apply EKF\n&gt;&gt;&gt; Q = np.eye(2) * 0.01  # Process noise covariance\n&gt;&gt;&gt; R = np.array([[0.1]])  # Observation noise covariance\n&gt;&gt;&gt; filtered_states = extended_kalman_filter(\n...     observations, state_transition, observation,\n...     process_jacobian, observation_jacobian, Q, R\n... )\n</code></pre> <p>Apply an Unscented Kalman Filter (UKF) to a time series with non-linear dynamics.</p> <p>The UKF uses the unscented transform to pick a minimal set of sample points (sigma points) around the mean. These sigma points are then propagated through the non-linear functions, and the mean and covariance of the estimate are recovered.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Input time series data (observations)</p> required <code>state_transition_func</code> <code>callable</code> <p>Function that computes the state transition</p> required <code>observation_func</code> <code>callable</code> <p>Function that computes the observation from state</p> required <code>process_covariance</code> <code>ndarray</code> <p>Process noise covariance matrix (Q)</p> required <code>observation_covariance</code> <code>ndarray</code> <p>Observation noise covariance matrix (R)</p> required <code>initial_state</code> <code>ndarray</code> <p>Initial state estimate. If None, zeros are used</p> <code>None</code> <code>initial_covariance</code> <code>ndarray</code> <p>Initial estimate covariance matrix. If None, identity is used</p> <code>None</code> <code>alpha</code> <code>float</code> <p>Spread of sigma points around mean</p> <code>1e-3</code> <code>beta</code> <code>float</code> <p>Prior knowledge about distribution (2 is optimal for Gaussian)</p> <code>2.0</code> <code>kappa</code> <code>float</code> <p>Secondary scaling parameter</p> <code>0.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Filtered time series (state estimates)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.filters import unscented_kalman_filter\n&gt;&gt;&gt; # Define non-linear system\n&gt;&gt;&gt; def state_transition(x):\n...     # Non-linear state transition function\n...     return np.array([x[0] + x[1], 0.5 * x[1]])\n&gt;&gt;&gt; def observation(x):\n...     # Non-linear observation function\n...     return np.array([np.sin(x[0])])\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; n = 100\n&gt;&gt;&gt; true_states = np.zeros((n, 2))\n&gt;&gt;&gt; true_states[0] = [0, 1]\n&gt;&gt;&gt; for i in range(1, n):\n...     true_states[i] = state_transition(true_states[i-1])\n&gt;&gt;&gt; observations = np.array([observation(x)[0] for x in true_states])\n&gt;&gt;&gt; observations += np.random.normal(0, 0.1, n)\n&gt;&gt;&gt; # Apply UKF\n&gt;&gt;&gt; Q = np.eye(2) * 0.01  # Process noise covariance\n&gt;&gt;&gt; R = np.array([[0.1]])  # Observation noise covariance\n&gt;&gt;&gt; filtered_states = unscented_kalman_filter(\n...     observations, state_transition, observation, Q, R\n... )\n</code></pre>"},{"location":"api/parray/filters/#signal-filters","title":"Signal Filters","text":"<p>Apply a Butterworth filter to a time series.</p> <p>The Butterworth filter is a type of signal processing filter designed to have a frequency response as flat as possible in the passband.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Input time series data</p> required <code>cutoff</code> <code>float or tuple of float</code> <p>Cutoff frequency. For lowpass and highpass, this is a scalar. For bandpass and bandstop, this is a tuple of (low, high)</p> required <code>order</code> <code>int</code> <p>Filter order</p> <code>4</code> <code>filter_type</code> <code>str</code> <p>Filter type: 'lowpass', 'highpass', 'bandpass', or 'bandstop'</p> <code>'lowpass'</code> <code>fs</code> <code>float</code> <p>Sampling frequency</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Filtered time series</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.filters import butterworth_filter\n&gt;&gt;&gt; # Create noisy data\n&gt;&gt;&gt; x = np.linspace(0, 10, 1000)\n&gt;&gt;&gt; signal = np.sin(2 * np.pi * 0.05 * x) + 0.5 * np.sin(2 * np.pi * 0.25 * x)\n&gt;&gt;&gt; # Apply lowpass filter to remove high frequency component\n&gt;&gt;&gt; filtered = butterworth_filter(signal, cutoff=0.1, filter_type='lowpass', fs=1.0)\n</code></pre> <p>Apply a Chebyshev filter to a time series.</p> <p>The Chebyshev filter is a filter with steeper roll-off than the Butterworth but more passband ripple (type I) or stopband ripple (type II).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Input time series data</p> required <code>cutoff</code> <code>float or tuple of float</code> <p>Cutoff frequency. For lowpass and highpass, this is a scalar. For bandpass and bandstop, this is a tuple of (low, high)</p> required <code>order</code> <code>int</code> <p>Filter order</p> <code>4</code> <code>ripple</code> <code>float</code> <p>Maximum ripple allowed in the passband (type I) or stopband (type II)</p> <code>1.0</code> <code>filter_type</code> <code>str</code> <p>Filter type: 'lowpass', 'highpass', 'bandpass', or 'bandstop'</p> <code>'lowpass'</code> <code>fs</code> <code>float</code> <p>Sampling frequency</p> <code>1.0</code> <code>type_num</code> <code>int</code> <p>Type of Chebyshev filter: 1 for Type I, 2 for Type II</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Filtered time series</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.filters import chebyshev_filter\n&gt;&gt;&gt; # Create noisy data\n&gt;&gt;&gt; x = np.linspace(0, 10, 1000)\n&gt;&gt;&gt; signal = np.sin(2 * np.pi * 0.05 * x) + 0.5 * np.sin(2 * np.pi * 0.25 * x)\n&gt;&gt;&gt; # Apply Chebyshev type I lowpass filter\n&gt;&gt;&gt; filtered = chebyshev_filter(signal, cutoff=0.1, ripple=0.5, type_num=1)\n</code></pre> <p>Apply a Savitzky-Golay filter to a time series.</p> <p>The Savitzky-Golay filter is a digital filter that can be applied to a set of digital data points to smooth the data by increasing the signal-to-noise ratio without greatly distorting the signal.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Input time series data</p> required <code>window_length</code> <code>int</code> <p>Length of the filter window (must be odd)</p> <code>11</code> <code>polyorder</code> <code>int</code> <p>Order of the polynomial used to fit the samples (must be less than window_length)</p> <code>3</code> <code>deriv</code> <code>int</code> <p>Order of the derivative to compute</p> <code>0</code> <code>delta</code> <code>float</code> <p>Spacing of the samples to which the filter is applied</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Filtered time series</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.filters import savitzky_golay_filter\n&gt;&gt;&gt; # Create noisy data\n&gt;&gt;&gt; x = np.linspace(0, 10, 100)\n&gt;&gt;&gt; signal = np.sin(x) + np.random.normal(0, 0.1, len(x))\n&gt;&gt;&gt; # Apply Savitzky-Golay filter\n&gt;&gt;&gt; filtered = savitzky_golay_filter(signal, window_length=11, polyorder=3)\n</code></pre> <p>Apply a Wiener filter to a time series.</p> <p>The Wiener filter is a filter used to produce an estimate of a desired or target signal by linear filtering of an observed noisy signal.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Input time series data</p> required <code>mysize</code> <code>int or tuple of int</code> <p>Size of the filter window</p> <code>3</code> <code>noise</code> <code>float</code> <p>Estimate of the noise power. If None, it's estimated from the data</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Filtered time series</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.filters import wiener_filter\n&gt;&gt;&gt; # Create noisy data\n&gt;&gt;&gt; x = np.linspace(0, 10, 100)\n&gt;&gt;&gt; signal = np.sin(x) + np.random.normal(0, 0.1, len(x))\n&gt;&gt;&gt; # Apply Wiener filter\n&gt;&gt;&gt; filtered = wiener_filter(signal, mysize=5)\n</code></pre> <p>Apply a median filter to a time series.</p> <p>The median filter is a nonlinear digital filtering technique used to remove noise from a signal. It replaces each entry with the median of neighboring entries.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Input time series data</p> required <code>kernel_size</code> <code>int</code> <p>Size of the filter kernel</p> <code>3</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Filtered time series</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.filters import median_filter\n&gt;&gt;&gt; # Create noisy data with outliers\n&gt;&gt;&gt; x = np.linspace(0, 10, 100)\n&gt;&gt;&gt; signal = np.sin(x)\n&gt;&gt;&gt; signal[10] = 5  # Add outlier\n&gt;&gt;&gt; signal[50] = -5  # Add outlier\n&gt;&gt;&gt; # Apply median filter to remove outliers\n&gt;&gt;&gt; filtered = median_filter(signal, kernel_size=5)\n</code></pre> <p>Apply a Hampel filter to a time series.</p> <p>The Hampel filter is used to identify and replace outliers in a time series. It uses the median and the median absolute deviation (MAD) to identify outliers.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Input time series data</p> required <code>window_size</code> <code>int</code> <p>Size of the window (number of points on each side of the current point)</p> <code>5</code> <code>n_sigmas</code> <code>float</code> <p>Number of standard deviations to use for outlier detection</p> <code>3.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Filtered time series</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.filters import hampel_filter\n&gt;&gt;&gt; # Create noisy data with outliers\n&gt;&gt;&gt; x = np.linspace(0, 10, 100)\n&gt;&gt;&gt; signal = np.sin(x)\n&gt;&gt;&gt; signal[10] = 5  # Add outlier\n&gt;&gt;&gt; signal[50] = -5  # Add outlier\n&gt;&gt;&gt; # Apply Hampel filter to remove outliers\n&gt;&gt;&gt; filtered = hampel_filter(signal, window_size=5, n_sigmas=3.0)\n</code></pre> <p>Apply the Hodrick-Prescott filter to decompose a time series into trend and cycle components.</p> <p>The Hodrick-Prescott filter is a mathematical tool used in macroeconomics to separate the cyclical component of a time series from raw data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Input time series data</p> required <code>lambda_param</code> <code>float</code> <p>Smoothing parameter. The larger the value, the smoother the trend component</p> <code>1600.0</code> <p>Returns:</p> Type Description <code>tuple of np.ndarray</code> <p>Tuple containing (trend, cycle) components</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.filters import hodrick_prescott_filter\n&gt;&gt;&gt; # Create data with trend and cycle\n&gt;&gt;&gt; x = np.linspace(0, 10, 100)\n&gt;&gt;&gt; trend = 0.1 * x**2\n&gt;&gt;&gt; cycle = np.sin(2 * np.pi * 0.1 * x)\n&gt;&gt;&gt; data = trend + cycle\n&gt;&gt;&gt; # Apply Hodrick-Prescott filter\n&gt;&gt;&gt; trend_component, cycle_component = hodrick_prescott_filter(data, lambda_param=100)\n</code></pre> <p>Apply the Baxter-King bandpass filter to extract business cycle components.</p> <p>The Baxter-King filter is a bandpass filter used to extract business cycle components from macroeconomic time series.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Input time series data</p> required <code>low</code> <code>float</code> <p>Minimum period of oscillations (in number of observations)</p> <code>6</code> <code>high</code> <code>float</code> <p>Maximum period of oscillations (in number of observations)</p> <code>32</code> <code>K</code> <code>int</code> <p>Number of terms in the approximation (lead/lag)</p> <code>12</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Filtered time series (cycle component)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.filters import baxter_king_filter\n&gt;&gt;&gt; # Create data with trend and multiple cycles\n&gt;&gt;&gt; x = np.linspace(0, 100, 1000)\n&gt;&gt;&gt; trend = 0.01 * x\n&gt;&gt;&gt; long_cycle = np.sin(2 * np.pi * x / 100)  # Period of 100\n&gt;&gt;&gt; business_cycle = np.sin(2 * np.pi * x / 20)  # Period of 20\n&gt;&gt;&gt; short_cycle = np.sin(2 * np.pi * x / 5)  # Period of 5\n&gt;&gt;&gt; data = trend + long_cycle + business_cycle + short_cycle\n&gt;&gt;&gt; # Extract business cycle component (periods between 8 and 32)\n&gt;&gt;&gt; cycle = baxter_king_filter(data, low=8, high=32, K=12)\n</code></pre>"},{"location":"api/parray/filters/#adaptive-filters","title":"Adaptive Filters","text":"<p>Apply an adaptive Kalman filter to a time series.</p> <p>The adaptive Kalman filter automatically adjusts its parameters based on the observed data, making it more robust to changing dynamics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>process_variance_init</code> <code>float</code> <p>Initial process noise variance (Q)</p> <code>1e-5</code> <code>measurement_variance_init</code> <code>float</code> <p>Initial measurement noise variance (R)</p> <code>1e-3</code> <code>adaptation_rate</code> <code>float</code> <p>Rate at which the filter adapts to changes</p> <code>0.01</code> <code>window_size</code> <code>int</code> <p>Size of the window for innovation estimation</p> <code>10</code> <code>initial_state</code> <code>float</code> <p>Initial state estimate. If None, the first data point is used</p> <code>None</code> <code>initial_covariance</code> <code>float</code> <p>Initial estimate covariance</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Filtered time series</p> <p>Apply a Least Mean Squares (LMS) adaptive filter to a time series.</p> <p>The LMS algorithm is an adaptive filter that adjusts its coefficients to minimize the mean square error between the desired signal and the filter output.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>desired</code> <code>ndarray</code> <p>Desired signal. If None, a delayed version of the input is used</p> <code>None</code> <code>filter_length</code> <code>int</code> <p>Length of the adaptive filter</p> <code>5</code> <code>mu</code> <code>float</code> <p>Step size (learning rate) of the adaptation</p> <code>0.01</code> <code>initial_weights</code> <code>ndarray</code> <p>Initial filter weights. If None, zeros are used</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple of np.ndarray</code> <p>Tuple containing (filtered_data, filter_weights)</p> <p>Apply a Recursive Least Squares (RLS) adaptive filter to a time series.</p> <p>The RLS algorithm is an adaptive filter that recursively finds the filter coefficients that minimize a weighted linear least squares cost function related to the input signals.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>desired</code> <code>ndarray</code> <p>Desired signal. If None, a delayed version of the input is used</p> <code>None</code> <code>filter_length</code> <code>int</code> <p>Length of the adaptive filter</p> <code>5</code> <code>forgetting_factor</code> <code>float</code> <p>Forgetting factor (0 &lt; lambda &lt;= 1)</p> <code>0.99</code> <code>delta</code> <code>float</code> <p>Regularization parameter for the initial correlation matrix</p> <code>1.0</code> <code>initial_weights</code> <code>ndarray</code> <p>Initial filter weights. If None, zeros are used</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple of np.ndarray</code> <p>Tuple containing (filtered_data, filter_weights)</p>"},{"location":"api/parray/filters/#particle-filters","title":"Particle Filters","text":"<p>Apply a particle filter to a time series.</p> <p>The particle filter is a sequential Monte Carlo method that uses a set of particles (samples) to represent the posterior distribution of some stochastic process given noisy and/or partial observations.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Input time series data (observations)</p> required <code>state_transition_func</code> <code>callable</code> <p>Function that propagates particles through the state transition model</p> required <code>observation_func</code> <code>callable</code> <p>Function that computes the expected observation from a state</p> required <code>process_noise_func</code> <code>callable</code> <p>Function that adds process noise to particles</p> required <code>observation_likelihood_func</code> <code>callable</code> <p>Function that computes the likelihood of an observation given a state</p> required <code>n_particles</code> <code>int</code> <p>Number of particles</p> <code>100</code> <code>initial_state_func</code> <code>callable</code> <p>Function that generates initial particles. If None, a default is used</p> <code>None</code> <code>resample_threshold</code> <code>float</code> <p>Threshold for effective sample size ratio below which resampling occurs</p> <code>0.5</code> <p>Returns:</p> Type Description <code>tuple of np.ndarray</code> <p>Tuple containing (filtered_states, particle_weights)</p> <p>Apply a bootstrap particle filter to a time series.</p> <p>The bootstrap particle filter is a simplified version of the particle filter that resamples at every step and uses the state transition prior as the proposal.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Input time series data (observations)</p> required <code>state_transition_func</code> <code>callable</code> <p>Function that propagates particles through the state transition model</p> required <code>observation_func</code> <code>callable</code> <p>Function that computes the expected observation from a state</p> required <code>process_noise_std</code> <code>float</code> <p>Standard deviation of the process noise</p> <code>0.1</code> <code>observation_noise_std</code> <code>float</code> <p>Standard deviation of the observation noise</p> <code>0.1</code> <code>n_particles</code> <code>int</code> <p>Number of particles</p> <code>100</code> <code>initial_state_mean</code> <code>float</code> <p>Mean of the initial state distribution. If None, the first observation is used</p> <code>None</code> <code>initial_state_std</code> <code>float</code> <p>Standard deviation of the initial state distribution</p> <code>1.0</code> <p>Returns:</p> Type Description <code>tuple of np.ndarray</code> <p>Tuple containing (filtered_states, particle_weights)</p>"},{"location":"api/parray/moving_averages/","title":"Moving Averages API Reference","text":"<p>Moving Averages Module</p> <p>This module provides various moving average implementations for financial time series analysis. All functions use numpy arrays for input and output to ensure high performance.</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.alma","title":"<code>alma(data, period=9, offset=0.85, sigma=6.0)</code>","text":"<p>Arnaud Legoux Moving Average (ALMA)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <code>period</code> <code>int</code> <p>Window size for the moving average</p> <code>9</code> <code>offset</code> <code>float</code> <p>Controls tradeoff between smoothness and responsiveness (0-1)</p> <code>0.85</code> <code>sigma</code> <code>float</code> <p>Controls the filter width</p> <code>6.0</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Arnaud Legoux moving average values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If period is not positive If offset is not between 0 and 1 If sigma is not positive</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.ema","title":"<code>ema(data, period=9, alpha=None)</code>","text":"<p>Exponential Moving Average (EMA)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <code>period</code> <code>int</code> <p>Window size for the moving average</p> <code>9</code> <code>alpha</code> <code>float</code> <p>Smoothing factor. If None, alpha = 2/(period+1)</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Exponential moving average values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If period is not positive</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.frama","title":"<code>frama(data, period=9, fc_period=198)</code>","text":"<p>Fractal Adaptive Moving Average (FRAMA)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <code>period</code> <code>int</code> <p>Window size for the moving average</p> <code>9</code> <code>fc_period</code> <code>int</code> <p>Fractal cycle period</p> <code>198</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Fractal adaptive moving average values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If period or fc_period is not positive</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.hma","title":"<code>hma(data, period=9)</code>","text":"<p>Hull Moving Average (HMA)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <code>period</code> <code>int</code> <p>Window size for the moving average</p> <code>9</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Hull moving average values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If period is not positive</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.jma","title":"<code>jma(data, period=9, phase=0)</code>","text":"<p>Jurik Moving Average (JMA)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <code>period</code> <code>int</code> <p>Window size for the moving average</p> <code>9</code> <code>phase</code> <code>float</code> <p>Phase parameter (-100 to 100)</p> <code>0</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Jurik moving average values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If period is not positive If phase is not between -100 and 100</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.kama","title":"<code>kama(data, period=9, fast_period=2, slow_period=30)</code>","text":"<p>Kaufman Adaptive Moving Average (KAMA)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <code>period</code> <code>int</code> <p>Window size for the efficiency ratio calculation</p> <code>9</code> <code>fast_period</code> <code>int</code> <p>Fast EMA period</p> <code>2</code> <code>slow_period</code> <code>int</code> <p>Slow EMA period</p> <code>30</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Kaufman adaptive moving average values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If period is not positive If fast_period is not positive If slow_period is not positive If fast_period is not less than slow_period</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.laguerre_filter","title":"<code>laguerre_filter(data, gamma=0.8)</code>","text":"<p>Laguerre Filter</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <code>gamma</code> <code>float</code> <p>Damping factor (0-1)</p> <code>0.8</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Laguerre filter values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If gamma is not between 0 and 1</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.lsma","title":"<code>lsma(data, period=9)</code>","text":"<p>Least Squares Moving Average (LSMA)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <code>period</code> <code>int</code> <p>Window size for the moving average</p> <code>9</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Least squares moving average values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If period is not positive</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.mcginley_dynamic","title":"<code>mcginley_dynamic(data, period=9, k=0.6)</code>","text":"<p>McGinley Dynamic Indicator</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <code>period</code> <code>int</code> <p>Window size for the moving average</p> <code>9</code> <code>k</code> <code>float</code> <p>Adjustment factor</p> <code>0.6</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>McGinley dynamic indicator values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If period is not positive If k is not between 0 and 1</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.modular_filter","title":"<code>modular_filter(data, period=9, phase=0.5)</code>","text":"<p>Modular Filter</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <code>period</code> <code>int</code> <p>Window size for the filter</p> <code>9</code> <code>phase</code> <code>float</code> <p>Phase parameter (0-1)</p> <code>0.5</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Modular filter values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If period is not positive If phase is not between 0 and 1</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.rdma","title":"<code>rdma(data)</code>","text":"<p>Rex Dog Moving Average (RDMA)</p> <p>This implementation follows the original RexDog definition, which is the average of six SMAs with periods 5, 9, 24, 50, 100, and 200.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Rex Dog moving average values</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.sma","title":"<code>sma(data, period=9)</code>","text":"<p>Simple Moving Average (SMA)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <code>period</code> <code>int</code> <p>Window size for the moving average</p> <code>9</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Simple moving average values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If period is not positive</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.smma","title":"<code>smma(data, period=9)</code>","text":"<p>Smoothed Moving Average (SMMA) or Running Moving Average (RMA)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <code>period</code> <code>int</code> <p>Window size for the moving average</p> <code>9</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Smoothed moving average values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If period is not positive</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.t3","title":"<code>t3(data, period=9, vfactor=0.7)</code>","text":"<p>Tillson T3 Moving Average</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <code>period</code> <code>int</code> <p>Window size for the moving average</p> <code>9</code> <code>vfactor</code> <code>float</code> <p>Volume factor (0-1)</p> <code>0.7</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>T3 moving average values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If period is not positive If volume factor is not between 0 and 1</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.tma","title":"<code>tma(data, period=9)</code>","text":"<p>Triangular Moving Average (TMA)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <code>period</code> <code>int</code> <p>Window size for the moving average</p> <code>9</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Triangular moving average values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If period is not positive</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.vama","title":"<code>vama(data, volatility, period=9)</code>","text":"<p>Volatility-Adjusted Moving Average (VAMA)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <code>volatility</code> <code>ArrayLike</code> <p>Volatility data corresponding to price data</p> required <code>period</code> <code>int</code> <p>Window size for the moving average</p> <code>9</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Volatility-adjusted moving average values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If period is not positive If price and volatility arrays have different lengths</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.vwma","title":"<code>vwma(data, volume, period=9)</code>","text":"<p>Volume-Weighted Moving Average (VWMA)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <code>volume</code> <code>ArrayLike</code> <p>Volume data corresponding to price data</p> required <code>period</code> <code>int</code> <p>Window size for the moving average</p> <code>9</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Volume-weighted moving average values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If period is not positive If price and volume arrays have different lengths</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.wma","title":"<code>wma(data, period=9)</code>","text":"<p>Weighted Moving Average (WMA)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <code>period</code> <code>int</code> <p>Window size for the moving average</p> <code>9</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Weighted moving average values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If period is not positive</p>"},{"location":"api/parray/moving_averages/#pypulate.moving_averages.movingaverages.zlma","title":"<code>zlma(data, period=9)</code>","text":"<p>Zero-Lag Moving Average (ZLMA)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input price data</p> required <code>period</code> <code>int</code> <p>Window size for the moving average</p> <code>9</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Zero-lag moving average values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If period is not positive</p>"},{"location":"api/parray/parray/","title":"Parray API Reference","text":"<p>This page documents the API for the <code>Parray</code> class in Pypulate.</p>"},{"location":"api/parray/parray/#class-definition","title":"Class Definition","text":""},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray","title":"<code>pypulate.dtypes.parray.Parray</code>","text":"<p>               Bases: <code>ndarray</code></p> <p>A wrapper around numpy arrays that provides method chaining for financial analysis.</p> <p>This class allows for fluent method chaining like: data.ema(9).sma(20)</p> <p>It also supports parallel processing and GPU acceleration for improved performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.dtypes import Parray\n&gt;&gt;&gt; data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; ts = Parray(data)\n&gt;&gt;&gt; result = ts.ema(3).sma(2)\n&gt;&gt;&gt; # Enable GPU acceleration\n&gt;&gt;&gt; ts.enable_gpu()\n&gt;&gt;&gt; result_gpu = ts.ema(3).sma(2)\n</code></pre>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.__new__","title":"<code>__new__(input_array, memory_optimized=False)</code>","text":"<p>Create a new Parray instance.</p> <p>Parameters:</p> Name Type Description Default <code>input_array</code> <code>array - like</code> <p>Input data to convert to a Parray</p> required <code>memory_optimized</code> <code>bool</code> <p>If True, use the smallest possible dtype that can represent the data</p> <code>False</code> <p>Returns:</p> Type Description <code>Parray</code> <p>A new Parray instance</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.__array_finalize__","title":"<code>__array_finalize__(obj)</code>","text":"<p>Finalize the creation of the array.</p> <p>This method is called whenever a new Parray is created.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>ndarray</code> <p>The array from which the new array was created</p> required"},{"location":"api/parray/parray/#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/parray/parray/#gpu-acceleration","title":"GPU Acceleration","text":"<p>               Bases: <code>ndarray</code></p> <p>A wrapper around numpy arrays that provides method chaining for financial analysis.</p> <p>This class allows for fluent method chaining like: data.ema(9).sma(20)</p> <p>It also supports parallel processing and GPU acceleration for improved performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.dtypes import Parray\n&gt;&gt;&gt; data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; ts = Parray(data)\n&gt;&gt;&gt; result = ts.ema(3).sma(2)\n&gt;&gt;&gt; # Enable GPU acceleration\n&gt;&gt;&gt; ts.enable_gpu()\n&gt;&gt;&gt; result_gpu = ts.ema(3).sma(2)\n</code></pre>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.gpu","title":"<code>gpu</code>  <code>property</code>","text":"<p>Get the GPU acceleration flag.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if GPU acceleration is enabled, False otherwise</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.is_gpu_available","title":"<code>is_gpu_available()</code>  <code>staticmethod</code>","text":"<p>Check if GPU acceleration is available.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if GPU acceleration is available, False otherwise</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.get_gpu_info","title":"<code>get_gpu_info()</code>  <code>staticmethod</code>","text":"<p>Get information about available GPUs.</p> <p>Returns:</p> Type Description <code>dict or None</code> <p>Dictionary containing GPU information if available, None otherwise</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.enable_gpu","title":"<code>enable_gpu()</code>","text":"<p>Enable GPU acceleration for operations that support it.</p> <p>This requires cupy to be installed. If cupy is not available, a warning will be printed and GPU acceleration will not be enabled.</p> <p>Returns:</p> Type Description <code>Parray</code> <p>Self with GPU acceleration enabled (if available)</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.disable_gpu","title":"<code>disable_gpu()</code>","text":"<p>Disable GPU acceleration.</p> <p>Returns:</p> Type Description <code>Parray</code> <p>Self with GPU acceleration disabled</p>"},{"location":"api/parray/parray/#parallel-processing","title":"Parallel Processing","text":"<p>               Bases: <code>ndarray</code></p> <p>A wrapper around numpy arrays that provides method chaining for financial analysis.</p> <p>This class allows for fluent method chaining like: data.ema(9).sma(20)</p> <p>It also supports parallel processing and GPU acceleration for improved performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.dtypes import Parray\n&gt;&gt;&gt; data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; ts = Parray(data)\n&gt;&gt;&gt; result = ts.ema(3).sma(2)\n&gt;&gt;&gt; # Enable GPU acceleration\n&gt;&gt;&gt; ts.enable_gpu()\n&gt;&gt;&gt; result_gpu = ts.ema(3).sma(2)\n</code></pre>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.parallel","title":"<code>parallel</code>  <code>property</code>","text":"<p>Get the parallel processing flag.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if parallel processing is enabled, False otherwise</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.enable_parallel","title":"<code>enable_parallel(num_workers=None, chunk_size=None)</code>","text":"<p>Enable parallel processing for operations that support it.</p> <p>Parameters:</p> Name Type Description Default <code>num_workers</code> <code>int</code> <p>Number of worker processes/threads to use. If None, use the default number of workers.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>Size of chunks to process in parallel. If None, use the default chunk size.</p> <code>None</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Self with parallel processing enabled</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.disable_parallel","title":"<code>disable_parallel()</code>","text":"<p>Disable parallel processing.</p> <p>Returns:</p> Type Description <code>Parray</code> <p>Self with parallel processing disabled</p>"},{"location":"api/parray/parray/#memory-optimization","title":"Memory Optimization","text":"<p>               Bases: <code>ndarray</code></p> <p>A wrapper around numpy arrays that provides method chaining for financial analysis.</p> <p>This class allows for fluent method chaining like: data.ema(9).sma(20)</p> <p>It also supports parallel processing and GPU acceleration for improved performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.dtypes import Parray\n&gt;&gt;&gt; data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; ts = Parray(data)\n&gt;&gt;&gt; result = ts.ema(3).sma(2)\n&gt;&gt;&gt; # Enable GPU acceleration\n&gt;&gt;&gt; ts.enable_gpu()\n&gt;&gt;&gt; result_gpu = ts.ema(3).sma(2)\n</code></pre>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.optimize_memory","title":"<code>optimize_memory()</code>","text":"<p>Optimize memory usage by using the smallest possible dtype.</p> <p>Returns:</p> Type Description <code>Parray</code> <p>A new Parray with optimized memory usage</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.disable_memory_optimization","title":"<code>disable_memory_optimization()</code>","text":"<p>Disable memory optimization.</p> <p>Returns:</p> Type Description <code>Parray</code>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.to_chunks","title":"<code>to_chunks(chunk_size=None)</code>","text":"<p>Split the array into chunks.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Size of each chunk. If None, use the default chunk size.</p> <code>None</code> <p>Returns:</p> Type Description <code>list of Parray</code> <p>List of chunks</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.from_chunks","title":"<code>from_chunks(chunks, axis=0)</code>  <code>classmethod</code>","text":"<p>Create a Parray from chunks of data.</p> <p>This is useful for processing large datasets that don't fit in memory.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list of array-like</code> <p>Chunks of data to combine</p> required <code>axis</code> <code>int</code> <p>Axis along which to concatenate the chunks</p> <code>0</code> <p>Returns:</p> Type Description <code>Parray</code> <p>A new Parray created from the chunks</p>"},{"location":"api/parray/parray/#moving-averages","title":"Moving Averages","text":"<p>               Bases: <code>ndarray</code></p> <p>A wrapper around numpy arrays that provides method chaining for financial analysis.</p> <p>This class allows for fluent method chaining like: data.ema(9).sma(20)</p> <p>It also supports parallel processing and GPU acceleration for improved performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.dtypes import Parray\n&gt;&gt;&gt; data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; ts = Parray(data)\n&gt;&gt;&gt; result = ts.ema(3).sma(2)\n&gt;&gt;&gt; # Enable GPU acceleration\n&gt;&gt;&gt; ts.enable_gpu()\n&gt;&gt;&gt; result_gpu = ts.ema(3).sma(2)\n</code></pre>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.sma","title":"<code>sma(period=9)</code>","text":"<p>Apply Simple Moving Average</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.ema","title":"<code>ema(period=9, alpha=None)</code>","text":"<p>Apply Exponential Moving Average</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.wma","title":"<code>wma(period=9)</code>","text":"<p>Apply Weighted Moving Average</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.tma","title":"<code>tma(period=9)</code>","text":"<p>Apply Triangular Moving Average</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.smma","title":"<code>smma(period=9)</code>","text":"<p>Apply Smoothed Moving Average</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.zlma","title":"<code>zlma(period=9)</code>","text":"<p>Apply Zero-Lag Moving Average</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.hma","title":"<code>hma(period=9)</code>","text":"<p>Apply Hull Moving Average</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.kama","title":"<code>kama(period=9, fast_period=2, slow_period=30)</code>","text":"<p>Apply Kaufman Adaptive Moving Average</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.t3","title":"<code>t3(period=9, vfactor=0.7)</code>","text":"<p>Apply Tillson T3 Moving Average</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.frama","title":"<code>frama(period=9, fc_period=198)</code>","text":"<p>Apply Fractal Adaptive Moving Average</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.mcginley_dynamic","title":"<code>mcginley_dynamic(period=9, k=0.6)</code>","text":"<p>Apply McGinley Dynamic Indicator</p>"},{"location":"api/parray/parray/#momentum-indicators","title":"Momentum Indicators","text":"<p>               Bases: <code>ndarray</code></p> <p>A wrapper around numpy arrays that provides method chaining for financial analysis.</p> <p>This class allows for fluent method chaining like: data.ema(9).sma(20)</p> <p>It also supports parallel processing and GPU acceleration for improved performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.dtypes import Parray\n&gt;&gt;&gt; data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; ts = Parray(data)\n&gt;&gt;&gt; result = ts.ema(3).sma(2)\n&gt;&gt;&gt; # Enable GPU acceleration\n&gt;&gt;&gt; ts.enable_gpu()\n&gt;&gt;&gt; result_gpu = ts.ema(3).sma(2)\n</code></pre>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.momentum","title":"<code>momentum(period=14)</code>","text":"<p>Calculate momentum over a specified period.</p> <p>Momentum measures the amount that a price has changed over a given period.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>int</code> <p>Number of periods to calculate momentum</p> <code>14</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Momentum values</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.roc","title":"<code>roc(period=14)</code>","text":"<p>Calculate Rate of Change (ROC) over a specified period.</p> <p>ROC measures the percentage change in price over a given period.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>int</code> <p>Number of periods to calculate ROC</p> <code>14</code> <p>Returns:</p> Type Description <code>Parray</code> <p>ROC values in percentage</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.percent_change","title":"<code>percent_change(periods=1)</code>","text":"<p>Calculate percentage change between consecutive periods.</p> <p>Parameters:</p> Name Type Description Default <code>periods</code> <code>int</code> <p>Number of periods to calculate change over</p> <code>1</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Percentage change values</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.difference","title":"<code>difference(periods=1)</code>","text":"<p>Calculate difference between consecutive values.</p> <p>Parameters:</p> Name Type Description Default <code>periods</code> <code>int</code> <p>Number of periods to calculate difference over</p> <code>1</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Difference values</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.rsi","title":"<code>rsi(period=14, smoothing_type='sma')</code>","text":"<p>Calculate Relative Strength Index (RSI) over a specified period.</p> <p>RSI measures the speed and change of price movements, indicating overbought (&gt;70) or oversold (&lt;30) conditions.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>int</code> <p>Number of periods to calculate RSI</p> <code>14</code> <code>smoothing_type</code> <code>str</code> <p>Type of smoothing to use: 'sma' (Simple Moving Average) or  'ema' (Exponential Moving Average)</p> <code>'sma'</code> <p>Returns:</p> Type Description <code>Parray</code> <p>RSI values (0-100)</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.macd","title":"<code>macd(fast_period=12, slow_period=26, signal_period=9)</code>","text":"<p>Calculate Moving Average Convergence Divergence (MACD).</p> <p>MACD is a trend-following momentum indicator that shows the relationship between two moving averages of a security's price.</p> <p>Parameters:</p> Name Type Description Default <code>fast_period</code> <code>int</code> <p>Period for the fast EMA</p> <code>12</code> <code>slow_period</code> <code>int</code> <p>Period for the slow EMA</p> <code>26</code> <code>signal_period</code> <code>int</code> <p>Period for the signal line (EMA of MACD line)</p> <code>9</code> <p>Returns:</p> Type Description <code>tuple of Parray</code> <p>Tuple containing (macd_line, signal_line, histogram)</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.stochastic_oscillator","title":"<code>stochastic_oscillator(high, low, k_period=14, d_period=3)</code>","text":"<p>Calculate Stochastic Oscillator.</p> <p>The Stochastic Oscillator is a momentum indicator that shows the location of the close relative to the high-low range over a set number of periods.</p> <p>Parameters:</p> Name Type Description Default <code>high</code> <code>ndarray</code> <p>High prices. If None, assumes self contains close prices and high=low=self</p> required <code>low</code> <code>ndarray</code> <p>Low prices. If None, assumes self contains close prices and high=low=self</p> required <code>k_period</code> <code>int</code> <p>Number of periods for %K</p> <code>14</code> <code>d_period</code> <code>int</code> <p>Number of periods for %D (moving average of %K)</p> <code>3</code> <p>Returns:</p> Type Description <code>tuple of Parray</code> <p>Tuple containing (%K, %D)</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.tsi","title":"<code>tsi(long_period=25, short_period=13, signal_period=7)</code>","text":"<p>Calculate True Strength Index (TSI).</p> <p>TSI is a momentum oscillator that helps identify trends and reversals.</p> <p>Parameters:</p> Name Type Description Default <code>long_period</code> <code>int</code> <p>Long period for double smoothing</p> <code>25</code> <code>short_period</code> <code>int</code> <p>Short period for double smoothing</p> <code>13</code> <code>signal_period</code> <code>int</code> <p>Period for the signal line</p> <code>7</code> <p>Returns:</p> Type Description <code>tuple of Parray</code> <p>Tuple containing (tsi_line, signal_line)</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.williams_r","title":"<code>williams_r(high=None, low=None, period=14)</code>","text":"<p>Calculate Williams %R.</p> <p>Williams %R is a momentum indicator that measures overbought and oversold levels.</p> <p>Parameters:</p> Name Type Description Default <code>high</code> <code>ndarray</code> <p>High prices. If None, assumes self contains close prices and high=low=self</p> <code>None</code> <code>low</code> <code>ndarray</code> <p>Low prices. If None, assumes self contains close prices and high=low=self</p> <code>None</code> <code>period</code> <code>int</code> <p>Number of periods for calculation</p> <code>14</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Williams %R values (-100 to 0)</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.cci","title":"<code>cci(period=20, constant=0.015)</code>","text":"<p>Calculate Commodity Channel Index (CCI).</p> <p>CCI measures the current price level relative to an average price level over a given period.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>int</code> <p>Number of periods for calculation</p> <code>20</code> <code>constant</code> <code>float</code> <p>Scaling constant</p> <code>0.015</code> <p>Returns:</p> Type Description <code>Parray</code> <p>CCI values</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.adx","title":"<code>adx(period=14)</code>","text":"<p>Calculate Average Directional Index (ADX).</p> <p>ADX measures the strength of a trend.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>int</code> <p>Number of periods for calculation</p> <code>14</code> <p>Returns:</p> Type Description <code>Parray</code> <p>ADX values</p>"},{"location":"api/parray/parray/#volatility-indicators","title":"Volatility Indicators","text":"<p>               Bases: <code>ndarray</code></p> <p>A wrapper around numpy arrays that provides method chaining for financial analysis.</p> <p>This class allows for fluent method chaining like: data.ema(9).sma(20)</p> <p>It also supports parallel processing and GPU acceleration for improved performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.dtypes import Parray\n&gt;&gt;&gt; data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; ts = Parray(data)\n&gt;&gt;&gt; result = ts.ema(3).sma(2)\n&gt;&gt;&gt; # Enable GPU acceleration\n&gt;&gt;&gt; ts.enable_gpu()\n&gt;&gt;&gt; result_gpu = ts.ema(3).sma(2)\n</code></pre>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.historical_volatility","title":"<code>historical_volatility(period=21, annualization_factor=252)</code>","text":"<p>Calculate historical volatility over a specified period.</p> <p>Historical volatility is the standard deviation of log returns, typically annualized.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>int</code> <p>Number of periods to calculate volatility</p> <code>21</code> <code>annualization_factor</code> <code>int</code> <p>Factor to annualize volatility (252 for daily data, 52 for weekly, 12 for monthly)</p> <code>252</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Historical volatility values as percentage</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.atr","title":"<code>atr(high=None, low=None, period=14)</code>","text":"<p>Calculate Average True Range (ATR) over a specified period.</p> <p>ATR measures market volatility by decomposing the entire range of an asset price.</p> <p>Parameters:</p> Name Type Description Default <code>high</code> <code>ndarray</code> <p>High prices. If None, assumes self contains close prices and high=low=close</p> <code>None</code> <code>low</code> <code>ndarray</code> <p>Low prices. If None, assumes self contains close prices and high=low=close</p> <code>None</code> <code>period</code> <code>int</code> <p>Number of periods to calculate ATR</p> <code>14</code> <p>Returns:</p> Type Description <code>Parray</code> <p>ATR values</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.bollinger_bands","title":"<code>bollinger_bands(period=20, std_dev=2.0)</code>","text":"<p>Calculate Bollinger Bands over a specified period.</p> <p>Bollinger Bands consist of a middle band (SMA), an upper band (SMA + kstd), and a lower band (SMA - kstd).</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>int</code> <p>Number of periods for the moving average</p> <code>20</code> <code>std_dev</code> <code>float</code> <p>Number of standard deviations for the upper and lower bands</p> <code>2.0</code> <p>Returns:</p> Type Description <code>tuple of Parray</code> <p>Tuple containing (upper_band, middle_band, lower_band)</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.keltner_channels","title":"<code>keltner_channels(high=None, low=None, period=20, atr_period=10, multiplier=2.0)</code>","text":"<p>Calculate Keltner Channels over a specified period.</p> <p>Keltner Channels consist of a middle band (EMA), an upper band (EMA + kATR), and a lower band (EMA - kATR).</p> <p>Parameters:</p> Name Type Description Default <code>high</code> <code>ndarray</code> <p>High prices. If None, assumes self contains close prices and high=low=close</p> <code>None</code> <code>low</code> <code>ndarray</code> <p>Low prices. If None, assumes self contains close prices and high=low=close</p> <code>None</code> <code>period</code> <code>int</code> <p>Number of periods for the EMA</p> <code>20</code> <code>atr_period</code> <code>int</code> <p>Number of periods for the ATR</p> <code>10</code> <code>multiplier</code> <code>float</code> <p>Multiplier for the ATR</p> <code>2.0</code> <p>Returns:</p> Type Description <code>tuple of Parray</code> <p>Tuple containing (upper_channel, middle_channel, lower_channel)</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.donchian_channels","title":"<code>donchian_channels(high=None, low=None, period=20)</code>","text":"<p>Calculate Donchian Channels over a specified period.</p> <p>Donchian Channels consist of an upper band (highest high), a lower band (lowest low), and a middle band (average of upper and lower).</p> <p>Parameters:</p> Name Type Description Default <code>high</code> <code>ndarray</code> <p>High prices. If None, uses self</p> <code>None</code> <code>low</code> <code>ndarray</code> <p>Low prices. If None, uses self</p> <code>None</code> <code>period</code> <code>int</code> <p>Number of periods for the channels</p> <code>20</code> <p>Returns:</p> Type Description <code>tuple of Parray</code> <p>Tuple containing (upper_channel, middle_channel, lower_channel)</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.volatility_ratio","title":"<code>volatility_ratio(period=21, smooth_period=5)</code>","text":"<p>Calculate Volatility Ratio over a specified period.</p> <p>Volatility Ratio compares recent volatility to historical volatility. Values above 1 indicate increasing volatility, values below 1 indicate decreasing volatility.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>int</code> <p>Number of periods for historical volatility</p> <code>21</code> <code>smooth_period</code> <code>int</code> <p>Number of periods to smooth the ratio</p> <code>5</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Volatility Ratio values</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.typical_price","title":"<code>typical_price(high, low)</code>","text":"<p>Calculate the typical price from close, high, and low prices.</p> <p>Parameters:</p> Name Type Description Default <code>high</code> <code>ndarray</code> <p>High prices. If None, uses self</p> required <code>low</code> <code>ndarray</code> <p>Low prices. If None, uses self</p> required <p>Returns:</p> Type Description <code>Parray</code> <p>Typical price values</p>"},{"location":"api/parray/parray/#statistical-functions","title":"Statistical Functions","text":"<p>               Bases: <code>ndarray</code></p> <p>A wrapper around numpy arrays that provides method chaining for financial analysis.</p> <p>This class allows for fluent method chaining like: data.ema(9).sma(20)</p> <p>It also supports parallel processing and GPU acceleration for improved performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.dtypes import Parray\n&gt;&gt;&gt; data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; ts = Parray(data)\n&gt;&gt;&gt; result = ts.ema(3).sma(2)\n&gt;&gt;&gt; # Enable GPU acceleration\n&gt;&gt;&gt; ts.enable_gpu()\n&gt;&gt;&gt; result_gpu = ts.ema(3).sma(2)\n</code></pre>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.rolling_max","title":"<code>rolling_max(period=14)</code>","text":"<p>Calculate rolling maximum over a specified period.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>int</code> <p>Window size for rolling maximum</p> <code>14</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Rolling maximum values</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.rolling_min","title":"<code>rolling_min(period=14)</code>","text":"<p>Calculate rolling minimum over a specified period.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>int</code> <p>Window size for rolling minimum</p> <code>14</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Rolling minimum values</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.rolling_std","title":"<code>rolling_std(period=14)</code>","text":"<p>Calculate rolling standard deviation over a specified period.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>int</code> <p>Window size for rolling standard deviation</p> <code>14</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Rolling standard deviation values</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.rolling_var","title":"<code>rolling_var(period=14)</code>","text":"<p>Calculate rolling variance over a specified period.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>int</code> <p>Window size for rolling variance</p> <code>14</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Rolling variance values</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.zscore","title":"<code>zscore(period=14)</code>","text":"<p>Calculate rolling z-score over a specified period.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>int</code> <p>Window size for rolling z-score calculation</p> <code>14</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Rolling z-score values</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.descriptive_stats","title":"<code>descriptive_stats()</code>","text":"<p>Calculate descriptive statistics for data.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of statistics including mean, median, std, min, max, etc.</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.correlation_matrix","title":"<code>correlation_matrix(method='pearson', min_periods=1)</code>","text":"<p>Calculate correlation matrix for multivariate data.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Correlation method: 'pearson', 'spearman', or 'kendall'</p> <code>'pearson'</code> <code>min_periods</code> <code>int</code> <p>Minimum number of observations required per pair of columns</p> <code>1</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Correlation matrix</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.covariance_matrix","title":"<code>covariance_matrix(ddof=1)</code>","text":"<p>Calculate covariance matrix for multivariate data.</p> <p>Parameters:</p> Name Type Description Default <code>ddof</code> <code>int</code> <p>Delta degrees of freedom</p> <code>1</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Covariance matrix</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.autocorrelation","title":"<code>autocorrelation(max_lag=20)</code>","text":"<p>Calculate autocorrelation function for time series data.</p> <p>Parameters:</p> Name Type Description Default <code>max_lag</code> <code>int</code> <p>Maximum lag to calculate autocorrelation</p> <code>20</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Autocorrelation values for each lag</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.partial_autocorrelation","title":"<code>partial_autocorrelation(max_lag=20)</code>","text":"<p>Calculate partial autocorrelation function for time series data.</p> <p>Parameters:</p> Name Type Description Default <code>max_lag</code> <code>int</code> <p>Maximum lag to calculate partial autocorrelation</p> <code>20</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Partial autocorrelation values for each lag</p>"},{"location":"api/parray/parray/#stationarity-tests","title":"Stationarity Tests","text":"<p>               Bases: <code>ndarray</code></p> <p>A wrapper around numpy arrays that provides method chaining for financial analysis.</p> <p>This class allows for fluent method chaining like: data.ema(9).sma(20)</p> <p>It also supports parallel processing and GPU acceleration for improved performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.dtypes import Parray\n&gt;&gt;&gt; data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; ts = Parray(data)\n&gt;&gt;&gt; result = ts.ema(3).sma(2)\n&gt;&gt;&gt; # Enable GPU acceleration\n&gt;&gt;&gt; ts.enable_gpu()\n&gt;&gt;&gt; result_gpu = ts.ema(3).sma(2)\n</code></pre>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.jarque_bera_test","title":"<code>jarque_bera_test()</code>","text":"<p>Perform Jarque-Bera test for normality.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>(test statistic, p-value)</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.augmented_dickey_fuller_test","title":"<code>augmented_dickey_fuller_test(regression='c', max_lag=None)</code>","text":"<p>Perform Augmented Dickey-Fuller test for stationarity.</p> <p>Parameters:</p> Name Type Description Default <code>regression</code> <code>str</code> <p>Regression model to use: 'c' for constant, 'ct' for constant and trend, 'ctt' for constant, trend, and trend squared, 'n' for no regression</p> <code>'c'</code> <code>max_lag</code> <code>int</code> <p>Maximum lag to consider</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(test statistic, p-value)</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.kpss_test","title":"<code>kpss_test(regression='c', lags=None)</code>","text":"<p>Perform KPSS test for stationarity.</p> <p>Parameters:</p> Name Type Description Default <code>regression</code> <code>str</code> <p>Regression model to use: 'c' for constant, 'ct' for constant and trend</p> <code>'c'</code> <code>lags</code> <code>int</code> <p>Number of lags to use</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(test statistic, p-value)</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.ljung_box_test","title":"<code>ljung_box_test(lags=10, boxpierce=False)</code>","text":"<p>Perform Ljung-Box test for autocorrelation.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int</code> <p>Number of lags to use</p> <code>10</code> <code>boxpierce</code> <code>bool</code> <p>If True, use Box-Pierce test instead of Ljung-Box</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple or dict</code> <p>(test statistic, p-value) or {lag: (test statistic, p-value)}</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.durbin_watson_test","title":"<code>durbin_watson_test()</code>","text":"<p>Perform Durbin-Watson test for autocorrelation.</p> <p>Returns:</p> Type Description <code>float</code> <p>Test statistic</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.arch_test","title":"<code>arch_test(lags=5)</code>","text":"<p>Perform ARCH test for heteroskedasticity.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int</code> <p>Number of lags to use</p> <code>5</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(test statistic, p-value)</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.kolmogorov_smirnov_test","title":"<code>kolmogorov_smirnov_test(dist='norm', params=None)</code>","text":"<p>Perform Kolmogorov-Smirnov test for distribution fit.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>str</code> <p>Distribution to test against</p> <code>'norm'</code> <code>params</code> <code>dict</code> <p>Parameters for the distribution</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(test statistic, p-value)</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.hurst_exponent","title":"<code>hurst_exponent(max_lag=None)</code>","text":"<p>Calculate Hurst exponent.</p> <p>Parameters:</p> Name Type Description Default <code>max_lag</code> <code>int</code> <p>Maximum lag to consider</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Hurst exponent</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.variance_ratio_test","title":"<code>variance_ratio_test(periods=None, robust=True)</code>","text":"<p>Perform variance ratio test.</p> <p>Parameters:</p> Name Type Description Default <code>periods</code> <code>list of int</code> <p>Periods to test</p> <code>None</code> <code>robust</code> <code>bool</code> <p>Whether to use robust standard errors</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>{period: (test statistic, p-value)}</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.granger_causality_test","title":"<code>granger_causality_test(y, max_lag=1)</code>","text":"<p>Perform Granger causality test.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array - like</code> <p>Second time series</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag to consider</p> <code>1</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(test statistic, p-value)</p>"},{"location":"api/parray/parray/#signal-generation","title":"Signal Generation","text":"<p>               Bases: <code>ndarray</code></p> <p>A wrapper around numpy arrays that provides method chaining for financial analysis.</p> <p>This class allows for fluent method chaining like: data.ema(9).sma(20)</p> <p>It also supports parallel processing and GPU acceleration for improved performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.dtypes import Parray\n&gt;&gt;&gt; data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; ts = Parray(data)\n&gt;&gt;&gt; result = ts.ema(3).sma(2)\n&gt;&gt;&gt; # Enable GPU acceleration\n&gt;&gt;&gt; ts.enable_gpu()\n&gt;&gt;&gt; result_gpu = ts.ema(3).sma(2)\n</code></pre>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.crossover","title":"<code>crossover(other)</code>","text":"<p>Detect when this series crosses above another series or value.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>array - like or scalar</code> <p>The other series or value to compare against</p> required <p>Returns:</p> Type Description <code>Parray</code> <p>Boolean array where True indicates a crossover (this crosses above other)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; prices = Parray([10, 11, 12, 11, 10, 9, 10, 11, 12])\n&gt;&gt;&gt; sma = prices.sma(3)\n&gt;&gt;&gt; crossovers = prices.crossover(sma)\n</code></pre>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.crossunder","title":"<code>crossunder(other)</code>","text":"<p>Detect when this series crosses below another series or value.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>array - like or scalar</code> <p>The other series or value to compare against</p> required <p>Returns:</p> Type Description <code>Parray</code> <p>Boolean array where True indicates a crossunder (this crosses below other)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; prices = Parray([10, 11, 12, 11, 10, 9, 10, 11, 12])\n&gt;&gt;&gt; sma = prices.sma(3)\n&gt;&gt;&gt; crossunders = prices.crossunder(sma)\n</code></pre>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.wave","title":"<code>wave(high=None, low=None, close=None)</code>","text":"<p>Extract wave points from OHLC financial data.</p> <p>Parameters:</p> Name Type Description Default <code>high</code> <code>array - like</code> <p>High prices. If None, assumes self contains open prices</p> <code>None</code> <code>low</code> <code>array - like</code> <p>Low prices. If None, assumes self contains open prices</p> <code>None</code> <code>close</code> <code>array - like</code> <p>Close prices. If None, assumes self contains open prices</p> <code>None</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Wave points</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.zigzag","title":"<code>zigzag(threshold=0.03)</code>","text":"<p>Extract zigzag pivot points from price data based on a percentage threshold.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Minimum percentage change required to identify a new pivot point (0.03 = 3%)</p> <code>0.03</code> <p>Returns:</p> Type Description <code>Parray</code> <p>2D array of zigzag points with shape (n, 2), where each row contains [index, price]</p> Notes <p>The algorithm identifies significant price movements while filtering out minor fluctuations. It marks pivot points where the price changes direction by at least the specified threshold percentage.</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.slope","title":"<code>slope(period=5)</code>","text":"<p>Calculate the slope of the time series over a specified period.</p> <p>This method uses linear regression to calculate the slope of the line that best fits the data over the specified period.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>int</code> <p>Number of points to use for slope calculation</p> <code>5</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Slope values for each point in the time series</p>"},{"location":"api/parray/parray/#filters-and-smoothing","title":"Filters and Smoothing","text":"<p>               Bases: <code>ndarray</code></p> <p>A wrapper around numpy arrays that provides method chaining for financial analysis.</p> <p>This class allows for fluent method chaining like: data.ema(9).sma(20)</p> <p>It also supports parallel processing and GPU acceleration for improved performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.dtypes import Parray\n&gt;&gt;&gt; data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; ts = Parray(data)\n&gt;&gt;&gt; result = ts.ema(3).sma(2)\n&gt;&gt;&gt; # Enable GPU acceleration\n&gt;&gt;&gt; ts.enable_gpu()\n&gt;&gt;&gt; result_gpu = ts.ema(3).sma(2)\n</code></pre>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.kalman_filter","title":"<code>kalman_filter(process_variance=1e-05, measurement_variance=0.001, initial_state=None, initial_covariance=1.0)</code>","text":"<p>Apply a standard Kalman filter to the time series.</p> <p>Parameters:</p> Name Type Description Default <code>process_variance</code> <code>float</code> <p>Process noise variance (Q)</p> <code>1e-5</code> <code>measurement_variance</code> <code>float</code> <p>Measurement noise variance (R)</p> <code>1e-3</code> <code>initial_state</code> <code>float</code> <p>Initial state estimate. If None, the first data point is used</p> <code>None</code> <code>initial_covariance</code> <code>float</code> <p>Initial estimate covariance</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Filtered time series</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.adaptive_kalman_filter","title":"<code>adaptive_kalman_filter(process_variance_init=1e-05, measurement_variance_init=0.001, adaptation_rate=0.01, window_size=10, initial_state=None, initial_covariance=1.0)</code>","text":"<p>Apply an adaptive Kalman filter to the time series.</p> <p>Parameters:</p> Name Type Description Default <code>process_variance_init</code> <code>float</code> <p>Initial process noise variance (Q)</p> <code>1e-5</code> <code>measurement_variance_init</code> <code>float</code> <p>Initial measurement noise variance (R)</p> <code>1e-3</code> <code>adaptation_rate</code> <code>float</code> <p>Rate at which the filter adapts to changes</p> <code>0.01</code> <code>window_size</code> <code>int</code> <p>Size of the window for innovation estimation</p> <code>10</code> <code>initial_state</code> <code>float</code> <p>Initial state estimate. If None, the first data point is used</p> <code>None</code> <code>initial_covariance</code> <code>float</code> <p>Initial estimate covariance</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Filtered time series</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.butterworth_filter","title":"<code>butterworth_filter(cutoff, order=4, filter_type='lowpass', fs=1.0)</code>","text":"<p>Apply a Butterworth filter to the time series.</p> <p>Parameters:</p> Name Type Description Default <code>cutoff</code> <code>float or tuple of float</code> <p>Cutoff frequency. For lowpass and highpass, this is a scalar. For bandpass and bandstop, this is a tuple of (low, high)</p> required <code>order</code> <code>int</code> <p>Filter order</p> <code>4</code> <code>filter_type</code> <code>str</code> <p>Filter type: 'lowpass', 'highpass', 'bandpass', or 'bandstop'</p> <code>'lowpass'</code> <code>fs</code> <code>float</code> <p>Sampling frequency</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Filtered time series</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.savitzky_golay_filter","title":"<code>savitzky_golay_filter(window_length=11, polyorder=3, deriv=0, delta=1.0)</code>","text":"<p>Apply a Savitzky-Golay filter to the time series.</p> <p>Parameters:</p> Name Type Description Default <code>window_length</code> <code>int</code> <p>Length of the filter window (must be odd)</p> <code>11</code> <code>polyorder</code> <code>int</code> <p>Order of the polynomial used to fit the samples</p> <code>3</code> <code>deriv</code> <code>int</code> <p>Order of the derivative to compute</p> <code>0</code> <code>delta</code> <code>float</code> <p>Spacing of the samples to which the filter is applied</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Filtered time series</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.hampel_filter","title":"<code>hampel_filter(window_size=5, n_sigmas=3.0)</code>","text":"<p>Apply a Hampel filter to the time series to remove outliers.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>Size of the window (number of points on each side of the current point)</p> <code>5</code> <code>n_sigmas</code> <code>float</code> <p>Number of standard deviations to use for outlier detection</p> <code>3.0</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Filtered time series</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.hodrick_prescott_filter","title":"<code>hodrick_prescott_filter(lambda_param=1600.0)</code>","text":"<p>Apply the Hodrick-Prescott filter to decompose the time series into trend and cycle components.</p> <p>Parameters:</p> Name Type Description Default <code>lambda_param</code> <code>float</code> <p>Smoothing parameter. The larger the value, the smoother the trend component</p> <code>1600.0</code> <p>Returns:</p> Type Description <code>tuple of Parray</code> <p>Tuple containing (trend, cycle) components</p>"},{"location":"api/parray/parray/#data-transformations","title":"Data Transformations","text":"<p>               Bases: <code>ndarray</code></p> <p>A wrapper around numpy arrays that provides method chaining for financial analysis.</p> <p>This class allows for fluent method chaining like: data.ema(9).sma(20)</p> <p>It also supports parallel processing and GPU acceleration for improved performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.dtypes import Parray\n&gt;&gt;&gt; data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; ts = Parray(data)\n&gt;&gt;&gt; result = ts.ema(3).sma(2)\n&gt;&gt;&gt; # Enable GPU acceleration\n&gt;&gt;&gt; ts.enable_gpu()\n&gt;&gt;&gt; result_gpu = ts.ema(3).sma(2)\n</code></pre>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.normalize","title":"<code>normalize(method='l2')</code>","text":"<p>Normalize data using vector normalization methods.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Normalization method: 'l1' or 'l2' - 'l1': L1 normalization (Manhattan norm) - 'l2': L2 normalization (Euclidean norm)</p> <code>'l2'</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Normalized data with unit norm</p> Notes <p>For min-max scaling, use the min_max_scale method instead.</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.normalize_l1","title":"<code>normalize_l1()</code>","text":"<p>Normalize data using L1 norm (Manhattan norm).</p> <p>Returns:</p> Type Description <code>Parray</code> <p>Normalized data with unit L1 norm</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.normalize_l2","title":"<code>normalize_l2()</code>","text":"<p>Normalize data using L2 norm (Euclidean norm).</p> <p>Returns:</p> Type Description <code>Parray</code> <p>Normalized data with unit L2 norm</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.standardize","title":"<code>standardize()</code>","text":"<p>Standardize data to have mean 0 and standard deviation 1 (Z-score normalization).</p> <p>Returns:</p> Type Description <code>Parray</code> <p>Standardized data with zero mean and unit variance</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.min_max_scale","title":"<code>min_max_scale(feature_range=(0, 1))</code>","text":"<p>Scale data to a specified range.</p> <p>Parameters:</p> Name Type Description Default <code>feature_range</code> <code>tuple</code> <p>Desired range of the scaled data</p> <code>(0, 1)</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Scaled data</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.robust_scale","title":"<code>robust_scale(method='iqr', quantile_range=(25.0, 75.0))</code>","text":"<p>Scale data using robust statistics.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Method to use: 'iqr' or 'mad'</p> <code>'iqr'</code> <code>quantile_range</code> <code>tuple</code> <p>Quantile range to use for IQR method</p> <code>(25.0, 75.0)</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Robustly scaled data</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.quantile_transform","title":"<code>quantile_transform(n_quantiles=1000, output_distribution='uniform')</code>","text":"<p>Transform data to follow a uniform or normal distribution.</p> <p>Parameters:</p> Name Type Description Default <code>n_quantiles</code> <code>int</code> <p>Number of quantiles to use</p> <code>1000</code> <code>output_distribution</code> <code>str</code> <p>Distribution to transform to: 'uniform' or 'normal'</p> <code>'uniform'</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Transformed data</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.winsorize","title":"<code>winsorize(limits=0.05)</code>","text":"<p>Limit extreme values in data.</p> <p>Parameters:</p> Name Type Description Default <code>limits</code> <code>float or tuple</code> <p>Proportion of values to limit</p> <code>0.05</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Winsorized data</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.remove_outliers","title":"<code>remove_outliers(method='zscore', threshold=3.0)</code>","text":"<p>Remove outliers from data.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Method to use: 'zscore', 'iqr', or 'mad'</p> <code>'zscore'</code> <code>threshold</code> <code>float</code> <p>Threshold for outlier detection</p> <code>3.0</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Data with outliers removed</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.fill_missing","title":"<code>fill_missing(method='mean', value=None)</code>","text":"<p>Fill missing values in data.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Method to fill missing values: 'mean', 'median', 'mode', 'forward', 'backward', 'value'</p> <code>'mean'</code> <code>value</code> <code>float</code> <p>Value to use when method='value'</p> <code>None</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Data with missing values filled</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.interpolate_missing","title":"<code>interpolate_missing(method='linear')</code>","text":"<p>Interpolate missing values in data.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Interpolation method: 'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'</p> <code>'linear'</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Data with missing values interpolated</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.log_transform","title":"<code>log_transform(base=None, offset=0.0)</code>","text":"<p>Apply logarithmic transformation.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>float</code> <p>Base of the logarithm. If None, natural logarithm is used.</p> <code>None</code> <code>offset</code> <code>float</code> <p>Offset to add to data before taking logarithm</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Log-transformed data</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.power_transform","title":"<code>power_transform(method='yeo-johnson', standardize=True)</code>","text":"<p>Apply power transformation to make data more Gaussian-like.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The power transform method: 'box-cox' or 'yeo-johnson'</p> <code>'yeo-johnson'</code> <code>standardize</code> <code>bool</code> <p>Whether to standardize the data after transformation</p> <code>True</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Power transformed data</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.scale_to_range","title":"<code>scale_to_range(feature_range=(0.0, 1.0))</code>","text":"<p>Scale data to a specified range.</p> <p>Parameters:</p> Name Type Description Default <code>feature_range</code> <code>tuple</code> <p>Desired range of the scaled data</p> <code>(0.0, 1.0)</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Scaled data</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.clip_outliers","title":"<code>clip_outliers(lower_percentile=1.0, upper_percentile=99.0)</code>","text":"<p>Clip values outside specified percentiles.</p> <p>Parameters:</p> Name Type Description Default <code>lower_percentile</code> <code>float</code> <p>Lower percentile to clip</p> <code>1.0</code> <code>upper_percentile</code> <code>float</code> <p>Upper percentile to clip</p> <code>99.0</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Clipped data</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.discretize","title":"<code>discretize(n_bins=5, strategy='uniform')</code>","text":"<p>Discretize continuous data into bins.</p> <p>Parameters:</p> Name Type Description Default <code>n_bins</code> <code>int</code> <p>Number of bins to create</p> <code>5</code> <code>strategy</code> <code>str</code> <p>Strategy to use for creating bins: 'uniform', 'quantile', or 'kmeans'</p> <code>'uniform'</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Array of bin labels (1 to n_bins)</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.polynomial_features","title":"<code>polynomial_features(degree=2)</code>","text":"<p>Generate polynomial features up to specified degree.</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int</code> <p>Maximum degree of polynomial features</p> <code>2</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Array with polynomial features as columns</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.resample","title":"<code>resample(factor, method='mean')</code>","text":"<p>Resample data by aggregating values.</p> <p>Parameters:</p> Name Type Description Default <code>factor</code> <code>int</code> <p>Resampling factor (e.g., 5 means aggregate every 5 points)</p> required <code>method</code> <code>str</code> <p>Aggregation method: 'mean', 'median', 'sum', 'min', 'max'</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Resampled data</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.dynamic_tanh","title":"<code>dynamic_tanh(alpha=1.0)</code>","text":"<p>Apply Dynamic Tanh (DyT) transformation to data, which helps normalize data  while preserving relative differences and handling outliers well.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>Scaling factor that controls the transformation intensity. Higher values lead to more aggressive normalization (less extreme values).</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Parray</code> <p>DyT-transformed data with values in range (-1, 1)</p> Notes <p>The Dynamic Tanh (DyT) transformation follows these steps: 1. Center data by subtracting the median 2. Scale data by dividing by (MAD * alpha), where MAD is Median Absolute Deviation    Higher alpha means more scaling (division by larger value) before tanh 3. Apply tanh transformation to the scaled data</p> <p>This transformation is particularly useful for financial data as it: - Is robust to outliers (uses median and MAD instead of mean and std) - Maps all values to the range (-1, 1) without clipping extreme values - Preserves the shape of the distribution better than min-max scaling - Handles multi-modal distributions better than standard normalization</p>"},{"location":"api/parray/parray/#time-series-operations","title":"Time Series Operations","text":"<p>               Bases: <code>ndarray</code></p> <p>A wrapper around numpy arrays that provides method chaining for financial analysis.</p> <p>This class allows for fluent method chaining like: data.ema(9).sma(20)</p> <p>It also supports parallel processing and GPU acceleration for improved performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.dtypes import Parray\n&gt;&gt;&gt; data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; ts = Parray(data)\n&gt;&gt;&gt; result = ts.ema(3).sma(2)\n&gt;&gt;&gt; # Enable GPU acceleration\n&gt;&gt;&gt; ts.enable_gpu()\n&gt;&gt;&gt; result_gpu = ts.ema(3).sma(2)\n</code></pre>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.lag_features","title":"<code>lag_features(lags)</code>","text":"<p>Create lagged features from data.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>list of int</code> <p>List of lag values. Zero lag returns the original values, negative lags are ignored, and lags larger than the data length result in NaN columns.</p> required <p>Returns:</p> Type Description <code>Parray</code> <p>Array with original and lagged features as columns</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.rolling_window","title":"<code>rolling_window(window_size, step=1)</code>","text":"<p>Create rolling windows from data.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>Size of each window</p> required <code>step</code> <code>int</code> <p>Step size between windows</p> <code>1</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Array with rolling windows as rows</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.rolling_statistics","title":"<code>rolling_statistics(window, statistics)</code>","text":"<p>Calculate multiple rolling statistics in a single pass.</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>int</code> <p>Size of the rolling window</p> required <code>statistics</code> <code>list of str</code> <p>List of statistics to calculate. Options include: 'mean', 'std', 'min', 'max', 'median', 'skew', 'kurt'</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with keys as statistic names and values as Parray objects</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.rolling_apply","title":"<code>rolling_apply(window, func, *args, **kwargs)</code>","text":"<p>Apply a function to rolling windows of the array.</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>int</code> <p>Size of the rolling window</p> required <code>func</code> <code>callable</code> <p>Function to apply to each window</p> required <code>*args</code> <p>Arguments to pass to the function</p> <code>()</code> <code>**kwargs</code> <p>Arguments to pass to the function</p> <code>()</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Result of applying the function to rolling windows</p>"},{"location":"api/parray/parray/#custom-processing","title":"Custom Processing","text":"<p>               Bases: <code>ndarray</code></p> <p>A wrapper around numpy arrays that provides method chaining for financial analysis.</p> <p>This class allows for fluent method chaining like: data.ema(9).sma(20)</p> <p>It also supports parallel processing and GPU acceleration for improved performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pypulate.dtypes import Parray\n&gt;&gt;&gt; data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n&gt;&gt;&gt; ts = Parray(data)\n&gt;&gt;&gt; result = ts.ema(3).sma(2)\n&gt;&gt;&gt; # Enable GPU acceleration\n&gt;&gt;&gt; ts.enable_gpu()\n&gt;&gt;&gt; result_gpu = ts.ema(3).sma(2)\n</code></pre>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.apply","title":"<code>apply(func, *args, **kwargs)</code>","text":"<p>Apply a function to the array.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>Function to apply</p> required <code>*args</code> <p>Arguments to pass to the function</p> <code>()</code> <code>**kwargs</code> <p>Arguments to pass to the function</p> <code>()</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Result of applying the function</p>"},{"location":"api/parray/parray/#pypulate.dtypes.parray.Parray.apply_along_axis","title":"<code>apply_along_axis(func, axis, *args, **kwargs)</code>","text":"<p>Apply a function along a specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>Function to apply</p> required <code>axis</code> <code>int</code> <p>Axis along which to apply the function</p> required <code>*args</code> <p>Arguments to pass to the function</p> <code>()</code> <code>**kwargs</code> <p>Arguments to pass to the function</p> <code>()</code> <p>Returns:</p> Type Description <code>Parray</code> <p>Result of applying the function along the axis</p>"},{"location":"api/parray/preprocessing/","title":"Preprocessing API","text":"<p>Preprocessing Module</p> <p>This module provides data preprocessing utilities for financial data analysis without dependencies on pandas, using only numpy and scipy.</p>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.clip_outliers","title":"<code>clip_outliers(data, lower_percentile=1.0, upper_percentile=99.0)</code>","text":"<p>Clip values outside specified percentiles.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array</p> required <code>lower_percentile</code> <code>float</code> <p>Lower percentile (between 0 and 100)</p> <code>1.0</code> <code>upper_percentile</code> <code>float</code> <p>Upper percentile (between 0 and 100)</p> <code>99.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Data with outliers clipped. For special cases: - Empty array: returns empty array - Single value: returns unchanged value - Constant values: returns unchanged array - All NaN: returns array of NaN</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If percentiles are not between 0 and 100 or if lower_percentile &gt; upper_percentile</p>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.difference","title":"<code>difference(data, order=1)</code>","text":"<p>Calculate differences between consecutive elements of a time series.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array</p> required <code>order</code> <code>int</code> <p>The order of differencing. Must be non-negative.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of differences with length n-order, where n is the length of the input array. For order=0, returns the original array. NaN values in input result in NaN differences only where NaN values are involved.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If order is negative or larger than the length of the data.</p>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.discretize","title":"<code>discretize(data, n_bins=5, strategy='uniform')</code>","text":"<p>Discretize continuous data into bins using efficient vectorized operations.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array to be discretized. Will be flattened if multi-dimensional.</p> required <code>n_bins</code> <code>int</code> <p>Number of bins to create. Must be positive.</p> <code>5</code> <code>strategy</code> <code>str</code> <p>Strategy to use for creating bins: - 'uniform': Equal-width bins - 'quantile': Equal-frequency bins - 'kmeans': Bins based on k-means clustering</p> <code>'uniform'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of bin labels (1 to n_bins). NaN values in input remain NaN in output. For special cases: - Empty array: returns empty array - Single value or constant array: returns array filled with 1.0 (NaN preserved) - All NaN: returns array of NaN</p> Notes <ul> <li>Uses efficient vectorized operations for binning</li> <li>Handles NaN values gracefully</li> <li>Memory efficient implementation avoiding unnecessary copies</li> <li>Bin labels are 1-based (1 to n_bins)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If strategy is not one of 'uniform', 'quantile', or 'kmeans'</li> <li>If n_bins is not positive</li> </ul>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.dynamic_tanh","title":"<code>dynamic_tanh(data, alpha=1.0)</code>","text":"<p>Apply Dynamic Tanh (DyT) transformation to data, which helps normalize data  while preserving relative differences and handling outliers well.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array, can contain None or np.nan as missing values</p> required <code>alpha</code> <code>float</code> <p>Scaling factor that controls the transformation intensity. Higher values lead to more aggressive normalization (less extreme values).</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>DyT-transformed data with values in range (-1, 1). For special cases: - Empty array: returns empty array - Single value: returns array of zeros - Constant values: returns array of zeros - All NaN: returns array of NaN</p> Notes <p>The Dynamic Tanh (DyT) transformation follows these steps: 1. Center data by subtracting the median 2. Scale data by dividing by (MAD * alpha), where MAD is Median Absolute Deviation    Higher alpha means more scaling (division by larger value) before tanh 3. Apply tanh transformation to the scaled data</p> <p>This transformation is particularly useful for financial data as it: - Is robust to outliers (uses median and MAD instead of mean and std) - Maps all values to the range (-1, 1) without clipping extreme values - Preserves the shape of the distribution better than min-max scaling - Handles multi-modal distributions better than standard normalization</p>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.fill_missing","title":"<code>fill_missing(data, method='mean', value=None)</code>","text":"<p>Fill missing values in data using efficient NumPy operations.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array, can contain None or np.nan as missing values</p> required <code>method</code> <code>str</code> <p>Method to fill missing values: 'mean', 'median', 'mode', 'forward', 'backward', 'value'</p> <code>'mean'</code> <code>value</code> <code>float</code> <p>Value to use when method='value'</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Data with missing values filled. For all-NaN input: - Statistical methods (mean, median, mode) return all-NaN array - Forward/backward fill return all-NaN array - Value fill returns array filled with specified value</p> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If method is not recognized</li> <li>If method='value' but no value is provided</li> </ul>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.interpolate_missing","title":"<code>interpolate_missing(data, method='linear')</code>","text":"<p>Interpolate missing values in data using efficient NumPy operations.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array, can contain None or np.nan as missing values</p> required <code>method</code> <code>str</code> <p>Interpolation method: 'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'</p> <code>'linear'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Data with missing values interpolated. For missing values at the start or end: - 'nearest' and 'zero' methods will use the nearest valid value - other methods will leave them as NaN</p>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.lag_features","title":"<code>lag_features(data, lags)</code>","text":"<p>Create lagged features from data using vectorized operations.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array</p> required <code>lags</code> <code>list of int</code> <p>List of lag values. Zero lag returns the original values, negative lags are ignored, and lags larger than the data length result in NaN columns.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array with original and lagged features as columns. First column is the original data, followed by columns for each lag. NaN values are used for undefined lag positions.</p>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.log_transform","title":"<code>log_transform(data, base=None, offset=0.0)</code>","text":"<p>Apply logarithmic transformation to data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array, can be list or numpy array</p> required <code>base</code> <code>float</code> <p>Base of logarithm. If None, natural logarithm is used. Common bases: None (natural log), 2, 10</p> <code>None</code> <code>offset</code> <code>float</code> <p>Offset added to data before taking logarithm (useful for non-positive data)</p> <code>0.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Log-transformed data. For special cases: - Empty array: returns empty array - Single value: returns log of (value + offset) - All NaN: returns array of NaN - Non-positive values: raises ValueError if any (value + offset) &lt;= 0</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any value after offset is non-positive</p>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.min_max_scale","title":"<code>min_max_scale(data, feature_range=(0, 1))</code>","text":"<p>Scale features to a given range using min-max scaling.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array, can contain None or np.nan as missing values</p> required <code>feature_range</code> <code>tuple</code> <p>Desired range of transformed data</p> <code>(0, 1)</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Scaled data. For special cases: - Empty array: returns empty array - Single value: returns array filled with feature_range[0] - Constant values: returns array filled with feature_range[0] - All NaN: returns array of NaN</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If feature_range[0] &gt;= feature_range[1]</p>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.normalize","title":"<code>normalize(data, method='l2')</code>","text":"<p>Normalize data using vector normalization methods.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array, can contain None or np.nan as missing values</p> required <code>method</code> <code>str</code> <p>Normalization method: 'l1' or 'l2' - 'l1': L1 normalization (Manhattan norm) - 'l2': L2 normalization (Euclidean norm)</p> <code>'l2'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Normalized data. For special cases: - Empty array: returns empty array - All zeros: returns array of zeros - Single value: returns array with 1.0 - All NaN: returns array of NaN</p> Notes <p>L1 normalization formula: X' = X / sum(|X|) L2 normalization formula: X' = X / sqrt(sum(X^2))</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If method is not 'l1' or 'l2'</p>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.polynomial_features","title":"<code>polynomial_features(data, degree=2)</code>","text":"<p>Generate polynomial features up to specified degree.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array. Will be flattened if multi-dimensional.</p> required <code>degree</code> <code>int</code> <p>Maximum degree of polynomial features. Must be a positive integer.</p> <code>2</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array with polynomial features as columns: - First column contains 1s (bias term) - Subsequent columns contain increasing powers (x, x\u00b2, ..., x^degree) Shape will be (n_samples, degree + 1)</p> Notes <ul> <li>Uses efficient vectorized operations for polynomial computation</li> <li>Handles NaN values gracefully (propagates through powers)</li> <li>Memory efficient implementation avoiding unnecessary copies</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If degree is not a positive integer</p>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.power_transform","title":"<code>power_transform(data, method='yeo-johnson', standardize=True)</code>","text":"<p>Apply power transformation (Box-Cox or Yeo-Johnson) to make data more Gaussian-like.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array, can be list or numpy array</p> required <code>method</code> <code>str</code> <p>The power transform method: - 'box-cox': only works with positive values - 'yeo-johnson': works with both positive and negative values</p> <code>'yeo-johnson'</code> <code>standardize</code> <code>bool</code> <p>Whether to standardize the data after transformation</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Power transformed data. NaN values in input remain NaN in output. For special cases: - Empty array: returns empty array - Single value: returns array of zeros if standardize=True,   or the log1p of the constant if standardize=False - Constant values: returns array of zeros if standardize=True,   or the log1p of the constant if standardize=False - All NaN: returns array of NaN</p> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If method is not one of 'box-cox', 'yeo-johnson'</li> </ul>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.quantile_transform","title":"<code>quantile_transform(data, n_quantiles=1000, output_distribution='uniform')</code>","text":"<p>Transform features using quantile information.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array</p> required <code>n_quantiles</code> <code>int</code> <p>Number of quantiles to use</p> <code>1000</code> <code>output_distribution</code> <code>str</code> <p>'uniform' or 'normal'</p> <code>'uniform'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Quantile transformed data</p>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.remove_outliers","title":"<code>remove_outliers(data, method='zscore', threshold=2.0)</code>","text":"<p>Remove outliers from data by replacing them with NaN.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array</p> required <code>method</code> <code>str</code> <p>Method to detect outliers: 'zscore', 'iqr', or 'mad'</p> <code>'zscore'</code> <code>threshold</code> <code>float</code> <p>Threshold for outlier detection: - For 'zscore': number of standard deviations - For 'iqr': multiplier of IQR - For 'mad': multiplier of MAD</p> <code>2.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Data with outliers replaced by NaN. Original NaN values remain NaN.</p>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.resample","title":"<code>resample(data, factor, method='mean')</code>","text":"<p>Resample data by aggregating values using efficient NumPy operations.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array, can contain None or np.nan as missing values</p> required <code>factor</code> <code>int</code> <p>Resampling factor (e.g., 5 means aggregate every 5 points)</p> required <code>method</code> <code>str</code> <p>Aggregation method: 'mean', 'median', 'sum', 'min', 'max' Note: For groups containing all NaN values: - sum will return NaN - mean, median, min, max will return NaN</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Resampled data. Length will be floor(len(data)/factor). Remaining data points that don't fill a complete group are discarded.</p>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.robust_scale","title":"<code>robust_scale(data, method='iqr', quantile_range=(25.0, 75.0))</code>","text":"<p>Scale features using statistics that are robust to outliers.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array, can contain None or np.nan as missing values</p> required <code>method</code> <code>str</code> <p>Method to use for scaling: - 'iqr': Use Interquartile Range - 'mad': Use Median Absolute Deviation</p> <code>'iqr'</code> <code>quantile_range</code> <code>tuple</code> <p>Quantile range used to calculate scale when method='iqr'</p> <code>(25.0, 75.0)</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Robustly scaled data. For special cases: - Empty array: returns empty array - Single value: returns array of zeros - Constant values: returns array of zeros - All NaN: returns array of NaN</p> Notes <p>For IQR method: (X - median) / IQR For MAD method: (X - median) / (MAD * 1.4826) The factor 1.4826 makes the MAD consistent with the standard deviation for normally distributed data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If method is not recognized or if quantile range is invalid</p>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.rolling_window","title":"<code>rolling_window(data, window_size, step=1)</code>","text":"<p>Create rolling windows of data using efficient NumPy striding.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array</p> required <code>window_size</code> <code>int</code> <p>Size of the rolling window</p> required <code>step</code> <code>int</code> <p>Step size between windows</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of rolling windows. Shape will be (n_windows, window_size) where n_windows = max(0, (len(data) - window_size) // step + 1)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If window_size or step is not positive</p>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.scale_to_range","title":"<code>scale_to_range(data, feature_range=(0.0, 1.0))</code>","text":"<p>Scale data to a specific range while preserving relative distances.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array, can contain None or np.nan as missing values</p> required <code>feature_range</code> <code>tuple</code> <p>Desired range for transformed data (min, max)</p> <code>(0.0, 1.0)</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Data scaled to target range. For special cases: - Empty array: returns empty array - Single value: returns array filled with feature_range[0] - Constant values: returns array filled with feature_range[0] - All NaN: returns array of NaN</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If feature_range[0] &gt;= feature_range[1]</p>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.standardize","title":"<code>standardize(data)</code>","text":"<p>Standardize data to have mean 0 and standard deviation 1 (Z-score normalization).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array, can contain None or np.nan as missing values</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Standardized data with zero mean and unit variance. For special cases: - Empty array: returns empty array - Single value: returns array of zeros - Constant values: returns array of zeros - All NaN: returns array of NaN</p>"},{"location":"api/parray/preprocessing/#pypulate.preprocessing.preprocessing.winsorize","title":"<code>winsorize(data, limits=0.05)</code>","text":"<p>Limit extreme values in data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array</p> required <code>limits</code> <code>float or tuple</code> <p>If a float, it is the proportion to cut on each side. If a tuple of two floats, they represent the proportions to cut from the lower and upper bounds.</p> <code>0.05</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Winsorized data</p>"},{"location":"api/parray/statistics/","title":"Statistics API","text":"<p>Statistics Module</p> <p>This module provides statistical analysis utilities for financial data analysis.</p>"},{"location":"api/parray/statistics/#pypulate.preprocessing.statistics.arch_test","title":"<code>arch_test(returns, lags=5)</code>","text":"<p>Perform Engle's ARCH test for heteroskedasticity.</p> <p>The ARCH test examines whether there is autoregressive conditional heteroskedasticity (ARCH) in the residuals. The null hypothesis is that there is no ARCH effect.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Input returns or residuals array. Can contain NaN values.</p> required <code>lags</code> <code>int</code> <p>Number of lags to test for ARCH effects.</p> <code>5</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: - test_statistic : float     The LM test statistic. Larger values indicate stronger evidence     against the null hypothesis of no ARCH effects. - p_value : float     The p-value for the test. Small p-values (e.g., &lt; 0.05) suggest     rejection of the null hypothesis, indicating presence of ARCH effects.     Returns (np.nan, np.nan) if:     - Insufficient data     - All values are NaN     - Constant series     - Zero variance series</p> <p>Raises:</p> Type Description <code>ValueError: If lags is not positive</code> <code>Warning: If sample size is less than 30</code>"},{"location":"api/parray/statistics/#pypulate.preprocessing.statistics.augmented_dickey_fuller_test","title":"<code>augmented_dickey_fuller_test(data, regression='c', max_lag=None)</code>","text":"<p>Perform Augmented Dickey-Fuller test for stationarity.</p> <p>The ADF test tests the null hypothesis that a unit root is present in a time series. The alternative hypothesis is stationarity or trend-stationarity, depending on the specified regression type.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array. Can contain NaN values which will be removed before calculation.</p> required <code>regression</code> <code>str</code> <p>Regression type: - 'c': Include constant (test for level stationarity) - 'ct': Include constant and trend (test for trend stationarity) - 'n': No constant or trend (test for zero-mean stationarity)</p> <code>'c'</code> <code>max_lag</code> <code>int</code> <p>Maximum lag order. If None, it is calculated using the rule: max_lag = int(ceil(12 * (n/100)^0.25)) where n is the sample size.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: - test_statistic : float     The ADF test statistic. More negative values indicate stronger rejection     of the null hypothesis (presence of a unit root). - p_value : float     The p-value for the test. Small p-values (e.g., &lt; 0.05) suggest     rejection of the null hypothesis, indicating stationarity.     Returns np.nan if insufficient data.</p> <p>Raises:</p> Type Description <code>ValueError: If regression is not one of 'n', 'c', 'ct'</code> <code>Warning: If sample size is less than 50</code> <code>Warning: If max_lag is negative</code> <code>Warning: If max_lag is greater than or equal to the sample size</code> <code>Warning: If max_lag is greater than or equal to the sample size minus 2</code> <code>Warning: If max_lag is greater than or equal to the sample size minus 2</code> <code>Warning: If max_lag is greater than or equal to the sample size minus 2</code> <code>Warning: If max_lag is greater than or equal to the sample size minus 2</code>"},{"location":"api/parray/statistics/#pypulate.preprocessing.statistics.autocorrelation","title":"<code>autocorrelation(data, max_lag=20)</code>","text":"<p>Calculate autocorrelation function (ACF) for time series data.</p> <p>The autocorrelation function measures the correlation between observations  at different time lags. It helps identify patterns and seasonality in time series data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input data array. Can contain NaN values which will be removed before calculation.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag to calculate autocorrelation. Must be positive and less than the length of the series after removing NaN values.</p> <code>20</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Autocorrelation values for lags 0 to max_lag. The first value (lag 0)  is always 1.0 for non-constant series. Values range from -1 to 1, where: - 1.0 indicates perfect positive correlation - -1.0 indicates perfect negative correlation - 0.0 indicates no correlation - np.nan indicates insufficient data or constant series</p> <p>Raises:</p> Type Description <code>ValueError: If max_lag is negative</code>"},{"location":"api/parray/statistics/#pypulate.preprocessing.statistics.correlation_matrix","title":"<code>correlation_matrix(data, method='pearson', min_periods=1)</code>","text":"<p>Calculate the correlation matrix for a 2D array.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>2D array of shape (n_samples, n_features)</p> required <code>method</code> <code>str</code> <p>The correlation method to use ('pearson', 'spearman', or 'kendall') Default is 'pearson'</p> <code>'pearson'</code> <code>min_periods</code> <code>int</code> <p>Minimum number of valid observations required for each pair of columns Default is 1</p> <code>1</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Correlation matrix of shape (n_features, n_features)</p> <p>Raises:</p> Type Description <code>ValueError: If input data is not a 2D array or method is not one of 'pearson', 'spearman', 'kendall'</code>"},{"location":"api/parray/statistics/#pypulate.preprocessing.statistics.covariance_matrix","title":"<code>covariance_matrix(data, ddof=1)</code>","text":"<p>Calculate covariance matrix for multivariate data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input data array with shape (n_samples, n_features). Can contain None or np.nan as missing values.</p> required <code>ddof</code> <code>int</code> <p>Delta degrees of freedom for the covariance calculation. The divisor used in calculations is N - ddof, where N represents the number of non-missing elements.</p> <code>1</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Covariance matrix with shape (n_features, n_features). For features i and j, the element [i,j] represents their covariance. The matrix is symmetric, with variances on the diagonal.</p> <p>Raises:</p> Type Description <code>ValueError: If input data is not a 2D array</code>"},{"location":"api/parray/statistics/#pypulate.preprocessing.statistics.descriptive_stats","title":"<code>descriptive_stats(data)</code>","text":"<p>Calculate descriptive statistics for data efficiently.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array. Must be 1-dimensional.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the following statistics: - count: Number of non-NaN values - mean: Arithmetic mean - std: Standard deviation (N-1) - min: Minimum value - q1: First quartile (25th percentile) - median: Median (50th percentile) - q3: Third quartile (75th percentile) - max: Maximum value - skewness: Sample skewness - kurtosis: Sample excess kurtosis</p> <p>Raises:</p> Type Description <code>ValueError: If input data is not a 1D array</code>"},{"location":"api/parray/statistics/#pypulate.preprocessing.statistics.durbin_watson_test","title":"<code>durbin_watson_test(residuals)</code>","text":"<p>Perform Durbin-Watson test for autocorrelation in regression residuals.</p> <p>The Durbin-Watson test examines whether there is autocorrelation in the residuals from a regression analysis. The test statistic ranges from 0 to 4: - Values around 2 suggest no autocorrelation - Values &lt; 2 suggest positive autocorrelation - Values &gt; 2 suggest negative autocorrelation</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>array - like</code> <p>Input residuals array. Can contain NaN values which will be removed.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The Durbin-Watson test statistic, ranging from 0 to 4. Returns np.nan if: - Insufficient data (less than 2 points) - All values are NaN - All values are constant (zero variance) - Sum of squared residuals is zero</p> <p>Raises:</p> Type Description <code>ValueError: If residuals is not an array-like</code> <code>Warning: If sample size is less than 30</code>"},{"location":"api/parray/statistics/#pypulate.preprocessing.statistics.granger_causality_test","title":"<code>granger_causality_test(x, y, max_lag=1)</code>","text":"<p>Perform Granger causality test to determine if x Granger-causes y.</p> <p>The Granger causality test examines whether past values of x help predict future values of y beyond what past values of y alone can predict. The null hypothesis is that x does not Granger-cause y.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>First time series (potential cause). Can contain NaN values.</p> required <code>y</code> <code>array - like</code> <p>Second time series (potential effect). Can contain NaN values.</p> required <code>max_lag</code> <code>int</code> <p>Maximum number of lags to include in the test. Must be positive and less than half the length of the shortest series after removing NaN values.</p> <code>1</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: - f_statistic : float     The F-statistic of the test. Larger values indicate stronger evidence     against the null hypothesis. - p_value : float     The p-value for the test. Small p-values (e.g., &lt; 0.05) suggest     rejection of the null hypothesis, indicating x Granger-causes y.     Returns np.nan if insufficient data or numerical issues.</p> <p>Raises:</p> Type Description <code>ValueError: If max_lag is not positive</code> <code>ValueError: If x and y have different shapes</code> <code>Warning: If sample size is less than 30 + 2 * max_lag</code> <code>Warning: If max_lag is negative</code> <code>Warning: If max_lag is greater than or equal to the sample size</code>"},{"location":"api/parray/statistics/#pypulate.preprocessing.statistics.hurst_exponent","title":"<code>hurst_exponent(data, max_lag=None)</code>","text":"<p>Calculate the Hurst exponent for time series data.</p> <p>The Hurst exponent measures the long-term memory of a time series. It relates to the autocorrelations of the time series, and the rate at which these decrease as the lag between pairs of values increases.</p> <p>Values: - H = 0.5: Random walk (Brownian motion) - 0 \u2264 H &lt; 0.5: Mean-reverting series (negative autocorrelation) - 0.5 &lt; H \u2264 1: Trending series (positive autocorrelation)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array. Can contain NaN values which will be removed.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag to use in calculation. If None, uses n/4 where n is the sample size after removing NaN values.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The Hurst exponent, a value between 0 and 1. Returns np.nan if insufficient data or numerical issues.</p> <p>Raises:</p> Type Description <code>ValueError: If max_lag is negative</code> <code>Warning: If sample size is less than 10</code> <code>Warning: If sample size is less than 100</code>"},{"location":"api/parray/statistics/#pypulate.preprocessing.statistics.jarque_bera_test","title":"<code>jarque_bera_test(data)</code>","text":"<p>Perform Jarque-Bera test for normality.</p> <p>The Jarque-Bera test is a goodness-of-fit test that determines whether sample data have the skewness and kurtosis matching a normal distribution. The test statistic is always non-negative, with a larger value indicating a greater deviation from normality.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array. Can contain NaN values which will be removed before calculation.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: - test_statistic : float     The JB test statistic. A value close to 0 indicates normality.     The statistic follows a chi-squared distribution with 2 degrees of freedom     under the null hypothesis of normality. - p_value : float     The p-value for the test. A small p-value (e.g., &lt; 0.05) suggests     rejection of normality. Values close to 1 suggest normality.     Returns np.nan if insufficient data.</p> <p>Raises:</p> Type Description <code>Warning: If sample size is less than 3</code> <code>Warning: If max_lag is negative</code> <code>Warning: If max_lag is greater than or equal to the sample size</code>"},{"location":"api/parray/statistics/#pypulate.preprocessing.statistics.kolmogorov_smirnov_test","title":"<code>kolmogorov_smirnov_test(data, dist='norm', params=None)</code>","text":"<p>Perform Kolmogorov-Smirnov test for distribution fitting.</p> <p>The KS test examines whether a sample comes from a specified continuous distribution. The null hypothesis is that the sample is drawn from the reference distribution.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array. Can contain NaN values which will be removed.</p> required <code>dist</code> <code>str</code> <p>The reference distribution to test against. Options: - 'norm': Normal distribution - 'uniform': Uniform distribution - 'expon': Exponential distribution</p> <code>'norm'</code> <code>params</code> <code>dict</code> <p>Parameters for the reference distribution. If None, estimated from data. For 'norm': {'loc': mean, 'scale': std} For 'uniform': {'loc': min, 'scale': max-min} For 'expon': {'loc': min, 'scale': mean}</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: - test_statistic : float     The KS test statistic. Larger values indicate stronger evidence     against the null hypothesis. - p_value : float     The p-value for the test. Small p-values (e.g., &lt; 0.05) suggest     rejection of the null hypothesis, indicating the data does not     follow the specified distribution.     Returns np.nan if insufficient data.</p> <p>Raises:</p> Type Description <code>ValueError: If dist is not one of 'norm', 'uniform', 'expon'</code> <code>Warning: If sample size is less than 3</code> <code>Warning: If sample size is less than 30</code>"},{"location":"api/parray/statistics/#pypulate.preprocessing.statistics.kpss_test","title":"<code>kpss_test(data, regression='c', lags=None)</code>","text":"<p>Perform KPSS test for stationarity.</p> <p>The KPSS (Kwiatkowski-Phillips-Schmidt-Shin) test tests the null hypothesis that a time series is stationary around a deterministic trend. This test complements the ADF test, as the null hypothesis is stationarity (opposite to ADF).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array. Can contain NaN values which will be removed before calculation.</p> required <code>regression</code> <code>str</code> <p>The null hypothesis: - 'c': The series is stationary around a constant (level) - 'ct': The series is stationary around a trend</p> <code>'c'</code> <code>lags</code> <code>int</code> <p>Number of lags to use for Newey-West estimator. If None, uses automatic selection based on Schwert's rule: [12 * (n/100)^(1/4)]</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: - test_statistic : float     The KPSS test statistic. Larger values indicate stronger evidence     against the null hypothesis of stationarity. - p_value : float     The p-value for the test. Small p-values (e.g., &lt; 0.05) suggest     rejection of the null hypothesis, indicating non-stationarity.     Returns np.nan if insufficient data.</p> <p>Raises:</p> Type Description <code>ValueError: If regression is not one of 'c', 'ct'</code> <code>Warning: If sample size is less than 3</code> <code>Warning: If sample size is less than 30</code> <code>Warning: If max_lag is negative</code> <code>Warning: If max_lag is greater than or equal to the sample size</code>"},{"location":"api/parray/statistics/#pypulate.preprocessing.statistics.ljung_box_test","title":"<code>ljung_box_test(data, lags=10, boxpierce=False)</code>","text":"<p>Perform Ljung-Box test for autocorrelation in time series residuals.</p> <p>The Ljung-Box test examines whether there is significant autocorrelation in the residuals of a time series. The null hypothesis is that the data is independently distributed (no autocorrelation). The alternative hypothesis is that the data exhibits serial correlation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array (typically residuals). Can contain NaN values.</p> required <code>lags</code> <code>int</code> <p>Number of lags to test. Must be positive and less than the sample size.</p> <code>10</code> <code>boxpierce</code> <code>bool</code> <p>If True, compute the Box-Pierce statistic instead of the Ljung-Box statistic. The Box-Pierce statistic is a simpler version but is less powerful for small samples.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: - test_statistic : float     The Q-statistic (Ljung-Box or Box-Pierce). Larger values indicate stronger     evidence against the null hypothesis of no autocorrelation. - p_value : float     The p-value for the test. Small p-values (e.g., &lt; 0.05) suggest     rejection of the null hypothesis, indicating presence of autocorrelation.     Returns np.nan if insufficient data or numerical issues.</p> <p>Raises:</p> Type Description <code>ValueError: If lags is not positive</code> <code>Warning: If sample size is less than 3 times the number of lags</code> <code>Warning: If max_lag is negative</code> <code>Warning: If max_lag is greater than or equal to the sample size</code>"},{"location":"api/parray/statistics/#pypulate.preprocessing.statistics.partial_autocorrelation","title":"<code>partial_autocorrelation(data, max_lag=20)</code>","text":"<p>Calculate partial autocorrelation function (PACF) for time series data.</p> <p>The partial autocorrelation function measures the correlation between observations at different time lags after removing the effects of intermediate observations. It is particularly useful for identifying the order of an AR(p) process.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array. Can contain NaN values which will be removed before calculation.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag to calculate partial autocorrelation. Must be positive and less than the length of the series after removing NaN values.</p> <code>20</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Partial autocorrelation values for lags 0 to max_lag. Values range from -1 to 1, where: - Values close to \u00b11 indicate strong partial correlation - Values close to 0 indicate weak partial correlation - np.nan indicates insufficient data or constant series</p> <p>Raises:</p> Type Description <code>ValueError: If max_lag is negative</code>"},{"location":"api/parray/statistics/#pypulate.preprocessing.statistics.rolling_statistics","title":"<code>rolling_statistics(data, window, statistics=['mean', 'std'])</code>","text":"<p>Calculate rolling statistics for time series data.</p> <p>Computes various statistics over a rolling window of specified size. Missing values (NaN) at the start of the output array correspond to the first window-1 observations.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array. Can contain NaN values.</p> required <code>window</code> <code>int</code> <p>Size of the rolling window. Must be positive.</p> required <code>statistics</code> <code>list of str</code> <p>List of statistics to compute. Options: - 'mean': Rolling mean - 'std': Rolling standard deviation - 'min': Rolling minimum - 'max': Rolling maximum - 'median': Rolling median - 'skew': Rolling skewness - 'kurt': Rolling kurtosis</p> <code>['mean', 'std']</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with statistic names as keys and numpy arrays as values. Each array has the same length as the input data, with the first window-1 elements being NaN.</p> <p>Raises:</p> Type Description <code>ValueError: If window is not positive</code> <code>ValueError: If statistics is not a list</code> <code>ValueError: If any statistic in statistics is not in ['mean', 'std', 'min', 'max', 'median', 'skew', 'kurt']</code> <code>Warning: If sample size is less than window</code>"},{"location":"api/parray/statistics/#pypulate.preprocessing.statistics.variance_ratio_test","title":"<code>variance_ratio_test(data, periods=None, robust=True)</code>","text":"<p>Perform Variance Ratio test for random walk hypothesis.</p> <p>The Variance Ratio test examines whether a time series follows a random walk by comparing variances at different sampling intervals. The null hypothesis is that the series follows a random walk.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data array. Can contain NaN values which will be removed before calculation. Must be strictly positive for log returns calculation.</p> required <code>periods</code> <code>list of int</code> <p>List of periods for variance ratio calculations.</p> <code>[2, 4, 8, 16]</code> <code>robust</code> <code>bool</code> <p>If True, use heteroskedasticity-robust standard errors.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with periods as keys and tuples of (test_statistic, p_value) as values. - test_statistic : float     The VR test statistic. Values far from 1 indicate deviation from random walk. - p_value : float     The p-value for the test. Small p-values (e.g., &lt; 0.05) suggest     rejection of the null hypothesis of random walk.     Returns (0, 1) for constant series (perfect random walk).</p> <p>Raises:</p> Type Description <code>ValueError: If periods is None</code> <code>ValueError: If data is not strictly positive for log returns calculation</code> <code>ValueError: If all periods are not positive integers</code> <code>ValueError: If any period is larger than the sample size</code>"},{"location":"api/parray/transforms/","title":"sransforms API Reference","text":"<p>Wave and zigzag transforms for financial time series data.</p> <p>This module provides functions for extracting wave points and zigzag patterns from financial time series data, which are useful for technical analysis.</p>"},{"location":"api/parray/transforms/#pypulate.transforms.wave.wave","title":"<code>wave(open, high, low, close)</code>","text":"<p>Extract wave points from OHLC financial data.</p> <p>This function processes OHLC data to extract price points based on candlestick patterns, and removes consecutive points that follow the same trend direction.</p> <p>Parameters:</p> Name Type Description Default <code>open</code> <code>ndarray</code> <p>Array of opening prices</p> required <code>high</code> <code>ndarray</code> <p>Array of high prices</p> required <code>low</code> <code>ndarray</code> <p>Array of low prices</p> required <code>close</code> <code>ndarray</code> <p>Array of closing prices</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>2D array of wave points with shape (n, 2), where each row contains [index, price]</p> Notes <p>The algorithm works as follows: 1. For each candle:    - If close &gt; open: adds low then high to the price list    - If close &lt; open: adds high then low to the price list 2. Removes intermediate points where three consecutive points form a consistent trend    (either all increasing or all decreasing)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the price arrays have different lengths</p>"},{"location":"api/parray/transforms/#pypulate.transforms.wave.zigzag","title":"<code>zigzag(prices, threshold=0.03)</code>","text":"<p>Extract zigzag pivot points from price data based on a percentage threshold.</p> <p>Parameters:</p> Name Type Description Default <code>prices</code> <code>ArrayLike</code> <p>1D array/list of price values or 2D array/list of [index, price] points</p> required <code>threshold</code> <code>float</code> <p>Minimum percentage change required to identify a new pivot point (0.03 = 3%)</p> <code>0.03</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>2D array of zigzag points with shape (n, 2), where each row contains [index, price]</p> Notes <p>The algorithm identifies significant price movements while filtering out minor fluctuations. It marks pivot points where the price changes direction by at least the specified threshold percentage.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the price arrays have different lengths</p>"},{"location":"api/parray/technical/momentum/","title":"Momentum API Reference","text":"<p>Momentum Indicators Module</p> <p>This module provides functions for calculating momentum-based indicators for financial time series analysis.</p>"},{"location":"api/parray/technical/momentum/#pypulate.technical.momentum.adx","title":"<code>adx(data, period=14)</code>","text":"<p>Calculate Average Directional Index (ADX).</p> <p>ADX is a technical indicator used to determine the strength of a trend.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>period</code> <code>int</code> <p>Number of periods for calculation</p> <code>14</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>ADX values</p>"},{"location":"api/parray/technical/momentum/#pypulate.technical.momentum.cci","title":"<code>cci(close, period=20, constant=0.015)</code>","text":"<p>Calculate Commodity Channel Index (CCI) using close prices.</p> <p>CCI measures the current price level relative to an average price level over a given period. This version uses only close prices instead of typical prices for simplified calculation.</p> <p>Parameters:</p> Name Type Description Default <code>close</code> <code>ndarray</code> <p>Close prices</p> required <code>period</code> <code>int</code> <p>Number of periods for calculation</p> <code>20</code> <code>constant</code> <code>float</code> <p>Scaling constant</p> <code>0.015</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>CCI values</p>"},{"location":"api/parray/technical/momentum/#pypulate.technical.momentum.macd","title":"<code>macd(data, fast_period=12, slow_period=26, signal_period=9)</code>","text":"<p>Calculate Moving Average Convergence Divergence (MACD).</p> <p>MACD is a trend-following momentum indicator that shows the relationship between two moving averages of a security's price.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>fast_period</code> <code>int</code> <p>Period for the fast EMA</p> <code>12</code> <code>slow_period</code> <code>int</code> <p>Period for the slow EMA</p> <code>26</code> <code>signal_period</code> <code>int</code> <p>Period for the signal line (EMA of MACD line)</p> <code>9</code> <p>Returns:</p> Type Description <code>tuple of numpy.ndarray</code> <p>Tuple containing (macd_line, signal_line, histogram)</p>"},{"location":"api/parray/technical/momentum/#pypulate.technical.momentum.momentum","title":"<code>momentum(data, period=14)</code>","text":"<p>Calculate momentum over a specified period.</p> <p>Momentum measures the amount that a price has changed over a given period.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>period</code> <code>int</code> <p>Number of periods to calculate momentum</p> <code>14</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Momentum values</p>"},{"location":"api/parray/technical/momentum/#pypulate.technical.momentum.percent_change","title":"<code>percent_change(data, periods=1)</code>","text":"<p>Calculate percentage change between consecutive periods.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>periods</code> <code>int</code> <p>Number of periods to calculate change over</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Percentage change values</p>"},{"location":"api/parray/technical/momentum/#pypulate.technical.momentum.roc","title":"<code>roc(data, period=14)</code>","text":"<p>Calculate Rate of Change (ROC) over a specified period.</p> <p>ROC measures the percentage change in price over a given period.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>period</code> <code>int</code> <p>Number of periods to calculate ROC</p> <code>14</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>ROC values in percentage</p>"},{"location":"api/parray/technical/momentum/#pypulate.technical.momentum.rsi","title":"<code>rsi(data, period=14, smoothing_type='sma')</code>","text":"<p>Calculate Relative Strength Index (RSI) over a specified period.</p> <p>RSI measures the speed and change of price movements, indicating overbought (&gt;70) or oversold (&lt;30) conditions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>period</code> <code>int</code> <p>Number of periods to calculate RSI</p> <code>14</code> <code>smoothing_type</code> <code>str</code> <p>Type of smoothing to use: 'sma' (Simple Moving Average) or  'ema' (Exponential Moving Average)</p> <code>'sma'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>RSI values (0-100)</p>"},{"location":"api/parray/technical/momentum/#pypulate.technical.momentum.stochastic_oscillator","title":"<code>stochastic_oscillator(close, high, low, k_period=14, d_period=3)</code>","text":"<p>Calculate Stochastic Oscillator.</p> <p>The Stochastic Oscillator is a momentum indicator that shows the location of the close relative to the high-low range over a set number of periods.</p> <p>Parameters:</p> Name Type Description Default <code>close</code> <code>ndarray</code> <p>Close prices</p> required <code>high</code> <code>ndarray</code> <p>High prices. If None, assumes close contains close prices and high=low=close</p> required <code>low</code> <code>ndarray</code> <p>Low prices. If None, assumes close contains close prices and high=low=close</p> required <code>k_period</code> <code>int</code> <p>Number of periods for %K</p> <code>14</code> <code>d_period</code> <code>int</code> <p>Number of periods for %D (moving average of %K)</p> <code>3</code> <p>Returns:</p> Type Description <code>tuple of numpy.ndarray</code> <p>Tuple containing (%K, %D)</p>"},{"location":"api/parray/technical/momentum/#pypulate.technical.momentum.tsi","title":"<code>tsi(data, long_period=25, short_period=13, signal_period=7)</code>","text":"<p>Calculate True Strength Index (TSI).</p> <p>TSI is a momentum oscillator that helps identify trends and reversals.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>long_period</code> <code>int</code> <p>Long period for double smoothing</p> <code>25</code> <code>short_period</code> <code>int</code> <p>Short period for double smoothing</p> <code>13</code> <code>signal_period</code> <code>int</code> <p>Period for the signal line</p> <code>7</code> <p>Returns:</p> Type Description <code>tuple of numpy.ndarray</code> <p>Tuple containing (tsi_line, signal_line)</p>"},{"location":"api/parray/technical/momentum/#pypulate.technical.momentum.williams_r","title":"<code>williams_r(close, high=None, low=None, period=14)</code>","text":"<p>Calculate Williams %R.</p> <p>Williams %R is a momentum indicator that measures overbought and oversold levels.</p> <p>Parameters:</p> Name Type Description Default <code>close</code> <code>ndarray</code> <p>Close prices</p> required <code>high</code> <code>ndarray</code> <p>High prices. If None, assumes close contains close prices and high=low=close</p> <code>None</code> <code>low</code> <code>ndarray</code> <p>Low prices. If None, assumes close contains close prices and high=low=close</p> <code>None</code> <code>period</code> <code>int</code> <p>Number of periods for calculation</p> <code>14</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Williams %R values (-100 to 0)</p>"},{"location":"api/parray/technical/utils/","title":"Utils API Reference","text":"<p>KPI Utility Functions</p> <p>This module provides utility functions for calculating Key Performance Indicators (KPIs) for financial time series analysis.</p>"},{"location":"api/parray/technical/utils/#pypulate.technical.utils.diff","title":"<code>diff(data, periods=1)</code>","text":"<p>Calculate difference between consecutive values.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>periods</code> <code>int</code> <p>Number of periods to calculate difference over</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Difference values</p>"},{"location":"api/parray/technical/utils/#pypulate.technical.utils.log","title":"<code>log(data)</code>","text":"<p>Calculate the natural logarithm of price data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input price data as list or numpy array</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Natural logarithm of the input data. Returns NaN for any non-positive values.</p>"},{"location":"api/parray/technical/utils/#pypulate.technical.utils.rolling_kurtosis","title":"<code>rolling_kurtosis(data, period=14)</code>","text":"<p>Calculate rolling kurtosis over a specified period.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>period</code> <code>int</code> <p>Window size for rolling kurtosis</p> <code>14</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Rolling kurtosis values</p>"},{"location":"api/parray/technical/utils/#pypulate.technical.utils.rolling_max","title":"<code>rolling_max(data, period=14)</code>","text":"<p>Calculate rolling maximum over a specified period.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>period</code> <code>int</code> <p>Window size for rolling maximum</p> <code>14</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Rolling maximum values</p>"},{"location":"api/parray/technical/utils/#pypulate.technical.utils.rolling_min","title":"<code>rolling_min(data, period=14)</code>","text":"<p>Calculate rolling minimum over a specified period.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>period</code> <code>int</code> <p>Window size for rolling minimum</p> <code>14</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Rolling minimum values</p>"},{"location":"api/parray/technical/utils/#pypulate.technical.utils.rolling_skew","title":"<code>rolling_skew(data, period=14)</code>","text":"<p>Calculate rolling skewness over a specified period.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>period</code> <code>int</code> <p>Window size for rolling skewness</p> <code>14</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Rolling skewness values</p>"},{"location":"api/parray/technical/utils/#pypulate.technical.utils.rolling_std","title":"<code>rolling_std(data, period=14)</code>","text":"<p>Calculate rolling standard deviation over a specified period.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>period</code> <code>int</code> <p>Window size for rolling standard deviation</p> <code>14</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Rolling standard deviation values</p>"},{"location":"api/parray/technical/utils/#pypulate.technical.utils.rolling_var","title":"<code>rolling_var(data, period=14)</code>","text":"<p>Calculate rolling variance over a specified period.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>period</code> <code>int</code> <p>Window size for rolling variance</p> <code>14</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Rolling variance values</p>"},{"location":"api/parray/technical/utils/#pypulate.technical.utils.slope","title":"<code>slope(data, period=5)</code>","text":"<p>Calculate the slope of the time series over a specified period.</p> <p>This function uses linear regression to calculate the slope of the line that best fits the data over the specified period.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>period</code> <code>int</code> <p>Number of points to use for slope calculation</p> <code>5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Slope values for each point in the time series</p>"},{"location":"api/parray/technical/utils/#pypulate.technical.utils.typical_price","title":"<code>typical_price(close, high, low)</code>","text":"<p>Calculate the typical price from close, high, and low prices.</p> <p>Parameters:</p> Name Type Description Default <code>close</code> <code>ndarray</code> <p>Close prices</p> required <code>high</code> <code>ndarray</code> <p>High prices</p> required <code>low</code> <code>ndarray</code> <p>Low prices</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Typical price values</p>"},{"location":"api/parray/technical/utils/#pypulate.technical.utils.zscore","title":"<code>zscore(data, period=14)</code>","text":"<p>Calculate rolling Z-score over a specified period.</p> <p>Z-score measures how many standard deviations a data point is from the mean.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>period</code> <code>int</code> <p>Window size for Z-score calculation</p> <code>14</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Z-score values</p>"},{"location":"api/parray/technical/volatility/","title":"Volatility API Reference","text":"<p>Volatility Measurement Functions</p> <p>This module provides functions for measuring volatility in financial time series data.</p>"},{"location":"api/parray/technical/volatility/#pypulate.technical.volatility.atr","title":"<code>atr(close, high, low, period=14)</code>","text":"<p>Calculate Average True Range (ATR) over a specified period.</p> <p>ATR measures market volatility by decomposing the entire range of an asset price.</p> <p>Parameters:</p> Name Type Description Default <code>close</code> <code>ndarray</code> <p>Close prices</p> required <code>high</code> <code>ndarray</code> <p>High prices. If None, assumes close contains close prices and high=low=close</p> required <code>low</code> <code>ndarray</code> <p>Low prices. If None, assumes close contains close prices and high=low=close</p> required <code>period</code> <code>int</code> <p>Number of periods to calculate ATR</p> <code>14</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>ATR values</p>"},{"location":"api/parray/technical/volatility/#pypulate.technical.volatility.bollinger_bands","title":"<code>bollinger_bands(data, period=20, std_dev=2.0)</code>","text":"<p>Calculate Bollinger Bands over a specified period.</p> <p>Bollinger Bands consist of a middle band (SMA), an upper band (SMA + kstd), and a lower band (SMA - kstd).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>period</code> <code>int</code> <p>Number of periods for the moving average</p> <code>20</code> <code>std_dev</code> <code>float</code> <p>Number of standard deviations for the upper and lower bands</p> <code>2.0</code> <p>Returns:</p> Type Description <code>tuple of numpy.ndarray</code> <p>Tuple containing (upper_band, middle_band, lower_band)</p>"},{"location":"api/parray/technical/volatility/#pypulate.technical.volatility.donchian_channels","title":"<code>donchian_channels(data, high, low, period=20)</code>","text":"<p>Calculate Donchian Channels over a specified period.</p> <p>Donchian Channels consist of an upper band (highest high), a lower band (lowest low), and a middle band (average of upper and lower).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data (typically close prices)</p> required <code>high</code> <code>ndarray</code> <p>High prices. If None, uses data</p> required <code>low</code> <code>ndarray</code> <p>Low prices. If None, uses data</p> required <code>period</code> <code>int</code> <p>Number of periods for the channels</p> <code>20</code> <p>Returns:</p> Type Description <code>tuple of numpy.ndarray</code> <p>Tuple containing (upper_channel, middle_channel, lower_channel)</p>"},{"location":"api/parray/technical/volatility/#pypulate.technical.volatility.historical_volatility","title":"<code>historical_volatility(data, period=21, annualization_factor=252)</code>","text":"<p>Calculate historical volatility over a specified period.</p> <p>Historical volatility is the standard deviation of log returns, typically annualized.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>period</code> <code>int</code> <p>Number of periods to calculate volatility</p> <code>21</code> <code>annualization_factor</code> <code>int</code> <p>Factor to annualize volatility (252 for daily data, 52 for weekly, 12 for monthly)</p> <code>252</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Historical volatility values as percentage</p>"},{"location":"api/parray/technical/volatility/#pypulate.technical.volatility.keltner_channels","title":"<code>keltner_channels(close, high, low, period=20, atr_period=10, multiplier=2.0)</code>","text":"<p>Calculate Keltner Channels over a specified period.</p> <p>Keltner Channels consist of a middle band (EMA), an upper band (EMA + kATR), and a lower band (EMA - kATR).</p> <p>Parameters:</p> Name Type Description Default <code>close</code> <code>ndarray</code> <p>Close prices</p> required <code>high</code> <code>ndarray</code> <p>High prices. If None, assumes close contains close prices and high=low=close</p> required <code>low</code> <code>ndarray</code> <p>Low prices. If None, assumes close contains close prices and high=low=close</p> required <code>period</code> <code>int</code> <p>Number of periods for the EMA</p> <code>20</code> <code>atr_period</code> <code>int</code> <p>Number of periods for the ATR</p> <code>10</code> <code>multiplier</code> <code>float</code> <p>Multiplier for the ATR</p> <code>2.0</code> <p>Returns:</p> Type Description <code>tuple of numpy.ndarray</code> <p>Tuple containing (upper_channel, middle_channel, lower_channel)</p>"},{"location":"api/parray/technical/volatility/#pypulate.technical.volatility.volatility_ratio","title":"<code>volatility_ratio(data, period=21, smooth_period=5)</code>","text":"<p>Calculate Volatility Ratio over a specified period.</p> <p>Volatility Ratio compares recent volatility to historical volatility. Values above 1 indicate increasing volatility, values below 1 indicate decreasing volatility.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input time series data</p> required <code>period</code> <code>int</code> <p>Number of periods for historical volatility</p> <code>21</code> <code>smooth_period</code> <code>int</code> <p>Number of periods to smooth the ratio</p> <code>5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Volatility Ratio values</p>"},{"location":"api/portfolio/portfolio/","title":"Portfolio API Reference","text":"<p>Portfolio Module</p> <p>This module provides a class for calculating various portfolio metrics including returns, risk-adjusted performance, and risk measurements.</p>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio","title":"<code>Portfolio</code>","text":"<p>A class for calculating various portfolio metrics and assessing portfolio health.</p> <p>This class provides methods for calculating portfolio returns, risk-adjusted performance metrics, and risk measurements, while maintaining state to assess overall portfolio health.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pypulate.dtypes import Portfolio\n&gt;&gt;&gt; portfolio = Portfolio()\n&gt;&gt;&gt; returns = portfolio.simple_return(105, 100)\n&gt;&gt;&gt; sharpe = portfolio.sharpe_ratio([0.01, 0.02, -0.01, 0.03, 0.01])\n&gt;&gt;&gt; health = portfolio.health\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.health","title":"<code>health</code>  <code>property</code>","text":"<p>Calculate and return the overall health of the portfolio based on stored metrics.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing: - overall_score: Float between 0 and 100 - status: String indicating health status - components: Dictionary of component scores and metrics     - returns: Return metrics and score     - risk_adjusted: Risk-adjusted performance metrics and score     - risk: Risk metrics and score</p>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Portfolio class with empty state.</p>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.annualized_return","title":"<code>annualized_return(total_return, years)</code>","text":"<p>Calculate the annualized return from a total return over a period of years.</p> <p>Parameters:</p> Name Type Description Default <code>total_return</code> <code>float or array - like</code> <p>The total return over the entire period as a decimal</p> required <code>years</code> <code>float or array - like</code> <p>The number of years in the period</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>The annualized return as a decimal If array inputs are provided, returns an array of annualized returns</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; annualized_return(0.2, 2)\n0.09544511501033215\n&gt;&gt;&gt; annualized_return([0.2, 0.3, 0.15], [2, 3, 1.5])\n[0.09544512, 0.09139288, 0.0976534 ]\n&gt;&gt;&gt; annualized_return(np.array([0.4, 0.5]), 2)\n[0.18321596, 0.22474487]\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.arithmetic_return","title":"<code>arithmetic_return(prices)</code>","text":"<p>Calculate the arithmetic average return from a series of prices.</p> <p>Parameters:</p> Name Type Description Default <code>prices</code> <code>array - like</code> <p>Array or list of prices</p> required <p>Returns:</p> Type Description <code>float</code> <p>The arithmetic average return as a decimal</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arithmetic_return([100, 105, 103, 108, 110])\n0.024503647197821957\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.benchmark_alpha","title":"<code>benchmark_alpha(returns, benchmark_returns)</code>","text":"<p>Calculate the benchmark alpha, which is the difference between portfolio return and benchmark return.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array of portfolio returns</p> required <code>benchmark_returns</code> <code>array - like</code> <p>Array of benchmark returns for the same periods</p> required <p>Returns:</p> Type Description <code>float</code> <p>The benchmark alpha (difference in mean returns)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; benchmark_alpha([0.01, 0.02, -0.01, 0.03, 0.01], [0.005, 0.01, -0.005, 0.02, 0.005])\n0.005\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.beta_adjusted_return","title":"<code>beta_adjusted_return(portfolio_return, benchmark_return, portfolio_beta)</code>","text":"<p>Calculate the beta-adjusted return (alpha) of a portfolio.</p> <p>Parameters:</p> Name Type Description Default <code>portfolio_return</code> <code>float or array - like</code> <p>The return of the portfolio as a decimal</p> required <code>benchmark_return</code> <code>float or array - like</code> <p>The return of the benchmark as a decimal</p> required <code>portfolio_beta</code> <code>float or array - like</code> <p>The beta of the portfolio relative to the benchmark</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>The beta-adjusted return (alpha) as a decimal If array inputs are provided, returns an array of beta-adjusted returns</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; beta_adjusted_return(0.12, 0.10, 1.2)\n0.0\n&gt;&gt;&gt; beta_adjusted_return([0.12, 0.15], [0.10, 0.08], 1.2)\n[0.   , 0.054]\n&gt;&gt;&gt; beta_adjusted_return(0.12, 0.10, [1.2, 1.5])\n[ 0.  , -0.03]\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.calmar_ratio","title":"<code>calmar_ratio(returns, max_drawdown=None, annualization_factor=1.0)</code>","text":"<p>Calculate the Calmar ratio, which measures return relative to maximum drawdown.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array of portfolio returns</p> required <code>max_drawdown</code> <code>float</code> <p>Maximum drawdown as a positive decimal. If None, it will be calculated from returns.</p> <code>None</code> <code>annualization_factor</code> <code>float</code> <p>Factor to annualize returns</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float</code> <p>The Calmar ratio</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; calmar_ratio([0.01, 0.02, -0.01, 0.03, 0.01], 0.15, 252)\n0.8\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.capm_alpha","title":"<code>capm_alpha(returns, benchmark_returns, risk_free_rate=0.0)</code>","text":"<p>Calculate the CAPM alpha (Jensen's alpha) and related statistics.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array of portfolio returns</p> required <code>benchmark_returns</code> <code>array - like</code> <p>Array of benchmark returns for the same periods</p> required <code>risk_free_rate</code> <code>float or array - like</code> <p>Risk-free rate for the same period as returns</p> <code>0.0</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(alpha, beta, r_squared, p_value, std_err) - alpha: The CAPM alpha (intercept) - beta: The CAPM beta (slope) - r_squared: The R-squared of the regression - p_value: The p-value for alpha - std_err: The standard error of alpha</p>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.capm_beta","title":"<code>capm_beta(portfolio_returns, market_returns)</code>","text":"<p>Calculate the CAPM beta of a portfolio.</p> <p>Parameters:</p> Name Type Description Default <code>portfolio_returns</code> <code>list or ndarray</code> <p>Array or list of portfolio returns</p> required <code>market_returns</code> <code>list or ndarray</code> <p>Array or list of market returns</p> required <p>Returns:</p> Type Description <code>float</code> <p>CAPM beta</p> Notes <p>Beta measures the sensitivity of portfolio returns to market returns. It is the covariance of portfolio returns and market returns divided by the variance of market returns.</p>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.conditional_value_at_risk","title":"<code>conditional_value_at_risk(returns, confidence_level=0.95, method='historical', current_value=1.0)</code>","text":"<p>Calculate the Conditional Value-at-Risk (CVaR) of a portfolio.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>list or ndarray</code> <p>Array or list of returns</p> required <code>confidence_level</code> <code>float</code> <p>Confidence level for CVaR calculation (e.g., 0.95 for 95% confidence)</p> <code>0.95</code> <code>method</code> <code>str</code> <p>Method for calculating CVaR ('historical' or 'parametric')</p> <code>'historical'</code> <code>current_value</code> <code>float</code> <p>Current value of the portfolio</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float</code> <p>Conditional Value-at-Risk (CVaR) as a positive number representing the potential loss</p> Notes <p>CVaR, also known as Expected Shortfall, measures the expected loss given that the loss exceeds the VaR threshold. It provides a more conservative risk measure than VaR.</p>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.correlation_matrix","title":"<code>correlation_matrix(returns_matrix)</code>","text":"<p>Calculate the correlation matrix of returns.</p> <p>Parameters:</p> Name Type Description Default <code>returns_matrix</code> <code>list of lists or np.ndarray</code> <p>Matrix of returns where each column represents an asset</p> required <p>Returns:</p> Type Description <code>np.ndarray or list of lists</code> <p>Correlation matrix</p> Notes <p>The correlation matrix measures the strength of the relationship between returns of different assets, normalized to be between -1 and 1.</p>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.covariance_matrix","title":"<code>covariance_matrix(returns_matrix)</code>","text":"<p>Calculate the covariance matrix of returns.</p> <p>Parameters:</p> Name Type Description Default <code>returns_matrix</code> <code>list of lists or np.ndarray</code> <p>Matrix of returns where each column represents an asset</p> required <p>Returns:</p> Type Description <code>np.ndarray or list of lists</code> <p>Covariance matrix</p> Notes <p>The covariance matrix measures how returns of different assets move together.</p>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.dollar_weighted_return","title":"<code>dollar_weighted_return(cash_flows, cash_flow_dates, end_value)</code>","text":"<p>Calculate the dollar-weighted return (internal rate of return) for a series of cash flows.</p> <p>Parameters:</p> Name Type Description Default <code>cash_flows</code> <code>array - like</code> <p>Array or list of cash flows (positive for inflows, negative for outflows)</p> required <code>cash_flow_dates</code> <code>array - like</code> <p>Array or list of dates (in days) when each cash flow occurs</p> required <code>end_value</code> <code>float</code> <p>The final value of the investment</p> required <p>Returns:</p> Type Description <code>float</code> <p>The dollar-weighted return as a decimal</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dollar_weighted_return([-1000, -500, 200], [0, 30, 60], 1400)\n0.36174448410245186\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.drawdown","title":"<code>drawdown(returns, as_list=False)</code>","text":"<p>Calculate drawdown metrics.</p>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.geometric_return","title":"<code>geometric_return(prices)</code>","text":"<p>Calculate the geometric average return from a series of prices.</p> <p>Parameters:</p> Name Type Description Default <code>prices</code> <code>array - like</code> <p>Array or list of prices</p> required <p>Returns:</p> Type Description <code>float</code> <p>The geometric average return as a decimal</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; geometric_return([100, 105, 103, 108, 110])\n0.02411368908444511\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.holding_period_return","title":"<code>holding_period_return(prices, dividends=None)</code>","text":"<p>Calculate the holding period return for a series of prices and optional dividends.</p> <p>Parameters:</p> Name Type Description Default <code>prices</code> <code>array - like</code> <p>Array or list of prices over the holding period</p> required <code>dividends</code> <code>array - like</code> <p>Array or list of dividends paid during the holding period</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The holding period return as a decimal</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; holding_period_return([100, 102, 105, 103, 106])\n0.06\n&gt;&gt;&gt; holding_period_return([100, 102, 105, 103, 106], [0, 1, 0, 2, 0])\n0.09\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.information_ratio","title":"<code>information_ratio(returns, benchmark_returns, annualization_factor=1.0)</code>","text":"<p>Calculate the Information ratio, which measures excess return per unit of tracking error.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array of portfolio returns</p> required <code>benchmark_returns</code> <code>array - like</code> <p>Array of benchmark returns for the same periods</p> required <code>annualization_factor</code> <code>float</code> <p>Factor to annualize the Information ratio (e.g., 252 for daily returns to annual)</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float</code> <p>The Information ratio</p>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.leveraged_return","title":"<code>leveraged_return(unleveraged_return, leverage_ratio, borrowing_rate)</code>","text":"<p>Calculate the return of a leveraged portfolio.</p> <p>Parameters:</p> Name Type Description Default <code>unleveraged_return</code> <code>float or array - like</code> <p>The return of the unleveraged portfolio as a decimal</p> required <code>leverage_ratio</code> <code>float or array - like</code> <p>The leverage ratio (e.g., 2.0 for 2:1 leverage)</p> required <code>borrowing_rate</code> <code>float or array - like</code> <p>The borrowing rate as a decimal</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>The leveraged return as a decimal If array inputs are provided, returns an array of leveraged returns</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; leveraged_return(0.10, 2.0, 0.05)\n0.15\n&gt;&gt;&gt; leveraged_return([0.10, 0.15], [2.0, 1.5], 0.05)\n[0.15, 0.2 ]\n&gt;&gt;&gt; leveraged_return(0.10, [2.0, 3.0], [0.05, 0.06])\n[0.15, 0.18]\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.linked_modified_dietz_return","title":"<code>linked_modified_dietz_return(period_returns)</code>","text":"<p>Calculate the linked Modified Dietz return over multiple periods.</p> <p>Parameters:</p> Name Type Description Default <code>period_returns</code> <code>array - like</code> <p>Array or list of Modified Dietz returns for each period</p> required <p>Returns:</p> Type Description <code>float</code> <p>The linked Modified Dietz return as a decimal</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; linked_modified_dietz_return([0.05, -0.02, 0.03, 0.04])\n0.10226479999999993\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.log_return","title":"<code>log_return(end_value, start_value)</code>","text":"<p>Calculate the logarithmic (continuously compounded) return between two values.</p> <p>Parameters:</p> Name Type Description Default <code>end_value</code> <code>float or array - like</code> <p>The ending value(s) of the investment</p> required <code>start_value</code> <code>float or array - like</code> <p>The starting value(s) of the investment</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>The logarithmic return If array inputs are provided, returns an array of logarithmic returns</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; log_return(105, 100)\n0.04879016416929972\n&gt;&gt;&gt; log_return([105, 110, 108], [100, 100, 100])\narray([0.04879016, 0.09531018, 0.07696104])\n&gt;&gt;&gt; log_return(np.array([105, 110]), np.array([100, 100]))\narray([0.04879016, 0.09531018])\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.long_short_equity_return","title":"<code>long_short_equity_return(long_portfolio_return, short_portfolio_return, long_exposure, short_exposure, risk_free_rate=0.0, short_rebate=0.0)</code>","text":"<p>Calculate the return of a long-short equity portfolio.</p> <p>Parameters:</p> Name Type Description Default <code>long_portfolio_return</code> <code>float or array - like</code> <p>The return of the long portfolio as a decimal</p> required <code>short_portfolio_return</code> <code>float or array - like</code> <p>The return of the short portfolio as a decimal</p> required <code>long_exposure</code> <code>float or array - like</code> <p>The exposure of the long portfolio as a decimal of NAV</p> required <code>short_exposure</code> <code>float or array - like</code> <p>The exposure of the short portfolio as a decimal of NAV</p> required <code>risk_free_rate</code> <code>float or array - like</code> <p>The risk-free rate as a decimal</p> <code>0.0</code> <code>short_rebate</code> <code>float or array - like</code> <p>The rebate received on short proceeds as a decimal</p> <code>0.0</code> <p>Returns:</p> Type Description <code>float or ndarray</code> <p>The return of the long-short equity portfolio as a decimal If array inputs are provided, returns an array of long-short equity returns</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; long_short_equity_return(0.10, -0.05, 1.0, 0.5, 0.02, 0.01)\n0.14\n&gt;&gt;&gt; long_short_equity_return([0.10, 0.12], [-0.05, -0.03], 1.0, 0.5, 0.02, 0.01)\n[0.14, 0.15]\n&gt;&gt;&gt; long_short_equity_return(0.10, -0.05, [1.0, 0.8], [0.5, 0.4], [0.02, 0.03], 0.01)\n[0.14 , 0.122]\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.market_neutral_return","title":"<code>market_neutral_return(long_return, short_return, long_weight=0.5, short_weight=0.5, short_borrowing_cost=0.0)</code>","text":"<p>Calculate the return of a market-neutral portfolio with long and short positions.</p> <p>Parameters:</p> Name Type Description Default <code>long_return</code> <code>float or array - like</code> <p>The return of the long portfolio as a decimal</p> required <code>short_return</code> <code>float or array - like</code> <p>The return of the short portfolio as a decimal</p> required <code>long_weight</code> <code>float or array - like</code> <p>The weight of the long portfolio</p> <code>0.5</code> <code>short_weight</code> <code>float or array - like</code> <p>The weight of the short portfolio</p> <code>0.5</code> <code>short_borrowing_cost</code> <code>float or array - like</code> <p>The cost of borrowing for the short position as a decimal</p> <code>0.0</code> <p>Returns:</p> Type Description <code>float or ndarray</code> <p>The market-neutral return as a decimal If array inputs are provided, returns an array of market-neutral returns</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; market_neutral_return(0.08, -0.05, 0.6, 0.4, 0.01)\n0.064\n&gt;&gt;&gt; market_neutral_return([0.08, 0.10], [-0.05, -0.03], 0.6, 0.4, 0.01)\n[0.064, 0.068]\n&gt;&gt;&gt; market_neutral_return(0.08, -0.05, [0.6, 0.7], [0.4, 0.3], [0.01, 0.02])\n[0.064, 0.065]\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.modified_dietz_return","title":"<code>modified_dietz_return(start_value, end_value, cash_flows, cash_flow_days, total_days)</code>","text":"<p>Calculate the Modified Dietz return, which approximates the money-weighted return.</p> <p>Parameters:</p> Name Type Description Default <code>start_value</code> <code>float</code> <p>The starting value of the investment</p> required <code>end_value</code> <code>float</code> <p>The ending value of the investment</p> required <code>cash_flows</code> <code>array - like</code> <p>Array or list of cash flows (positive for inflows, negative for outflows)</p> required <code>cash_flow_days</code> <code>array - like</code> <p>Array or list of days when each cash flow occurs (day 0 is the start)</p> required <code>total_days</code> <code>int</code> <p>Total number of days in the period</p> required <p>Returns:</p> Type Description <code>float</code> <p>The Modified Dietz return as a decimal</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; modified_dietz_return(1000, 1200, [100, -50], [10, 20], 30)\n0.14285714285714285\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.money_weighted_return","title":"<code>money_weighted_return(cash_flows, cash_flow_times, final_value, initial_value=0, max_iterations=100, tolerance=1e-06)</code>","text":"<p>Calculate the money-weighted return (internal rate of return) for a series of cash flows.</p> <p>Parameters:</p> Name Type Description Default <code>cash_flows</code> <code>array - like</code> <p>Array or list of cash flows (positive for inflows, negative for outflows)</p> required <code>cash_flow_times</code> <code>array - like</code> <p>Array or list of times (in years) when each cash flow occurs</p> required <code>final_value</code> <code>float</code> <p>The final value of the investment</p> required <code>initial_value</code> <code>float</code> <p>The initial value of the investment</p> <code>0</code> <code>max_iterations</code> <code>int</code> <p>Maximum number of iterations for the numerical solver</p> <code>100</code> <code>tolerance</code> <code>float</code> <p>Convergence tolerance for the numerical solver</p> <code>1e-6</code> <p>Returns:</p> Type Description <code>float</code> <p>The money-weighted return (IRR) as a decimal</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; money_weighted_return([-1000, -500, 1700], [0, 0.5, 1], 0)\n0.16120409753798307\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.multifactor_alpha","title":"<code>multifactor_alpha(returns, factor_returns, risk_free_rate=0.0)</code>","text":"<p>Calculate the alpha from a multifactor model (e.g., Fama-French).</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array of portfolio returns</p> required <code>factor_returns</code> <code>array - like</code> <p>2D array where each column represents returns for a factor</p> required <code>risk_free_rate</code> <code>float or array - like</code> <p>Risk-free rate for the same period as returns</p> <code>0.0</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(alpha, betas, r_squared, p_value, std_err) - alpha: The multifactor alpha (intercept) - betas: Array of factor betas (coefficients) - r_squared: The R-squared of the regression - p_value: The p-value for alpha - std_err: The standard error of alpha</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Example with market, size, and value factors\n&gt;&gt;&gt; portfolio_returns = [0.01, 0.02, -0.01, 0.03, 0.01]\n&gt;&gt;&gt; factor_returns = [\n...     [0.005, 0.01, -0.005, 0.02, 0.005],  # Market\n...     [0.002, 0.003, -0.001, 0.004, 0.001],  # Size\n...     [0.001, 0.002, -0.002, 0.003, 0.002]   # Value\n... ]\n&gt;&gt;&gt; multifactor_alpha(portfolio_returns, factor_returns, 0.001)\n(0.0032, array([0.9, 0.5, 0.3]), 0.92, 0.04, 0.0015)  # Example values\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.omega_ratio","title":"<code>omega_ratio(returns, threshold=0.0, annualization_factor=1.0)</code>","text":"<p>Calculate the Omega ratio, which measures the probability-weighted ratio of gains versus losses.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array of portfolio returns</p> required <code>threshold</code> <code>float</code> <p>The threshold return</p> <code>0.0</code> <code>annualization_factor</code> <code>float</code> <p>Factor to annualize the threshold</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float</code> <p>The Omega ratio</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; omega_ratio([0.01, 0.02, -0.01, 0.03, 0.01], 0.005)\n2.0\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.semi_standard_deviation","title":"<code>semi_standard_deviation(returns, threshold=0.0, annualize=False, periods_per_year=252)</code>","text":"<p>Calculate the semi-standard deviation of returns below a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>list or ndarray</code> <p>Array or list of returns</p> required <code>threshold</code> <code>float</code> <p>Threshold below which to calculate semi-standard deviation</p> <code>0.0</code> <code>annualize</code> <code>bool</code> <p>Whether to annualize the semi-standard deviation</p> <code>False</code> <code>periods_per_year</code> <code>int</code> <p>Number of periods in a year (252 for daily returns, 12 for monthly, 4 for quarterly)</p> <code>252</code> <p>Returns:</p> Type Description <code>float</code> <p>Semi-standard deviation of returns</p> Notes <p>Semi-standard deviation only considers returns below the threshold (typically 0), making it a measure of downside risk.</p>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.sharpe_ratio","title":"<code>sharpe_ratio(returns, risk_free_rate=0.0, annualization_factor=1.0)</code>","text":"<p>Calculate the Sharpe ratio, which measures excess return per unit of risk.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array of portfolio returns</p> required <code>risk_free_rate</code> <code>float or array - like</code> <p>Risk-free rate for the same period as returns</p> <code>0.0</code> <code>annualization_factor</code> <code>float</code> <p>Factor to annualize the Sharpe ratio (e.g., 252 for daily returns to annual)</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float or ndarray</code> <p>The Sharpe ratio</p>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.simple_return","title":"<code>simple_return(end_value, start_value)</code>","text":"<p>Calculate the simple return (percentage change) between two values.</p> <p>Parameters:</p> Name Type Description Default <code>end_value</code> <code>float or array - like</code> <p>The ending value(s) of the investment</p> required <code>start_value</code> <code>float or array - like</code> <p>The starting value(s) of the investment</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>The simple return as a decimal (e.g., 0.05 for 5%) If array inputs are provided, returns an array of simple returns</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; simple_return(105, 100)\n0.05\n&gt;&gt;&gt; simple_return([105, 110, 108], [100, 100, 100])\narray([0.05, 0.1 , 0.08])\n&gt;&gt;&gt; simple_return(np.array([105, 110]), np.array([100, 100]))\narray([0.05, 0.1 ])\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.sortino_ratio","title":"<code>sortino_ratio(returns, risk_free_rate=0.0, target_return=0.0, annualization_factor=1.0)</code>","text":"<p>Calculate the Sortino ratio, which measures excess return per unit of downside risk.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array of portfolio returns</p> required <code>risk_free_rate</code> <code>float or array - like</code> <p>Risk-free rate for the same period as returns</p> <code>0.0</code> <code>target_return</code> <code>float</code> <p>Minimum acceptable return</p> <code>0.0</code> <code>annualization_factor</code> <code>float</code> <p>Factor to annualize the Sortino ratio</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float</code> <p>The Sortino ratio</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sortino_ratio([0.01, 0.02, -0.01, 0.03, 0.01], 0.001, 0.0, 252)\n3.7947331922020545\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.standard_deviation","title":"<code>standard_deviation(returns, annualize=False, periods_per_year=252)</code>","text":"<p>Calculate the standard deviation of returns.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>list or ndarray</code> <p>Array or list of returns</p> required <code>annualize</code> <code>bool</code> <p>Whether to annualize the standard deviation</p> <code>False</code> <code>periods_per_year</code> <code>int</code> <p>Number of periods in a year (252 for daily returns, 12 for monthly, 4 for quarterly)</p> <code>252</code> <p>Returns:</p> Type Description <code>float</code> <p>Standard deviation of returns</p> Notes <p>Standard deviation measures the dispersion of returns around the mean. It is the square root of the variance.</p>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.time_weighted_return","title":"<code>time_weighted_return(period_returns)</code>","text":"<p>Calculate the time-weighted return from a series of period returns.</p> <p>Parameters:</p> Name Type Description Default <code>period_returns</code> <code>array - like</code> <p>Array or list of returns for each period</p> required <p>Returns:</p> Type Description <code>float</code> <p>The time-weighted return as a decimal</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; time_weighted_return([0.05, -0.02, 0.03, 0.04])\n0.10226479999999993\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.total_return_index","title":"<code>total_return_index(prices, dividends=None)</code>","text":"<p>Calculate the total return index from a series of prices and optional dividends.</p> <p>Parameters:</p> Name Type Description Default <code>prices</code> <code>array - like</code> <p>Array or list of prices</p> required <code>dividends</code> <code>array - like</code> <p>Array or list of dividends paid</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The total return index</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; total_return_index([100, 102, 105, 103, 106])\n[100., 102., 105., 103., 106.]\n&gt;&gt;&gt; total_return_index([100, 102, 105, 103, 106], [0, 1, 0, 2, 0])\n[100.        , 103.        , 106.02941176, 106.02941176,\n109.11764706]\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.tracking_error","title":"<code>tracking_error(portfolio_returns, benchmark_returns, annualize=False, periods_per_year=252)</code>","text":"<p>Calculate the tracking error between portfolio returns and benchmark returns.</p> <p>Parameters:</p> Name Type Description Default <code>portfolio_returns</code> <code>list or ndarray</code> <p>Array or list of portfolio returns</p> required <code>benchmark_returns</code> <code>list or ndarray</code> <p>Array or list of benchmark returns</p> required <code>annualize</code> <code>bool</code> <p>Whether to annualize the tracking error</p> <code>False</code> <code>periods_per_year</code> <code>int</code> <p>Number of periods in a year (252 for daily returns, 12 for monthly, 4 for quarterly)</p> <code>252</code> <p>Returns:</p> Type Description <code>float</code> <p>Tracking error</p> Notes <p>Tracking error measures how closely a portfolio follows its benchmark. It is the standard deviation of the difference between portfolio and benchmark returns.</p>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.treynor_ratio","title":"<code>treynor_ratio(returns, benchmark_returns, risk_free_rate=0.0, annualization_factor=1.0)</code>","text":"<p>Calculate the Treynor ratio, which measures excess return per unit of systematic risk.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array of portfolio returns</p> required <code>benchmark_returns</code> <code>array - like</code> <p>Array of benchmark returns for the same periods</p> required <code>risk_free_rate</code> <code>float or array - like</code> <p>Risk-free rate for the same period as returns</p> <code>0.0</code> <code>annualization_factor</code> <code>float</code> <p>Factor to annualize the Treynor ratio</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float</code> <p>The Treynor ratio</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; treynor_ratio([0.01, 0.02, -0.01, 0.03, 0.01], [0.005, 0.01, -0.005, 0.02, 0.005], 0.001, 252)\n0.0378\n</code></pre>"},{"location":"api/portfolio/portfolio/#pypulate.dtypes.portfolio.Portfolio.value_at_risk","title":"<code>value_at_risk(returns, confidence_level=0.95, method='historical', parametric_mean=None, parametric_std=None, current_value=1.0)</code>","text":"<p>Calculate the Value-at-Risk (VaR) of a portfolio.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>list or ndarray</code> <p>Array or list of returns</p> required <code>confidence_level</code> <code>float</code> <p>Confidence level for VaR calculation (e.g., 0.95 for 95% confidence)</p> <code>0.95</code> <code>method</code> <code>str</code> <p>Method for calculating VaR ('historical', 'parametric', or 'monte_carlo')</p> <code>'historical'</code> <code>parametric_mean</code> <code>float</code> <p>Mean for parametric VaR calculation (if None, calculated from returns)</p> <code>None</code> <code>parametric_std</code> <code>float</code> <p>Standard deviation for parametric VaR calculation (if None, calculated from returns)</p> <code>None</code> <code>current_value</code> <code>float</code> <p>Current value of the portfolio</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float</code> <p>Value-at-Risk (VaR) as a positive number representing the potential loss</p> Notes <p>VaR measures the potential loss in value of a portfolio over a defined period for a given confidence interval.</p>"},{"location":"api/portfolio/return_measurement/","title":"Return Measurement API","text":"<p>Return measurement functions for portfolio analysis.</p> <p>This module provides functions for measuring returns in various portfolio scenarios, including portfolios with no cash flows, portfolios with inflows and outflows, and market-neutral and leveraged portfolios.</p> <p>All functions support both Python lists and NumPy arrays as inputs.</p>"},{"location":"api/portfolio/return_measurement/#pypulate.portfolio.return_measurement.annualized_return","title":"<code>annualized_return(total_return, years)</code>","text":"<p>Calculate the annualized return from a total return over a period of years.</p> <p>Parameters:</p> Name Type Description Default <code>total_return</code> <code>float or array - like</code> <p>The total return over the entire period as a decimal</p> required <code>years</code> <code>float or array - like</code> <p>The number of years in the period</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>The annualized return as a decimal If array inputs are provided, returns an array of annualized returns</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; annualized_return(0.2, 2)\n0.09544511501033215\n&gt;&gt;&gt; annualized_return([0.2, 0.3, 0.15], [2, 3, 1.5])\n[0.09544512, 0.09139288, 0.0976534 ]\n&gt;&gt;&gt; annualized_return(np.array([0.4, 0.5]), 2)\n[0.18321596, 0.22474487]\n</code></pre>"},{"location":"api/portfolio/return_measurement/#pypulate.portfolio.return_measurement.arithmetic_return","title":"<code>arithmetic_return(prices)</code>","text":"<p>Calculate the arithmetic average return from a series of prices.</p> <p>Parameters:</p> Name Type Description Default <code>prices</code> <code>array - like</code> <p>Array or list of prices</p> required <p>Returns:</p> Type Description <code>float</code> <p>The arithmetic average return as a decimal</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arithmetic_return([100, 105, 103, 108, 110])\n0.024503647197821957\n</code></pre>"},{"location":"api/portfolio/return_measurement/#pypulate.portfolio.return_measurement.beta_adjusted_return","title":"<code>beta_adjusted_return(portfolio_return, benchmark_return, portfolio_beta)</code>","text":"<p>Calculate the beta-adjusted return (alpha) of a portfolio.</p> <p>Parameters:</p> Name Type Description Default <code>portfolio_return</code> <code>float or array - like</code> <p>The return of the portfolio as a decimal</p> required <code>benchmark_return</code> <code>float or array - like</code> <p>The return of the benchmark as a decimal</p> required <code>portfolio_beta</code> <code>float or array - like</code> <p>The beta of the portfolio relative to the benchmark</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>The beta-adjusted return (alpha) as a decimal If array inputs are provided, returns an array of beta-adjusted returns</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; beta_adjusted_return(0.12, 0.10, 1.2)\n0.0\n&gt;&gt;&gt; beta_adjusted_return([0.12, 0.15], [0.10, 0.08], 1.2)\n[0.   , 0.054]\n&gt;&gt;&gt; beta_adjusted_return(0.12, 0.10, [1.2, 1.5])\n[ 0.  , -0.03]\n</code></pre>"},{"location":"api/portfolio/return_measurement/#pypulate.portfolio.return_measurement.dollar_weighted_return","title":"<code>dollar_weighted_return(cash_flows, cash_flow_dates, end_value)</code>","text":"<p>Calculate the dollar-weighted return (internal rate of return) for a series of cash flows.</p> <p>Parameters:</p> Name Type Description Default <code>cash_flows</code> <code>array - like</code> <p>Array or list of cash flows (positive for inflows, negative for outflows)</p> required <code>cash_flow_dates</code> <code>array - like</code> <p>Array or list of dates (in days) when each cash flow occurs</p> required <code>end_value</code> <code>float</code> <p>The final value of the investment</p> required <p>Returns:</p> Type Description <code>float</code> <p>The dollar-weighted return as a decimal</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dollar_weighted_return([-1000, -500, 200], [0, 30, 60], 1400)\n0.36174448410245186\n</code></pre>"},{"location":"api/portfolio/return_measurement/#pypulate.portfolio.return_measurement.geometric_return","title":"<code>geometric_return(prices)</code>","text":"<p>Calculate the geometric average return from a series of prices.</p> <p>Parameters:</p> Name Type Description Default <code>prices</code> <code>array - like</code> <p>Array or list of prices</p> required <p>Returns:</p> Type Description <code>float</code> <p>The geometric average return as a decimal</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; geometric_return([100, 105, 103, 108, 110])\n0.02411368908444511\n</code></pre>"},{"location":"api/portfolio/return_measurement/#pypulate.portfolio.return_measurement.holding_period_return","title":"<code>holding_period_return(prices, dividends=None)</code>","text":"<p>Calculate the holding period return for a series of prices and optional dividends.</p> <p>Parameters:</p> Name Type Description Default <code>prices</code> <code>array - like</code> <p>Array or list of prices over the holding period</p> required <code>dividends</code> <code>array - like</code> <p>Array or list of dividends paid during the holding period</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The holding period return as a decimal</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; holding_period_return([100, 102, 105, 103, 106])\n0.06\n&gt;&gt;&gt; holding_period_return([100, 102, 105, 103, 106], [0, 1, 0, 2, 0])\n0.09\n</code></pre>"},{"location":"api/portfolio/return_measurement/#pypulate.portfolio.return_measurement.leveraged_return","title":"<code>leveraged_return(unleveraged_return, leverage_ratio, borrowing_rate)</code>","text":"<p>Calculate the return of a leveraged portfolio.</p> <p>Parameters:</p> Name Type Description Default <code>unleveraged_return</code> <code>float or array - like</code> <p>The return of the unleveraged portfolio as a decimal</p> required <code>leverage_ratio</code> <code>float or array - like</code> <p>The leverage ratio (e.g., 2.0 for 2:1 leverage)</p> required <code>borrowing_rate</code> <code>float or array - like</code> <p>The borrowing rate as a decimal</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>The leveraged return as a decimal If array inputs are provided, returns an array of leveraged returns</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; leveraged_return(0.10, 2.0, 0.05)\n0.15\n&gt;&gt;&gt; leveraged_return([0.10, 0.15], [2.0, 1.5], 0.05)\n[0.15, 0.2 ]\n&gt;&gt;&gt; leveraged_return(0.10, [2.0, 3.0], [0.05, 0.06])\n[0.15, 0.18]\n</code></pre>"},{"location":"api/portfolio/return_measurement/#pypulate.portfolio.return_measurement.linked_modified_dietz_return","title":"<code>linked_modified_dietz_return(period_returns)</code>","text":"<p>Calculate the linked Modified Dietz return over multiple periods.</p> <p>Parameters:</p> Name Type Description Default <code>period_returns</code> <code>array - like</code> <p>Array or list of Modified Dietz returns for each period</p> required <p>Returns:</p> Type Description <code>float</code> <p>The linked Modified Dietz return as a decimal</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; linked_modified_dietz_return([0.05, -0.02, 0.03, 0.04])\n0.10226479999999993\n</code></pre>"},{"location":"api/portfolio/return_measurement/#pypulate.portfolio.return_measurement.log_return","title":"<code>log_return(end_value, start_value)</code>","text":"<p>Calculate the logarithmic (continuously compounded) return between two values.</p> <p>Parameters:</p> Name Type Description Default <code>end_value</code> <code>float or array - like</code> <p>The ending value(s) of the investment</p> required <code>start_value</code> <code>float or array - like</code> <p>The starting value(s) of the investment</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>The logarithmic return If array inputs are provided, returns an array of logarithmic returns</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; log_return(105, 100)\n0.04879016416929972\n&gt;&gt;&gt; log_return([105, 110, 108], [100, 100, 100])\narray([0.04879016, 0.09531018, 0.07696104])\n&gt;&gt;&gt; log_return(np.array([105, 110]), np.array([100, 100]))\narray([0.04879016, 0.09531018])\n</code></pre>"},{"location":"api/portfolio/return_measurement/#pypulate.portfolio.return_measurement.long_short_equity_return","title":"<code>long_short_equity_return(long_portfolio_return, short_portfolio_return, long_exposure, short_exposure, risk_free_rate=0.0, short_rebate=0.0)</code>","text":"<p>Calculate the return of a long-short equity portfolio.</p> <p>Parameters:</p> Name Type Description Default <code>long_portfolio_return</code> <code>float or array - like</code> <p>The return of the long portfolio as a decimal</p> required <code>short_portfolio_return</code> <code>float or array - like</code> <p>The return of the short portfolio as a decimal</p> required <code>long_exposure</code> <code>float or array - like</code> <p>The exposure of the long portfolio as a decimal of NAV</p> required <code>short_exposure</code> <code>float or array - like</code> <p>The exposure of the short portfolio as a decimal of NAV</p> required <code>risk_free_rate</code> <code>float or array - like</code> <p>The risk-free rate as a decimal</p> <code>0.0</code> <code>short_rebate</code> <code>float or array - like</code> <p>The rebate received on short proceeds as a decimal</p> <code>0.0</code> <p>Returns:</p> Type Description <code>float or ndarray</code> <p>The return of the long-short equity portfolio as a decimal If array inputs are provided, returns an array of long-short equity returns</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; long_short_equity_return(0.10, -0.05, 1.0, 0.5, 0.02, 0.01)\n0.14\n&gt;&gt;&gt; long_short_equity_return([0.10, 0.12], [-0.05, -0.03], 1.0, 0.5, 0.02, 0.01)\narray([0.14, 0.15])\n&gt;&gt;&gt; long_short_equity_return(0.10, -0.05, [1.0, 0.8], [0.5, 0.4], [0.02, 0.03], 0.01)\narray([0.14 , 0.122])\n</code></pre>"},{"location":"api/portfolio/return_measurement/#pypulate.portfolio.return_measurement.market_neutral_return","title":"<code>market_neutral_return(long_return, short_return, long_weight=0.5, short_weight=0.5, short_borrowing_cost=0.0)</code>","text":"<p>Calculate the return of a market-neutral portfolio with long and short positions.</p> <p>Parameters:</p> Name Type Description Default <code>long_return</code> <code>float or array - like</code> <p>The return of the long portfolio as a decimal</p> required <code>short_return</code> <code>float or array - like</code> <p>The return of the short portfolio as a decimal</p> required <code>long_weight</code> <code>float or array - like</code> <p>The weight of the long portfolio</p> <code>0.5</code> <code>short_weight</code> <code>float or array - like</code> <p>The weight of the short portfolio</p> <code>0.5</code> <code>short_borrowing_cost</code> <code>float or array - like</code> <p>The cost of borrowing for the short position as a decimal</p> <code>0.0</code> <p>Returns:</p> Type Description <code>float or ndarray</code> <p>The market-neutral return as a decimal If array inputs are provided, returns an array of market-neutral returns</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; market_neutral_return(0.08, -0.05, 0.6, 0.4, 0.01)\n0.064\n&gt;&gt;&gt; market_neutral_return([0.08, 0.10], [-0.05, -0.03], 0.6, 0.4, 0.01)\n[0.064, 0.068]\n&gt;&gt;&gt; market_neutral_return(0.08, -0.05, [0.6, 0.7], [0.4, 0.3], [0.01, 0.02])\n[0.064, 0.065]\n</code></pre>"},{"location":"api/portfolio/return_measurement/#pypulate.portfolio.return_measurement.modified_dietz_return","title":"<code>modified_dietz_return(start_value, end_value, cash_flows, cash_flow_days, total_days)</code>","text":"<p>Calculate the Modified Dietz return, which approximates the money-weighted return.</p> <p>Parameters:</p> Name Type Description Default <code>start_value</code> <code>float</code> <p>The starting value of the investment</p> required <code>end_value</code> <code>float</code> <p>The ending value of the investment</p> required <code>cash_flows</code> <code>array - like</code> <p>Array or list of cash flows (positive for inflows, negative for outflows)</p> required <code>cash_flow_days</code> <code>array - like</code> <p>Array or list of days when each cash flow occurs (day 0 is the start)</p> required <code>total_days</code> <code>int</code> <p>Total number of days in the period</p> required <p>Returns:</p> Type Description <code>float</code> <p>The Modified Dietz return as a decimal</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; modified_dietz_return(1000, 1200, [100, -50], [10, 20], 30)\n0.14285714285714285\n</code></pre>"},{"location":"api/portfolio/return_measurement/#pypulate.portfolio.return_measurement.money_weighted_return","title":"<code>money_weighted_return(cash_flows, cash_flow_times, final_value, initial_value=0, max_iterations=100, tolerance=1e-06)</code>","text":"<p>Calculate the money-weighted return (internal rate of return) for a series of cash flows.</p> <p>Parameters:</p> Name Type Description Default <code>cash_flows</code> <code>array - like</code> <p>Array or list of cash flows (positive for inflows, negative for outflows)</p> required <code>cash_flow_times</code> <code>array - like</code> <p>Array or list of times (in years) when each cash flow occurs</p> required <code>final_value</code> <code>float</code> <p>The final value of the investment</p> required <code>initial_value</code> <code>float</code> <p>The initial value of the investment</p> <code>0</code> <code>max_iterations</code> <code>int</code> <p>Maximum number of iterations for the numerical solver</p> <code>100</code> <code>tolerance</code> <code>float</code> <p>Convergence tolerance for the numerical solver</p> <code>1e-6</code> <p>Returns:</p> Type Description <code>float</code> <p>The money-weighted return (IRR) as a decimal</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; money_weighted_return([-1000, -500, 1700], [0, 0.5, 1], 0)\n0.16120409753798307\n</code></pre>"},{"location":"api/portfolio/return_measurement/#pypulate.portfolio.return_measurement.simple_return","title":"<code>simple_return(end_value, start_value)</code>","text":"<p>Calculate the simple return (percentage change) between two values.</p> <p>Parameters:</p> Name Type Description Default <code>end_value</code> <code>float or array - like</code> <p>The ending value(s) of the investment</p> required <code>start_value</code> <code>float or array - like</code> <p>The starting value(s) of the investment</p> required <p>Returns:</p> Type Description <code>float or ndarray</code> <p>The simple return as a decimal (e.g., 0.05 for 5%) If array inputs are provided, returns an array of simple returns</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; simple_return(105, 100)\n0.05\n&gt;&gt;&gt; simple_return([105, 110, 108], [100, 100, 100])\narray([0.05, 0.1 , 0.08])\n&gt;&gt;&gt; simple_return(np.array([105, 110]), np.array([100, 100]))\narray([0.05, 0.1 ])\n</code></pre>"},{"location":"api/portfolio/return_measurement/#pypulate.portfolio.return_measurement.time_weighted_return","title":"<code>time_weighted_return(period_returns)</code>","text":"<p>Calculate the time-weighted return from a series of period returns.</p> <p>Parameters:</p> Name Type Description Default <code>period_returns</code> <code>array - like</code> <p>Array or list of returns for each period</p> required <p>Returns:</p> Type Description <code>float</code> <p>The time-weighted return as a decimal</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; time_weighted_return([0.05, -0.02, 0.03, 0.04])\n0.10226479999999993\n</code></pre>"},{"location":"api/portfolio/return_measurement/#pypulate.portfolio.return_measurement.total_return_index","title":"<code>total_return_index(prices, dividends=None)</code>","text":"<p>Calculate the total return index from a series of prices and optional dividends.</p> <p>Parameters:</p> Name Type Description Default <code>prices</code> <code>array - like</code> <p>Array or list of prices</p> required <code>dividends</code> <code>array - like</code> <p>Array or list of dividends paid</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The total return index</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; total_return_index([100, 102, 105, 103, 106])\n[100., 102., 105., 103., 106.]\n&gt;&gt;&gt; total_return_index([100, 102, 105, 103, 106], [0, 1, 0, 2, 0])\n[100.        , 103.        , 106.02941176, 106.02941176,\n   109.11764706]\n</code></pre>"},{"location":"api/portfolio/risk_adjusted/","title":"Risk-Adjusted Performance API","text":"<p>Risk-adjusted performance measurement functions for portfolio analysis.</p> <p>This module provides functions for measuring risk-adjusted performance metrics including Sharpe ratio, Information ratio, CAPM alpha, and multifactor models.</p> <p>All functions support both Python lists and NumPy arrays as inputs.</p>"},{"location":"api/portfolio/risk_adjusted/#pypulate.portfolio.risk_adjusted.benchmark_alpha","title":"<code>benchmark_alpha(returns, benchmark_returns)</code>","text":"<p>Calculate the benchmark alpha, which is the difference between portfolio return and benchmark return.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array of portfolio returns</p> required <code>benchmark_returns</code> <code>array - like</code> <p>Array of benchmark returns for the same periods</p> required <p>Returns:</p> Type Description <code>float</code> <p>The benchmark alpha (difference in mean returns)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; benchmark_alpha([0.01, 0.02, -0.01, 0.03, 0.01], [0.005, 0.01, -0.005, 0.02, 0.005])\n0.005\n</code></pre>"},{"location":"api/portfolio/risk_adjusted/#pypulate.portfolio.risk_adjusted.calmar_ratio","title":"<code>calmar_ratio(returns, max_drawdown=None, annualization_factor=1.0)</code>","text":"<p>Calculate the Calmar ratio, which measures return relative to maximum drawdown.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array of portfolio returns</p> required <code>max_drawdown</code> <code>float</code> <p>Maximum drawdown as a positive decimal. If None, it will be calculated from returns.</p> <code>None</code> <code>annualization_factor</code> <code>float</code> <p>Factor to annualize returns</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float</code> <p>The Calmar ratio</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; calmar_ratio([0.01, 0.02, -0.01, 0.03, 0.01], 0.15, 252)\n0.8\n</code></pre>"},{"location":"api/portfolio/risk_adjusted/#pypulate.portfolio.risk_adjusted.capm_alpha","title":"<code>capm_alpha(returns, benchmark_returns, risk_free_rate=0.0)</code>","text":"<p>Calculate the CAPM alpha (Jensen's alpha) and related statistics.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array of portfolio returns</p> required <code>benchmark_returns</code> <code>array - like</code> <p>Array of benchmark returns for the same periods</p> required <code>risk_free_rate</code> <code>float or array - like</code> <p>Risk-free rate for the same period as returns</p> <code>0.0</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(alpha, beta, r_squared, p_value, std_err) - alpha: The CAPM alpha (intercept) - beta: The CAPM beta (slope) - r_squared: The R-squared of the regression - p_value: The p-value for alpha - std_err: The standard error of alpha</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; capm_alpha([0.01, 0.02, -0.01, 0.03, 0.01], [0.005, 0.01, -0.005, 0.02, 0.005], 0.001)\n(0.0046, 1.2, 0.9, 0.0023, 0.0012)\n</code></pre>"},{"location":"api/portfolio/risk_adjusted/#pypulate.portfolio.risk_adjusted.information_ratio","title":"<code>information_ratio(returns, benchmark_returns, annualization_factor=1.0)</code>","text":"<p>Calculate the Information ratio, which measures active return per unit of active risk.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array of portfolio returns</p> required <code>benchmark_returns</code> <code>array - like</code> <p>Array of benchmark returns for the same periods</p> required <code>annualization_factor</code> <code>float</code> <p>Factor to annualize the Information ratio (e.g., 252 for daily returns to annual)</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float</code> <p>The Information ratio</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; information_ratio([0.01, 0.02, -0.01, 0.03, 0.01], [0.005, 0.01, -0.005, 0.02, 0.005], 252)\n2.8284271247461903\n</code></pre>"},{"location":"api/portfolio/risk_adjusted/#pypulate.portfolio.risk_adjusted.multifactor_alpha","title":"<code>multifactor_alpha(returns, factor_returns, risk_free_rate=0.0)</code>","text":"<p>Calculate the alpha from a multifactor model (e.g., Fama-French).</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array of portfolio returns</p> required <code>factor_returns</code> <code>array - like</code> <p>2D array where each column represents returns for a factor</p> required <code>risk_free_rate</code> <code>float or array - like</code> <p>Risk-free rate for the same period as returns</p> <code>0.0</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(alpha, betas, r_squared, p_value, std_err) - alpha: The multifactor alpha (intercept) - betas: Array of factor betas (coefficients) - r_squared: The R-squared of the regression - p_value: The p-value for alpha - std_err: The standard error of alpha</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Example with market, size, and value factors\n&gt;&gt;&gt; portfolio_returns = [0.01, 0.02, -0.01, 0.03, 0.01]\n&gt;&gt;&gt; factor_returns = [\n...     [0.005, 0.01, -0.005, 0.02, 0.005],  # Market\n...     [0.002, 0.003, -0.001, 0.004, 0.001],  # Size\n...     [0.001, 0.002, -0.002, 0.003, 0.002]   # Value\n... ]\n&gt;&gt;&gt; multifactor_alpha(portfolio_returns, factor_returns, 0.001)\n(0.0032, array([0.9, 0.5, 0.3]), 0.92, 0.04, 0.0015)  # Example values\n</code></pre>"},{"location":"api/portfolio/risk_adjusted/#pypulate.portfolio.risk_adjusted.omega_ratio","title":"<code>omega_ratio(returns, threshold=0.0, annualization_factor=1.0)</code>","text":"<p>Calculate the Omega ratio, which measures the probability-weighted ratio of gains versus losses.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array of portfolio returns</p> required <code>threshold</code> <code>float</code> <p>The threshold return</p> <code>0.0</code> <code>annualization_factor</code> <code>float</code> <p>Factor to annualize the threshold</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float</code> <p>The Omega ratio</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; omega_ratio([0.01, 0.02, -0.01, 0.03, 0.01], 0.005)\n2.0\n</code></pre>"},{"location":"api/portfolio/risk_adjusted/#pypulate.portfolio.risk_adjusted.sharpe_ratio","title":"<code>sharpe_ratio(returns, risk_free_rate=0.0, annualization_factor=1.0)</code>","text":"<p>Calculate the Sharpe ratio, which measures excess return per unit of risk.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array of periodic returns</p> required <code>risk_free_rate</code> <code>float or array - like</code> <p>Risk-free rate for the same period as returns</p> <code>0.0</code> <code>annualization_factor</code> <code>float</code> <p>Factor to annualize the Sharpe ratio (e.g., 252 for daily returns to annual)</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float or ndarray</code> <p>The Sharpe ratio If array input is provided for risk_free_rate, returns an array of Sharpe ratios</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sharpe_ratio([0.01, 0.02, -0.01, 0.03, 0.01], 0.001, 252)\n2.5298221281347035\n&gt;&gt;&gt; sharpe_ratio([0.01, 0.02, -0.01, 0.03, 0.01], [0.001, 0.002], 252)\narray([2.52982213, 2.26684001])\n</code></pre>"},{"location":"api/portfolio/risk_adjusted/#pypulate.portfolio.risk_adjusted.sortino_ratio","title":"<code>sortino_ratio(returns, risk_free_rate=0.0, target_return=0.0, annualization_factor=1.0)</code>","text":"<p>Calculate the Sortino ratio, which measures excess return per unit of downside risk.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array of portfolio returns</p> required <code>risk_free_rate</code> <code>float or array - like</code> <p>Risk-free rate for the same period as returns</p> <code>0.0</code> <code>target_return</code> <code>float</code> <p>Minimum acceptable return</p> <code>0.0</code> <code>annualization_factor</code> <code>float</code> <p>Factor to annualize the Sortino ratio</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float</code> <p>The Sortino ratio</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sortino_ratio([0.01, 0.02, -0.01, 0.03, 0.01], 0.001, 0.0, 252)\n3.7947331922020545\n</code></pre>"},{"location":"api/portfolio/risk_adjusted/#pypulate.portfolio.risk_adjusted.treynor_ratio","title":"<code>treynor_ratio(returns, benchmark_returns, risk_free_rate=0.0, annualization_factor=1.0)</code>","text":"<p>Calculate the Treynor ratio, which measures excess return per unit of systematic risk.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array of portfolio returns</p> required <code>benchmark_returns</code> <code>array - like</code> <p>Array of benchmark returns for the same periods</p> required <code>risk_free_rate</code> <code>float or array - like</code> <p>Risk-free rate for the same period as returns</p> <code>0.0</code> <code>annualization_factor</code> <code>float</code> <p>Factor to annualize the Treynor ratio</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float</code> <p>The Treynor ratio</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; treynor_ratio([0.01, 0.02, -0.01, 0.03, 0.01], [0.005, 0.01, -0.005, 0.02, 0.005], 0.001, 252)\n0.0378\n</code></pre>"},{"location":"api/portfolio/risk_measurement/","title":"Risk Measurement API","text":"<p>Risk measurement functions for portfolio analysis.</p> <p>This module provides various risk metrics used in portfolio management and financial analysis.</p>"},{"location":"api/portfolio/risk_measurement/#pypulate.portfolio.risk_measurement.capm_beta","title":"<code>capm_beta(portfolio_returns, market_returns)</code>","text":"<p>Calculate the CAPM beta of a portfolio.</p> <p>Parameters:</p> Name Type Description Default <code>portfolio_returns</code> <code>array - like</code> <p>Array or list of portfolio returns</p> required <code>market_returns</code> <code>array - like</code> <p>Array or list of market returns</p> required <p>Returns:</p> Type Description <code>float</code> <p>CAPM beta</p> Notes <p>Beta measures the sensitivity of portfolio returns to market returns. It is the covariance of portfolio returns and market returns divided by the variance of market returns.</p>"},{"location":"api/portfolio/risk_measurement/#pypulate.portfolio.risk_measurement.conditional_value_at_risk","title":"<code>conditional_value_at_risk(returns, confidence_level=0.95, method='historical', current_value=1.0)</code>","text":"<p>Calculate the Conditional Value-at-Risk (CVaR) of a portfolio.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array or list of returns</p> required <code>confidence_level</code> <code>float</code> <p>Confidence level for CVaR calculation (e.g., 0.95 for 95% confidence)</p> <code>0.95</code> <code>method</code> <code>str</code> <p>Method for calculating CVaR ('historical' or 'parametric')</p> <code>'historical'</code> <code>current_value</code> <code>float</code> <p>Current value of the portfolio</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float</code> <p>Conditional Value-at-Risk (CVaR) as a positive number representing the potential loss</p> Notes <p>CVaR, also known as Expected Shortfall, measures the expected loss given that the loss exceeds the VaR threshold. It provides a more conservative risk measure than VaR.</p>"},{"location":"api/portfolio/risk_measurement/#pypulate.portfolio.risk_measurement.correlation_matrix","title":"<code>correlation_matrix(returns_matrix)</code>","text":"<p>Calculate the correlation matrix of returns.</p> <p>Parameters:</p> Name Type Description Default <code>returns_matrix</code> <code>array - like</code> <p>Matrix of returns where each column represents an asset</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Correlation matrix</p> Notes <p>The correlation matrix measures the strength of the relationship between returns of different assets, normalized to be between -1 and 1.</p>"},{"location":"api/portfolio/risk_measurement/#pypulate.portfolio.risk_measurement.covariance_matrix","title":"<code>covariance_matrix(returns_matrix)</code>","text":"<p>Calculate the covariance matrix of returns.</p> <p>Parameters:</p> Name Type Description Default <code>returns_matrix</code> <code>array - like</code> <p>Matrix of returns where each column represents an asset</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Covariance matrix</p> Notes <p>The covariance matrix measures how returns of different assets move together.</p>"},{"location":"api/portfolio/risk_measurement/#pypulate.portfolio.risk_measurement.drawdown","title":"<code>drawdown(returns, as_list=False)</code>","text":"<p>Calculate the drawdown, maximum drawdown, and drawdown duration of returns.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array or list of returns</p> required <code>as_list</code> <code>bool</code> <p>If True, returns the drawdowns as a list instead of numpy array</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple containing:</code> <ul> <li>Array or list of drawdowns</li> <li>Maximum drawdown (as a positive number)</li> <li>Start index of maximum drawdown</li> <li>End index of maximum drawdown</li> </ul> Notes <p>Drawdown measures the decline from a historical peak in cumulative returns.</p>"},{"location":"api/portfolio/risk_measurement/#pypulate.portfolio.risk_measurement.semi_standard_deviation","title":"<code>semi_standard_deviation(returns, threshold=0.0, annualize=False, periods_per_year=252)</code>","text":"<p>Calculate the semi-standard deviation of returns below a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array or list of returns</p> required <code>threshold</code> <code>float</code> <p>Threshold below which to calculate semi-standard deviation</p> <code>0.0</code> <code>annualize</code> <code>bool</code> <p>Whether to annualize the semi-standard deviation</p> <code>False</code> <code>periods_per_year</code> <code>int</code> <p>Number of periods in a year (252 for daily returns, 12 for monthly, 4 for quarterly)</p> <code>252</code> <p>Returns:</p> Type Description <code>float</code> <p>Semi-standard deviation of returns</p> Notes <p>Semi-standard deviation only considers returns below the threshold (typically 0), making it a measure of downside risk.</p>"},{"location":"api/portfolio/risk_measurement/#pypulate.portfolio.risk_measurement.standard_deviation","title":"<code>standard_deviation(returns, annualize=False, periods_per_year=252)</code>","text":"<p>Calculate the standard deviation of returns.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array or list of returns</p> required <code>annualize</code> <code>bool</code> <p>Whether to annualize the standard deviation</p> <code>False</code> <code>periods_per_year</code> <code>int</code> <p>Number of periods in a year (252 for daily returns, 12 for monthly, 4 for quarterly)</p> <code>252</code> <p>Returns:</p> Type Description <code>float</code> <p>Standard deviation of returns</p> Notes <p>Standard deviation measures the dispersion of returns around the mean. It is the square root of the variance.</p>"},{"location":"api/portfolio/risk_measurement/#pypulate.portfolio.risk_measurement.tracking_error","title":"<code>tracking_error(portfolio_returns, benchmark_returns, annualize=False, periods_per_year=252)</code>","text":"<p>Calculate the tracking error between portfolio returns and benchmark returns.</p> <p>Parameters:</p> Name Type Description Default <code>portfolio_returns</code> <code>array - like</code> <p>Array or list of portfolio returns</p> required <code>benchmark_returns</code> <code>array - like</code> <p>Array or list of benchmark returns</p> required <code>annualize</code> <code>bool</code> <p>Whether to annualize the tracking error</p> <code>False</code> <code>periods_per_year</code> <code>int</code> <p>Number of periods in a year (252 for daily returns, 12 for monthly, 4 for quarterly)</p> <code>252</code> <p>Returns:</p> Type Description <code>float</code> <p>Tracking error</p> Notes <p>Tracking error measures how closely a portfolio follows its benchmark. It is the standard deviation of the difference between portfolio and benchmark returns.</p>"},{"location":"api/portfolio/risk_measurement/#pypulate.portfolio.risk_measurement.value_at_risk","title":"<code>value_at_risk(returns, confidence_level=0.95, method='historical', parametric_mean=None, parametric_std=None, current_value=1.0)</code>","text":"<p>Calculate the Value-at-Risk (VaR) of a portfolio.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>array - like</code> <p>Array or list of returns</p> required <code>confidence_level</code> <code>float</code> <p>Confidence level for VaR calculation (e.g., 0.95 for 95% confidence)</p> <code>0.95</code> <code>method</code> <code>str</code> <p>Method for calculating VaR ('historical', 'parametric', or 'monte_carlo')</p> <code>'historical'</code> <code>parametric_mean</code> <code>float</code> <p>Mean for parametric VaR calculation (if None, calculated from returns)</p> <code>None</code> <code>parametric_std</code> <code>float</code> <p>Standard deviation for parametric VaR calculation (if None, calculated from returns)</p> <code>None</code> <code>current_value</code> <code>float</code> <p>Current value of the portfolio</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float</code> <p>Value-at-Risk (VaR) as a positive number representing the potential loss</p> Notes <p>VaR measures the potential loss in value of a portfolio over a defined period for a given confidence interval.</p>"},{"location":"api/service-pricing/bundle-pricing/","title":"Bundle Pricing API Reference","text":"<p>This page documents the API for the bundle pricing module in Pypulate.</p>"},{"location":"api/service-pricing/bundle-pricing/#pypulate.pricing.bundle_pricing.calculate_bundle_price","title":"<code>calculate_bundle_price(items, item_prices, bundle_discounts, minimum_bundle_size=2)</code>","text":"<p>Calculate price for bundled items with discounts.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list</code> <p>List of items in the bundle</p> required <code>item_prices</code> <code>dict</code> <p>Individual prices for each item</p> required <code>bundle_discounts</code> <code>dict</code> <p>Discount rates for different bundle combinations</p> required <code>minimum_bundle_size</code> <code>int</code> <p>Minimum items required for bundle pricing</p> <code>2</code> <p>Returns:</p> Type Description <code>float</code> <p>Total price for the bundle</p>"},{"location":"api/service-pricing/dynamic-pricing/","title":"Dynamic Pricing API Reference","text":"<p>This page documents the API for the business KPIs module in Pypulate.</p> <p>Dynamic Pricing Module</p> <p>This module provides functions for calculating dynamic pricing adjustments.</p>"},{"location":"api/service-pricing/dynamic-pricing/#pypulate.pricing.dynamic_pricing.PricingRule","title":"<code>PricingRule</code>","text":"<p>A class for managing custom pricing rules.</p> <p>This class provides methods for: - Adding custom pricing rules - Applying custom pricing rules - Managing rule metadata</p>"},{"location":"api/service-pricing/dynamic-pricing/#pypulate.pricing.dynamic_pricing.PricingRule.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the PricingRule class.</p>"},{"location":"api/service-pricing/dynamic-pricing/#pypulate.pricing.dynamic_pricing.PricingRule.add_rule","title":"<code>add_rule(rule_name, calculation_function, description='')</code>","text":"<p>Add a custom pricing rule.</p> <p>Parameters:</p> Name Type Description Default <code>rule_name</code> <code>str</code> <p>Name of the custom pricing rule</p> required <code>calculation_function</code> <code>callable</code> <p>Function that implements the custom pricing logic</p> required <code>description</code> <code>str</code> <p>Description of the pricing rule</p> <code>''</code>"},{"location":"api/service-pricing/dynamic-pricing/#pypulate.pricing.dynamic_pricing.PricingRule.apply_rule","title":"<code>apply_rule(rule_name, *args, **kwargs)</code>","text":"<p>Apply a custom pricing rule.</p> <p>Parameters:</p> Name Type Description Default <code>rule_name</code> <code>str</code> <p>Name of the custom pricing rule</p> required <code>*args</code> <code>Any</code> <p>Arguments to pass to the custom pricing function</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arguments to pass to the custom pricing function</p> <code>()</code> <p>Returns:</p> Type Description <code>float</code> <p>Price calculated using the custom rule</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the specified rule_name doesn't exist</p>"},{"location":"api/service-pricing/dynamic-pricing/#pypulate.pricing.dynamic_pricing.PricingRule.get_rule_description","title":"<code>get_rule_description(rule_name)</code>","text":"<p>Get the description of a pricing rule.</p> <p>Parameters:</p> Name Type Description Default <code>rule_name</code> <code>str</code> <p>Name of the pricing rule</p> required <p>Returns:</p> Type Description <code>str</code> <p>Description of the pricing rule</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the specified rule_name doesn't exist</p>"},{"location":"api/service-pricing/dynamic-pricing/#pypulate.pricing.dynamic_pricing.PricingRule.list_rules","title":"<code>list_rules()</code>","text":"<p>List all available pricing rules and their descriptions.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of rule names and their descriptions</p>"},{"location":"api/service-pricing/dynamic-pricing/#pypulate.pricing.dynamic_pricing.apply_dynamic_pricing","title":"<code>apply_dynamic_pricing(base_price, demand_factor, competition_factor, seasonality_factor=1.0, min_price=None, max_price=None)</code>","text":"<p>Calculate dynamically adjusted price based on market factors.</p> <p>Parameters:</p> Name Type Description Default <code>base_price</code> <code>float</code> <p>Base price before adjustments</p> required <code>demand_factor</code> <code>float</code> <p>Demand multiplier (1.0 is neutral)</p> required <code>competition_factor</code> <code>float</code> <p>Competition multiplier (1.0 is neutral)</p> required <code>seasonality_factor</code> <code>float</code> <p>Seasonal adjustment factor</p> <code>1.0</code> <code>min_price</code> <code>float</code> <p>Minimum price floor</p> <code>None</code> <code>max_price</code> <code>float</code> <p>Maximum price ceiling</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Dynamically adjusted price</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; apply_dynamic_pricing(100.0, 1.2, 0.9, 1.1)\n118.8  # 100.0 * 1.2 * 0.9 * 1.1\n&gt;&gt;&gt; apply_dynamic_pricing(100.0, 1.5, 0.8, min_price=90.0, max_price=150.0)\n120.0  # 100.0 * 1.5 * 0.8, bounded by min/max\n</code></pre>"},{"location":"api/service-pricing/freemium-pricing/","title":"Freemium Pricing API Reference","text":"<p>This page documents the API for the freemium pricing module in Pypulate.</p>"},{"location":"api/service-pricing/freemium-pricing/#pypulate.pricing.freemium_pricing.calculate_freemium_price","title":"<code>calculate_freemium_price(base_features, premium_features, feature_usage, free_limits, overage_rates)</code>","text":"<p>Calculate price for freemium model with usage limits.</p> <p>Parameters:</p> Name Type Description Default <code>base_features</code> <code>list</code> <p>List of free features</p> required <code>premium_features</code> <code>list</code> <p>List of premium features</p> required <code>feature_usage</code> <code>dict</code> <p>Usage metrics for each feature</p> required <code>free_limits</code> <code>dict</code> <p>Usage limits for free tier</p> required <code>overage_rates</code> <code>dict</code> <p>Rates for usage beyond free limits</p> required <p>Returns:</p> Type Description <code>float</code> <p>Calculated price</p>"},{"location":"api/service-pricing/loyalty-based-pricing/","title":"Loyalty-Based Pricing API Reference","text":"<p>This page documents the API for the loyalty-based pricing module in Pypulate.</p>"},{"location":"api/service-pricing/loyalty-based-pricing/#pypulate.pricing.loyalty_based_pricing.calculate_loyalty_price","title":"<code>calculate_loyalty_price(base_price, customer_tenure, loyalty_tiers, additional_benefits={})</code>","text":"<p>Calculate price with loyalty discounts and benefits.</p> <p>Parameters:</p> Name Type Description Default <code>base_price</code> <code>float</code> <p>Base price before loyalty benefits</p> required <code>customer_tenure</code> <code>int</code> <p>Customer's tenure in months</p> required <code>loyalty_tiers</code> <code>dict</code> <p>Discount rates for different tenure levels</p> required <code>additional_benefits</code> <code>dict</code> <p>Additional benefits for loyal customers</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing: - loyalty_price: final price after discount - loyalty_tier: the applicable tier - loyalty_discount: discount amount - additional_benefits: benefits dictionary</p>"},{"location":"api/service-pricing/peak-pricing/","title":"Peak Pricing API Reference","text":"<p>This page documents the API for the peak pricing module in Pypulate.</p>"},{"location":"api/service-pricing/peak-pricing/#pypulate.pricing.peak_pricing.calculate_peak_pricing","title":"<code>calculate_peak_pricing(base_price, usage_time, peak_hours, peak_multiplier=1.5, off_peak_multiplier=0.8)</code>","text":"<p>Calculate price based on peak/off-peak hours.</p> <p>Parameters:</p> Name Type Description Default <code>base_price</code> <code>float</code> <p>Base price per unit</p> required <code>usage_time</code> <code>str</code> <p>Time of usage (format: \"HH:MM\")</p> required <code>peak_hours</code> <code>dict</code> <p>Dictionary of weekdays and their peak hours Format: {\"monday\": (\"09:00\", \"17:00\")}</p> required <code>peak_multiplier</code> <code>float</code> <p>Price multiplier during peak hours</p> <code>1.5</code> <code>off_peak_multiplier</code> <code>float</code> <p>Price multiplier during off-peak hours</p> <code>0.8</code> <p>Returns:</p> Type Description <code>float</code> <p>Calculated price</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; calculate_peak_pricing(100, \"10:00\", {\"monday\": (\"09:00\", \"17:00\")})\n150.0  # $100 * 1.5\n</code></pre>"},{"location":"api/service-pricing/subscription-pricing/","title":"Subscription Based Pricing API Reference","text":"<p>This page documents the API for the business KPIs module in Pypulate.</p> <p>Subscription Pricing Module</p> <p>This module provides functions for calculating subscription-based pricing.</p>"},{"location":"api/service-pricing/subscription-pricing/#pypulate.pricing.subscription_pricing.calculate_subscription_price","title":"<code>calculate_subscription_price(base_price, features, feature_prices, duration_months=1, discount_rate=0.0)</code>","text":"<p>Calculate subscription price including selected features.</p> <p>Parameters:</p> Name Type Description Default <code>base_price</code> <code>float</code> <p>Base subscription price</p> required <code>features</code> <code>list</code> <p>List of selected feature names</p> required <code>feature_prices</code> <code>dict</code> <p>Dictionary of feature names and their prices</p> required <code>duration_months</code> <code>int</code> <p>Subscription duration in months</p> <code>1</code> <code>discount_rate</code> <code>float</code> <p>Annual discount rate for longer subscriptions</p> <code>0.0</code> <p>Returns:</p> Type Description <code>float</code> <p>Total subscription price</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; features = ['premium', 'api_access']\n&gt;&gt;&gt; feature_prices = {'premium': 49.99, 'api_access': 29.99}\n&gt;&gt;&gt; calculate_subscription_price(99.99, features, feature_prices)\n179.97  # 99.99 + 49.99 + 29.99\n&gt;&gt;&gt; calculate_subscription_price(99.99, features, feature_prices, \n...                            duration_months=12, discount_rate=0.10)\n1943.68  # (99.99 + 49.99 + 29.99) * 12 * (1 - 0.10)\n</code></pre>"},{"location":"api/service-pricing/tiered-pricing/","title":"Tiered Pricing API Reference","text":"<p>This page documents the API for the business KPIs module in Pypulate.</p> <p>Tiered Pricing Module</p> <p>This module provides functions for calculating tiered pricing structures.</p>"},{"location":"api/service-pricing/tiered-pricing/#pypulate.pricing.tiered_pricing.calculate_tiered_price","title":"<code>calculate_tiered_price(usage_units, tiers, cumulative=True)</code>","text":"<p>Calculate price based on tiered pricing structure.</p> <p>Parameters:</p> Name Type Description Default <code>usage_units</code> <code>float</code> <p>The number of units consumed</p> required <code>tiers</code> <code>dict</code> <p>Dictionary of tier ranges and their prices Format: {\"0-1000\": 0.10, \"1001-2000\": 0.08, \"2001+\": 0.05}</p> required <code>cumulative</code> <code>bool</code> <p>If True, price is calculated cumulatively across tiers If False, entire usage is priced at the tier it falls into</p> <code>True</code> <p>Returns:</p> Type Description <code>float</code> <p>Total price based on tiered pricing</p>"},{"location":"api/service-pricing/time-based-pricing/","title":"Time-Based Pricing API Reference","text":"<p>This page documents the API for the time-based pricing module in Pypulate.</p>"},{"location":"api/service-pricing/time-based-pricing/#pypulate.pricing.time_based_pricing.calculate_time_based_price","title":"<code>calculate_time_based_price(base_price, duration, time_unit='hour', minimum_duration=1.0, rounding_method='up')</code>","text":"<p>Calculate price based on time duration.</p> <p>Parameters:</p> Name Type Description Default <code>base_price</code> <code>float</code> <p>Base price per time unit</p> required <code>duration</code> <code>float</code> <p>Duration of usage</p> required <code>time_unit</code> <code>str</code> <p>Unit of time ('minute', 'hour', 'day')</p> <code>'hour'</code> <code>minimum_duration</code> <code>float</code> <p>Minimum billable duration</p> <code>1.0</code> <code>rounding_method</code> <code>str</code> <p>How to round partial units ('up', 'down', 'nearest')</p> <code>'up'</code> <p>Returns:</p> Type Description <code>float</code> <p>Calculated price</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; calculate_time_based_price(100, 2.5, 'hour')\n250.0  # 2.5 hours at $100/hour\n</code></pre>"},{"location":"api/service-pricing/usage-based-pricing/","title":"Usage Based Pricing API Reference","text":"<p>This page documents the API for the business KPIs module in Pypulate.</p> <p>Usage Pricing Module</p> <p>This module provides functions for calculating usage-based pricing.</p>"},{"location":"api/service-pricing/usage-based-pricing/#pypulate.pricing.usage_pricing.calculate_usage_price","title":"<code>calculate_usage_price(usage_metrics, metric_rates, minimum_charge=0.0, maximum_charge=None)</code>","text":"<p>Calculate price based on usage metrics.</p> <p>Parameters:</p> Name Type Description Default <code>usage_metrics</code> <code>dict</code> <p>Dictionary of metric names and their usage values</p> required <code>metric_rates</code> <code>dict</code> <p>Dictionary of metric names and their per-unit rates</p> required <code>minimum_charge</code> <code>float</code> <p>Minimum charge to apply</p> <code>0.0</code> <code>maximum_charge</code> <code>float</code> <p>Maximum charge cap</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Total usage-based price</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; metrics = {'api_calls': 1000, 'storage_gb': 50}\n&gt;&gt;&gt; rates = {'api_calls': 0.001, 'storage_gb': 0.10}\n&gt;&gt;&gt; calculate_usage_price(metrics, rates)\n6.0  # (1000 * 0.001) + (50 * 0.10)\n&gt;&gt;&gt; calculate_usage_price(metrics, rates, minimum_charge=10.0)\n10.0  # Max of calculated price and minimum charge\n</code></pre>"},{"location":"api/service-pricing/usage-based-pricing/#pypulate.pricing.usage_pricing.calculate_volume_discount","title":"<code>calculate_volume_discount(base_price, volume, discount_tiers)</code>","text":"<p>Calculate price with volume-based discounts.</p> <p>Parameters:</p> Name Type Description Default <code>base_price</code> <code>float</code> <p>Base price per unit</p> required <code>volume</code> <code>int</code> <p>Number of units</p> required <code>discount_tiers</code> <code>dict</code> <p>Dictionary of volume thresholds and discount rates Format: {100: 0.05, 500: 0.10, 1000: 0.15}</p> required <p>Returns:</p> Type Description <code>float</code> <p>Total price after volume discount</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tiers = {100: 0.05, 500: 0.10, 1000: 0.15}\n&gt;&gt;&gt; calculate_volume_discount(10.0, 750, tiers)\n6750.0  # 750 * 10.0 * (1 - 0.10)\n</code></pre>"},{"location":"user-guide/allocation/","title":"Portfolio Allocation Examples","text":"<p>This document demonstrates various portfolio optimization methods available in the <code>pypulate</code> library using the <code>Allocation</code> class.</p>"},{"location":"user-guide/allocation/#overview","title":"Overview","text":"<p>The <code>Allocation</code> class provides several portfolio optimization methods:</p> <ol> <li>Mean-Variance Optimization</li> <li>Minimum Variance Portfolio</li> <li>Maximum Sharpe Ratio Portfolio</li> <li>Risk Parity Portfolio</li> <li>Maximum Diversification Portfolio</li> <li>Equal Weight Portfolio</li> <li>Market Cap Weight Portfolio</li> <li>Kelly Criterion Portfolio</li> <li>Black-Litterman Portfolio</li> <li>Hierarchical Risk Parity Portfolio</li> </ol>"},{"location":"user-guide/allocation/#basic-usage","title":"Basic Usage","text":"<p>First, let's import the necessary components:</p> <pre><code>from pypulate import Allocation, Parray\n</code></pre>"},{"location":"user-guide/allocation/#basic-portfolio-optimization","title":"Basic Portfolio Optimization","text":"<p>Let's start with a simple example using 3 assets:</p> <pre><code># Sample returns data (3 assets, 252 days of daily returns)\nreturns = Parray([\n    # Asset 1 (e.g., AAPL)\n    [0.02, -0.01, 0.015, 0.03, -0.005, 0.01, 0.02, -0.015, 0.025, 0.01] + \n    [0.015, -0.02, 0.01, 0.02, -0.01, 0.015, 0.025, -0.01, 0.02, 0.015] * 24 + \n    [0.015, -0.02, 0.01, 0.02, -0.01, 0.015, 0.025, -0.01, 0.02, 0.015],\n\n    # Asset 2 (e.g., MSFT)\n    [0.015, 0.02, -0.01, 0.025, 0.01, -0.015, 0.02, 0.01, -0.02, 0.015] + \n    [0.02, -0.015, 0.015, 0.025, -0.01, 0.02, 0.015, -0.015, 0.02, 0.01] * 24 + \n    [0.02, -0.015, 0.015, 0.025, -0.01, 0.02, 0.015, -0.015, 0.02, 0.01],\n\n    # Asset 3 (e.g., GOOGL)\n    [0.025, -0.02, 0.02, 0.015, -0.015, 0.02, 0.025, -0.02, 0.03, 0.02] + \n    [0.025, -0.02, 0.02, 0.015, -0.015, 0.02, 0.025, -0.02, 0.03, 0.02] * 24 + \n    [0.025, -0.02, 0.02, 0.015, -0.015, 0.02, 0.025, -0.02, 0.03, 0.02]\n]).T\n\n# Initialize the Allocation class\nallocation = Allocation()\n\n# Set risk-free rate (e.g., current 10-year Treasury yield)\nrisk_free_rate = 0.04  # 4% annual rate\n\n# Perform Mean-Variance Optimization\nweights, ret, risk = allocation.mean_variance(\n    returns=returns,\n    target_return=None,  # Maximize Sharpe ratio\n    risk_free_rate=risk_free_rate\n)\n\nprint(f\"Optimal Portfolio Weights: {weights}\")\nprint(f\"Expected Return: {ret:.4f}\")\nprint(f\"Portfolio Risk: {risk:.4f}\")\n</code></pre>"},{"location":"user-guide/allocation/#risk-parity-portfolio","title":"Risk Parity Portfolio","text":"<p>Risk Parity aims to equalize the risk contribution of each asset:</p> <pre><code># Calculate Risk Parity weights\nweights, ret, risk = allocation.risk_parity(returns=returns)\n\nprint(f\"Risk Parity Weights: {weights}\")\nprint(f\"Expected Return: {ret:.4f}\")\nprint(f\"Portfolio Risk: {risk:.4f}\")\n</code></pre>"},{"location":"user-guide/allocation/#kelly-criterion-with-conservative-sizing","title":"Kelly Criterion with Conservative Sizing","text":"<p>The Kelly Criterion can be aggressive, so we often use a fraction of the optimal weights:</p> <pre><code># Calculate Kelly Criterion weights\nweights, ret, risk = allocation.kelly_criterion(\n    returns=returns,\n    risk_free_rate=risk_free_rate\n)\n\n# Use half-Kelly for more conservative position sizing\nhalf_kelly_weights = weights * 0.5\n\nprint(f\"Full Kelly Weights: {weights}\")\nprint(f\"Half-Kelly Weights: {half_kelly_weights}\")\n</code></pre>"},{"location":"user-guide/allocation/#black-litterman-portfolio-with-views","title":"Black-Litterman Portfolio with Views","text":"<p>Black-Litterman allows incorporating market views into the optimization:</p> <pre><code># Market capitalizations\nmarket_caps = Parray([2.5e12, 2.8e12, 1.8e12])  # in USD\n\n# Define views (e.g., AAPL to outperform by 2%, GOOGL to underperform by 1%)\nviews = {0: 0.02, 2: -0.01}  # Asset indices and expected excess returns\nview_confidences = {0: 0.8, 2: 0.7}  # Confidence in views (0-1)\n\n# Calculate Black-Litterman weights\nweights, ret, risk = allocation.black_litterman(\n    returns=returns,\n    market_caps=market_caps,\n    views=views,\n    view_confidences=view_confidences,\n    tau=0.05,  # Uncertainty in the prior distribution\n    risk_free_rate=risk_free_rate\n)\n\nprint(f\"Black-Litterman Weights: {weights}\")\n</code></pre>"},{"location":"user-guide/allocation/#hierarchical-risk-parity","title":"Hierarchical Risk Parity","text":"<p>HRP uses hierarchical clustering to build a more robust portfolio:</p> <pre><code># Calculate HRP weights\nweights, ret, risk = allocation.hierarchical_risk_parity(\n    returns=returns,\n    linkage_method='ward',  # Using Ward linkage for clustering\n    distance_metric='correlation'  # Using correlation-based distance\n)\n\nprint(f\"HRP Weights: {weights}\")\n</code></pre>"},{"location":"user-guide/allocation/#comparing-different-methods","title":"Comparing Different Methods","text":"<p>Here's how to compare the performance of different optimization methods:</p> <pre><code># Define methods to compare\nmethods = [\n    (\"Mean-Variance\", allocation.mean_variance(returns, risk_free_rate=risk_free_rate)),\n    (\"Minimum Variance\", allocation.minimum_variance(returns)),\n    (\"Maximum Sharpe\", allocation.maximum_sharpe(returns, risk_free_rate=risk_free_rate)),\n    (\"Risk Parity\", allocation.risk_parity(returns)),\n    (\"Kelly Criterion\", allocation.kelly_criterion(returns, risk_free_rate=risk_free_rate))\n]\n\n# Compare results\nprint(\"\\nMethod Comparison Summary:\")\nprint(\"-\" * 50)\nprint(f\"{'Method':&lt;25} {'Return':&gt;10} {'Risk':&gt;10} {'Sharpe':&gt;10}\")\nprint(\"-\" * 50)\nfor method_name, (weights, ret, risk) in methods:\n    sharpe = (ret - risk_free_rate) / risk\n    print(f\"{method_name:&lt;25} {ret*100:&gt;9.2f}% {risk*100:&gt;9.2f}% {sharpe:&gt;9.2f}\")\n</code></pre>"},{"location":"user-guide/allocation/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/allocation/#1-data-preparation","title":"1. Data Preparation","text":"<ul> <li>1.1. Data Quality: Use clean, adjusted price data</li> <li>1.2. Missing Values: Handle missing values appropriately</li> <li>1.3. Transaction Costs: Consider transaction costs and liquidity</li> </ul>"},{"location":"user-guide/allocation/#2-risk-management","title":"2. Risk Management","text":"<ul> <li>2.1. Position Sizing: Consider using half-Kelly or quarter-Kelly for more conservative position sizing</li> <li>2.2. Constraints: Implement position limits and constraints</li> <li>2.3. Monitoring: Monitor portfolio turnover and rebalancing needs</li> </ul>"},{"location":"user-guide/allocation/#3-method-selection","title":"3. Method Selection","text":"<ul> <li>3.1. Mean-Variance: Good for traditional portfolio optimization</li> <li>3.2. Risk Parity: Better for risk management</li> <li>3.3. Kelly Criterion: Best for long-term growth</li> <li>3.4. Black-Litterman: Ideal when you have strong market views</li> <li>3.5. HRP: More robust to estimation errors</li> </ul>"},{"location":"user-guide/allocation/#4-portfolio-maintenance","title":"4. Portfolio Maintenance","text":"<ul> <li>4.1. Rebalancing Thresholds: Set appropriate rebalancing thresholds</li> <li>4.2. Cost Management: Consider transaction costs when rebalancing</li> <li>4.3. Tracking Error: Monitor tracking error against benchmarks</li> </ul>"},{"location":"user-guide/allocation/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"user-guide/allocation/#1-estimation-issues","title":"1. Estimation Issues","text":"<ul> <li>1.1. Overfitting: Use sufficient historical data</li> <li>1.2. Sample Bias: Consider using rolling windows</li> <li>1.3. Validation: Implement out-of-sample testing</li> </ul>"},{"location":"user-guide/allocation/#2-statistical-challenges","title":"2. Statistical Challenges","text":"<ul> <li>2.1. Estimation Error: Use robust estimation methods</li> <li>2.2. Shrinkage: Consider shrinkage estimators</li> <li>2.3. Regularization: Implement proper regularization</li> </ul>"},{"location":"user-guide/allocation/#3-implementation-realities","title":"3. Implementation Realities","text":"<ul> <li>3.1. Bid-Ask Spreads: Account for bid-ask spreads</li> <li>3.2. Market Impact: Consider market impact of trades</li> <li>3.3. Turnover Constraints: Implement turnover constraints</li> </ul>"},{"location":"user-guide/credit-scoring/","title":"Credit Scoring and Risk Assessment","text":"<p>This document demonstrates the credit scoring and risk assessment capabilities available in the <code>pypulate</code> library using the <code>CreditScoring</code> class.</p>"},{"location":"user-guide/credit-scoring/#overview","title":"Overview","text":"<p>The <code>CreditScoring</code> class provides a comprehensive suite of credit risk models and assessment tools:</p> <ol> <li>Altman Z-Score for bankruptcy prediction</li> <li>Merton Model for default probability</li> <li>Debt Service Coverage Ratio (DSCR)</li> <li>Weight of Evidence (WOE) and Information Value (IV)</li> <li>Logistic Regression Scoring</li> <li>Credit Scorecard Creation</li> <li>Credit Rating Transition Matrix</li> <li>Financial Ratios Analysis</li> <li>Expected Credit Loss (ECL) Calculation</li> <li>Risk-Based Loan Pricing</li> <li>Scoring Model Validation</li> <li>Loss Given Default (LGD) Estimation</li> <li>Exposure at Default (EAD) Calculation</li> </ol>"},{"location":"user-guide/credit-scoring/#basic-usage","title":"Basic Usage","text":"<p>First, let's import the necessary components:</p> <pre><code>from pypulate import CreditScoring\n</code></pre>"},{"location":"user-guide/credit-scoring/#corporate-credit-risk-assessment","title":"Corporate Credit Risk Assessment","text":"<p>Let's start with a basic corporate credit risk assessment using the Altman Z-Score model:</p> <pre><code># Initialize the CreditScoring class\ncredit = CreditScoring()\n\n# Company financial data\nworking_capital = 1200000\nretained_earnings = 1500000\nebit = 800000\nmarket_value_equity = 5000000\nsales = 4500000\ntotal_assets = 6000000\ntotal_liabilities = 2500000\n\n# Calculate Altman Z-Score\nz_score_result = credit.altman_z_score(\n    working_capital=working_capital,\n    retained_earnings=retained_earnings,\n    ebit=ebit,\n    market_value_equity=market_value_equity,\n    sales=sales,\n    total_assets=total_assets,\n    total_liabilities=total_liabilities\n)\n\nprint(f\"Altman Z-Score: {z_score_result['z_score']:.2f}\")\nprint(f\"Risk Assessment: {z_score_result['risk_assessment']}\")\n</code></pre>"},{"location":"user-guide/credit-scoring/#default-probability-estimation","title":"Default Probability Estimation","text":"<p>The Merton model provides a structural approach to estimating default probability:</p> <pre><code># Company data for Merton model\nasset_value = 10000000\ndebt_face_value = 5000000\nasset_volatility = 0.25\nrisk_free_rate = 0.03\ntime_to_maturity = 1.0\n\n# Calculate default probability using Merton model\nmerton_result = credit.merton_model(\n    asset_value=asset_value,\n    debt_face_value=debt_face_value,\n    asset_volatility=asset_volatility,\n    risk_free_rate=risk_free_rate,\n    time_to_maturity=time_to_maturity\n)\n\nprint(f\"Probability of Default: {merton_result['probability_of_default']:.2%}\")\nprint(f\"Distance to Default: {merton_result['distance_to_default']:.2f}\")\n</code></pre>"},{"location":"user-guide/credit-scoring/#debt-servicing-capacity","title":"Debt Servicing Capacity","text":"<p>The Debt Service Coverage Ratio helps assess a borrower's ability to service debt:</p> <pre><code># Debt service data\nnet_operating_income = 500000\ntotal_debt_service = 300000\n\n# Calculate DSCR\ndscr_result = credit.debt_service_coverage_ratio(\n    net_operating_income=net_operating_income,\n    total_debt_service=total_debt_service\n)\n\nprint(f\"DSCR: {dscr_result['dscr']:.2f}\")\nprint(f\"Assessment: {dscr_result['assessment']}\")\n</code></pre>"},{"location":"user-guide/credit-scoring/#credit-scorecard-development","title":"Credit Scorecard Development","text":"<p>Create a points-based credit scorecard for retail lending:</p> <pre><code># Applicant features\nfeatures = {\n    \"age\": 35,\n    \"income\": 75000,\n    \"years_employed\": 5,\n    \"debt_to_income\": 0.3,\n    \"previous_defaults\": 0\n}\n\n# Feature weights (derived from statistical analysis)\nweights = {\n    \"age\": 2.5,\n    \"income\": 3.2,\n    \"years_employed\": 4.0,\n    \"debt_to_income\": -5.5,\n    \"previous_defaults\": -25.0\n}\n\n# Feature offsets (reference points)\noffsets = {\n    \"age\": 25,\n    \"income\": 50000,\n    \"years_employed\": 2,\n    \"debt_to_income\": 0.4,\n    \"previous_defaults\": 1\n}\n\n# Create scorecard\nscorecard_result = credit.create_scorecard(\n    features=features,\n    weights=weights,\n    offsets=offsets,\n    scaling_factor=20,\n    base_score=600\n)\n\nprint(f\"Total Credit Score: {scorecard_result['total_score']:.0f}\")\nprint(f\"Risk Category: {scorecard_result['risk_category']}\")\nprint(\"\\nPoints Breakdown:\")\nfor feature, points in scorecard_result['points_breakdown'].items():\n    print(f\"  {feature}: {points:.0f} points\")\n</code></pre>"},{"location":"user-guide/credit-scoring/#expected-credit-loss-calculation","title":"Expected Credit Loss Calculation","text":"<p>Calculate the expected credit loss for a loan:</p> <pre><code># Loan risk parameters\npd = 0.05  # Probability of default\nlgd = 0.4  # Loss given default\nead = 100000  # Exposure at default\ntime_horizon = 1.0  # 1 year\ndiscount_rate = 0.03  # 3% discount rate\n\n# Calculate ECL\necl_result = credit.expected_credit_loss(\n    pd=pd,\n    lgd=lgd,\n    ead=ead,\n    time_horizon=time_horizon,\n    discount_rate=discount_rate\n)\n\nprint(f\"Expected Credit Loss: ${ecl_result['ecl']:.2f}\")\nprint(f\"ECL as % of Exposure: {ecl_result['ecl_percentage']:.2%}\")\n</code></pre>"},{"location":"user-guide/credit-scoring/#risk-based-loan-pricing","title":"Risk-Based Loan Pricing","text":"<p>Determine the appropriate interest rate for a loan based on risk:</p> <pre><code># Loan and risk parameters\nloan_amount = 250000\nterm = 5  # 5 years\npd = 0.03  # Annual probability of default\nlgd = 0.35  # Loss given default\nfunding_cost = 0.04  # Cost of funds\noperating_cost = 0.01  # Operating costs as % of loan\ncapital_requirement = 0.08  # Capital requirement\ntarget_roe = 0.15  # Target return on equity\n\n# Calculate loan pricing\npricing_result = credit.loan_pricing(\n    loan_amount=loan_amount,\n    term=term,\n    pd=pd,\n    lgd=lgd,\n    funding_cost=funding_cost,\n    operating_cost=operating_cost,\n    capital_requirement=capital_requirement,\n    target_roe=target_roe\n)\n\nprint(f\"Recommended Interest Rate: {pricing_result['recommended_rate']:.2%}\")\nprint(f\"Effective Annual Rate: {pricing_result['effective_annual_rate']:.2%}\")\nprint(\"\\nRate Components:\")\nfor component, value in pricing_result['components'].items():\n    print(f\"  {component}: {value:.2%}\")\n</code></pre>"},{"location":"user-guide/credit-scoring/#credit-rating-transition-analysis","title":"Credit Rating Transition Analysis","text":"<p>Analyze how credit ratings migrate over time:</p> <pre><code># Historical credit ratings data\nratings_t0 = ['AAA', 'AA', 'A', 'BBB', 'BB', 'A', 'BBB', 'BB', 'B', 'CCC']  # Initial ratings\nratings_t1 = ['AA', 'A', 'BBB', 'BB', 'B', 'A', 'BBB', 'CCC', 'CCC', 'D']   # Ratings after 1 year\n\n# Calculate transition matrix\ntransition_result = credit.transition_matrix(\n    ratings_t0=ratings_t0,\n    ratings_t1=ratings_t1\n)\n\nprint(\"Credit Rating Transition Matrix (Probabilities):\")\nprob_matrix = transition_result['probability_matrix']\nratings = transition_result['ratings']\n\n# Print the matrix with proper formatting\nprint(f\"{'':5}\", end=\"\")\nfor r in ratings:\n    print(f\"{r:6}\", end=\"\")\nprint()\n\nfor i, row in enumerate(prob_matrix):\n    print(f\"{ratings[i]:5}\", end=\"\")\n    for val in row:\n        print(f\"{val:.2f}  \", end=\"\")\n    print()\n</code></pre>"},{"location":"user-guide/credit-scoring/#financial-ratios-analysis","title":"Financial Ratios Analysis","text":"<p>Analyze a company's financial health through key ratios:</p> <pre><code># Company financial data\ncurrent_assets = 2000000\ncurrent_liabilities = 1200000\ntotal_assets = 8000000\ntotal_liabilities = 4000000\nebit = 1200000\ninterest_expense = 300000\nnet_income = 700000\ntotal_equity = 4000000\nsales = 6000000\n\n# Calculate financial ratios\nratios_result = credit.financial_ratios(\n    current_assets=current_assets,\n    current_liabilities=current_liabilities,\n    total_assets=total_assets,\n    total_liabilities=total_liabilities,\n    ebit=ebit,\n    interest_expense=interest_expense,\n    net_income=net_income,\n    total_equity=total_equity,\n    sales=sales\n)\n\nprint(\"Key Financial Ratios:\")\nfor category, ratios in ratios_result.items():\n    if category != 'overall_assessment':\n        print(f\"\\n{category.replace('_', ' ').title()}:\")\n        for ratio_name, value in ratios.items():\n            if ratio_name != 'assessment':\n                print(f\"  {ratio_name.replace('_', ' ').title()}: {value:.2f}\")\n        print(f\"  Assessment: {ratios['assessment']}\")\n\nprint(f\"\\nOverall Financial Health: {ratios_result['overall_assessment']}\")\n</code></pre>"},{"location":"user-guide/credit-scoring/#scoring-model-validation","title":"Scoring Model Validation","text":"<p>Validate the performance of a credit scoring model:</p> <pre><code>import numpy as np\n\n# Simulated data: predicted scores and actual defaults\nnp.random.seed(42)\nnum_samples = 1000\npredicted_scores = np.random.normal(650, 100, num_samples)\n# Higher scores should correspond to lower default probability\ndefault_probs = 1 / (1 + np.exp((predicted_scores - 600) / 50))\nactual_defaults = np.random.binomial(1, default_probs)\n\n# Validate the scoring model\nvalidation_result = credit.scoring_model_validation(\n    predicted_scores=predicted_scores,\n    actual_defaults=actual_defaults,\n    score_bins=10\n)\n\nprint(f\"Gini Coefficient: {validation_result['gini']:.4f}\")\nprint(f\"KS Statistic: {validation_result['ks']:.4f}\")\nprint(f\"AUC-ROC: {validation_result['auc']:.4f}\")\nprint(f\"Accuracy: {validation_result['accuracy']:.4f}\")\n</code></pre>"},{"location":"user-guide/credit-scoring/#loss-given-default-estimation","title":"Loss Given Default Estimation","text":"<p>Estimate the loss given default for a secured loan:</p> <pre><code># Loan and collateral data\ncollateral_value = 180000\nloan_amount = 200000\nliquidation_costs = 0.15\ntime_to_recovery = 1.5\n\n# Calculate LGD\nlgd_result = credit.loss_given_default(\n    collateral_value=collateral_value,\n    loan_amount=loan_amount,\n    liquidation_costs=liquidation_costs,\n    time_to_recovery=time_to_recovery\n)\n\nprint(f\"Loss Given Default: {lgd_result['lgd']:.2%}\")\nprint(f\"Recovery Rate: {lgd_result['recovery_rate']:.2%}\")\nprint(f\"Expected Loss Amount: ${lgd_result['expected_loss']:.2f}\")\n</code></pre>"},{"location":"user-guide/credit-scoring/#exposure-at-default-calculation","title":"Exposure at Default Calculation","text":"<p>Calculate the exposure at default for a credit facility:</p> <pre><code># Credit facility data\ncurrent_balance = 500000\nundrawn_amount = 300000\ncredit_conversion_factor = 0.6\n\n# Calculate EAD\nead_result = credit.exposure_at_default(\n    current_balance=current_balance,\n    undrawn_amount=undrawn_amount,\n    credit_conversion_factor=credit_conversion_factor\n)\n\nprint(f\"Exposure at Default: ${ead_result['ead']:.2f}\")\nprint(f\"EAD Components:\")\nprint(f\"  Drawn Balance: ${ead_result['drawn_balance']:.2f}\")\nprint(f\"  Expected Draw: ${ead_result['expected_draw']:.2f}\")\n</code></pre>"},{"location":"user-guide/credit-scoring/#tracking-model-usage","title":"Tracking Model Usage","text":"<p>The CreditScoring class maintains a history of all calculations:</p> <pre><code># Get history of calculations\nhistory = credit.get_history()\n\nprint(f\"Number of calculations performed: {len(history)}\")\nprint(\"\\nRecent calculations:\")\nfor i, entry in enumerate(history[-3:]):\n    print(f\"{i+1}. Model: {entry['model']}\")\n</code></pre>"},{"location":"user-guide/credit-scoring/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/credit-scoring/#1-data-quality","title":"1. Data Quality","text":"<ul> <li>Data Cleaning: Ensure financial data is accurate and complete</li> <li>Outlier Treatment: Handle outliers appropriately</li> <li>Missing Values: Develop a consistent approach for missing data</li> </ul>"},{"location":"user-guide/credit-scoring/#2-model-selection","title":"2. Model Selection","text":"<ul> <li>Purpose Fit: Choose models appropriate for the specific credit assessment need</li> <li>Complexity: Balance model complexity with interpretability</li> <li>Validation: Regularly validate model performance</li> </ul>"},{"location":"user-guide/credit-scoring/#3-risk-management","title":"3. Risk Management","text":"<ul> <li>Stress Testing: Test models under adverse scenarios</li> <li>Sensitivity Analysis: Understand how changes in inputs affect outputs</li> <li>Model Limitations: Be aware of each model's limitations</li> </ul>"},{"location":"user-guide/credit-scoring/#4-implementation","title":"4. Implementation","text":"<ul> <li>Documentation: Document assumptions and methodologies</li> <li>Monitoring: Regularly monitor model performance</li> <li>Updating: Update models as economic conditions change</li> </ul>"},{"location":"user-guide/credit-scoring/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"user-guide/credit-scoring/#1-model-misuse","title":"1. Model Misuse","text":"<ul> <li>Inappropriate Application: Using models outside their intended domain</li> <li>Overreliance: Relying too heavily on quantitative models without qualitative assessment</li> <li>Outdated Models: Using models that haven't been updated for current conditions</li> </ul>"},{"location":"user-guide/credit-scoring/#2-data-issues","title":"2. Data Issues","text":"<ul> <li>Sample Bias: Training on non-representative data</li> <li>Look-ahead Bias: Using information not available at decision time</li> <li>Data Staleness: Using outdated financial information</li> </ul>"},{"location":"user-guide/credit-scoring/#3-implementation-challenges","title":"3. Implementation Challenges","text":"<ul> <li>Parameter Sensitivity: Results highly sensitive to input parameters</li> <li>Model Risk: Risk of model error or misspecification</li> <li>Interpretation Errors: Misinterpreting model outputs </li> </ul>"},{"location":"user-guide/filters/","title":"Filters","text":"<p>Pypulate provides a comprehensive set of filtering techniques for financial time series data. This page explains the different types of filters available and how to use them.</p>"},{"location":"user-guide/filters/#overview","title":"Overview","text":"<p>Filters in Pypulate are designed to clean, smooth, and extract meaningful information from noisy financial time series data. The module includes:</p> <ol> <li>Kalman Filters: Optimal estimators for linear systems</li> <li>Signal Filters: Classical signal processing filters</li> <li>Adaptive Filters: Filters that adapt to changing data characteristics</li> <li>Particle Filters: Monte Carlo methods for non-linear/non-Gaussian systems</li> </ol>"},{"location":"user-guide/filters/#kalman-filters","title":"Kalman Filters","text":"<p>Kalman filters are optimal estimators that infer parameters of interest from indirect, inaccurate, and uncertain observations.</p>"},{"location":"user-guide/filters/#standard-kalman-filter","title":"Standard Kalman Filter","text":"<p>The standard Kalman filter is ideal for linear systems with Gaussian noise:</p> <pre><code>import numpy as np\nfrom pypulate.filters import kalman_filter\n\n# Create noisy data\nx = np.linspace(0, 10, 100)\ntrue_signal = np.sin(x)\nnoisy_signal = true_signal + np.random.normal(0, 0.1, len(x))\n\n# Apply Kalman filter\nfiltered_signal = kalman_filter(\n    noisy_signal,\n    process_variance=1e-5,\n    measurement_variance=1e-3\n)\n</code></pre>"},{"location":"user-guide/filters/#extended-kalman-filter-ekf","title":"Extended Kalman Filter (EKF)","text":"<p>The EKF is used for non-linear systems by linearizing around the current estimate:</p> <pre><code>import numpy as np\nfrom pypulate.filters import extended_kalman_filter\n\n# Define non-linear system\ndef state_transition(x):\n    # Non-linear state transition function\n    return np.array([x[0] + x[1], 0.5 * x[1]])\n\ndef observation(x):\n    # Non-linear observation function\n    return np.array([np.sin(x[0])])\n\ndef process_jacobian(x):\n    # Jacobian of state transition function\n    return np.array([[1, 1], [0, 0.5]])\n\ndef observation_jacobian(x):\n    # Jacobian of observation function\n    return np.array([[np.cos(x[0]), 0]])\n\n# Apply EKF\nQ = np.eye(2) * 0.01  # Process noise covariance\nR = np.array([[0.1]])  # Observation noise covariance\nfiltered_states = extended_kalman_filter(\n    observations, state_transition, observation,\n    process_jacobian, observation_jacobian, Q, R\n)\n</code></pre>"},{"location":"user-guide/filters/#unscented-kalman-filter-ukf","title":"Unscented Kalman Filter (UKF)","text":"<p>The UKF uses sigma points to handle non-linearities without requiring Jacobians:</p> <pre><code>import numpy as np\nfrom pypulate.filters import unscented_kalman_filter\n\n# Define non-linear system\ndef state_transition(x):\n    # Non-linear state transition function\n    return np.array([x[0] + x[1], 0.5 * x[1]])\n\ndef observation(x):\n    # Non-linear observation function\n    return np.array([np.sin(x[0])])\n\n# Apply UKF\nQ = np.eye(2) * 0.01  # Process noise covariance\nR = np.array([[0.1]])  # Observation noise covariance\nfiltered_states = unscented_kalman_filter(\n    observations, state_transition, observation, Q, R\n)\n</code></pre>"},{"location":"user-guide/filters/#signal-filters","title":"Signal Filters","text":"<p>Signal filters are used to remove noise and extract specific frequency components from time series data.</p>"},{"location":"user-guide/filters/#butterworth-filter","title":"Butterworth Filter","text":"<p>The Butterworth filter provides a flat frequency response in the passband:</p> <pre><code>import numpy as np\nfrom pypulate.filters import butterworth_filter\n\n# Create noisy data with multiple frequency components\nx = np.linspace(0, 10, 1000)\nsignal = np.sin(2 * np.pi * 0.05 * x) + 0.5 * np.sin(2 * np.pi * 0.25 * x)\n\n# Apply lowpass filter to remove high frequency component\nfiltered = butterworth_filter(\n    signal,\n    cutoff=0.1,  # Cutoff frequency\n    order=4,     # Filter order\n    filter_type='lowpass'\n)\n</code></pre>"},{"location":"user-guide/filters/#savitzky-golay-filter","title":"Savitzky-Golay Filter","text":"<p>The Savitzky-Golay filter smooths data by fitting successive sub-sets of adjacent data points with a low-degree polynomial:</p> <pre><code>import numpy as np\nfrom pypulate.filters import savitzky_golay_filter\n\n# Create noisy data\nx = np.linspace(0, 10, 100)\nsignal = np.sin(x) + np.random.normal(0, 0.1, len(x))\n\n# Apply Savitzky-Golay filter\nfiltered = savitzky_golay_filter(\n    signal,\n    window_length=11,  # Must be odd\n    polyorder=3        # Polynomial order\n)\n</code></pre>"},{"location":"user-guide/filters/#median-filter","title":"Median Filter","text":"<p>The median filter is excellent for removing outliers:</p> <pre><code>import numpy as np\nfrom pypulate.filters import median_filter\n\n# Create data with outliers\nx = np.linspace(0, 10, 100)\nsignal = np.sin(x)\nsignal[10] = 5  # Add outlier\nsignal[50] = -5  # Add outlier\n\n# Apply median filter\nfiltered = median_filter(signal, kernel_size=5)\n</code></pre>"},{"location":"user-guide/filters/#hampel-filter","title":"Hampel Filter","text":"<p>The Hampel filter is specifically designed for outlier detection and removal:</p> <pre><code>import numpy as np\nfrom pypulate.filters import hampel_filter\n\n# Create data with outliers\nx = np.linspace(0, 10, 100)\nsignal = np.sin(x)\nsignal[10] = 5  # Add outlier\nsignal[50] = -5  # Add outlier\n\n# Apply Hampel filter\nfiltered = hampel_filter(\n    signal,\n    window_size=5,  # Window size\n    n_sigmas=3.0    # Threshold for outlier detection\n)\n</code></pre>"},{"location":"user-guide/filters/#hodrick-prescott-filter","title":"Hodrick-Prescott Filter","text":"<p>The Hodrick-Prescott filter decomposes a time series into trend and cycle components:</p> <pre><code>import numpy as np\nfrom pypulate.filters import hodrick_prescott_filter\n\n# Create data with trend and cycle\nx = np.linspace(0, 10, 100)\ntrend = 0.1 * x**2\ncycle = np.sin(2 * np.pi * 0.1 * x)\ndata = trend + cycle\n\n# Apply Hodrick-Prescott filter\ntrend_component, cycle_component = hodrick_prescott_filter(\n    data,\n    lambda_param=100  # Smoothing parameter\n)\n</code></pre>"},{"location":"user-guide/filters/#adaptive-filters","title":"Adaptive Filters","text":"<p>Adaptive filters automatically adjust their parameters based on the input data.</p>"},{"location":"user-guide/filters/#adaptive-kalman-filter","title":"Adaptive Kalman Filter","text":"<p>The adaptive Kalman filter adjusts its noise parameters based on the observed data:</p> <pre><code>import numpy as np\nfrom pypulate.filters import adaptive_kalman_filter\n\n# Create noisy data with changing dynamics\nx = np.linspace(0, 10, 200)\ntrue_signal = np.sin(x) + 0.1 * x\nnoise_level = 0.1 * (1 + np.sin(x/2))  # Changing noise level\nnoisy_signal = true_signal + noise_level * np.random.randn(len(x))\n\n# Apply adaptive Kalman filter\nfiltered_signal = adaptive_kalman_filter(\n    noisy_signal,\n    adaptation_rate=0.05,  # Rate of adaptation\n    window_size=10         # Window for innovation estimation\n)\n</code></pre>"},{"location":"user-guide/filters/#least-mean-squares-lms-filter","title":"Least Mean Squares (LMS) Filter","text":"<p>The LMS filter is a simple adaptive filter that minimizes the mean square error:</p> <pre><code>import numpy as np\nfrom pypulate.filters import least_mean_squares_filter\n\n# Create noisy data\nx = np.linspace(0, 10, 1000)\nclean_signal = np.sin(2 * np.pi * 0.05 * x)\nnoise = 0.2 * np.random.randn(len(x))\nnoisy_signal = clean_signal + noise\n\n# Apply LMS filter\nfiltered_signal, weights = least_mean_squares_filter(\n    noisy_signal,\n    filter_length=10,  # Filter length\n    mu=0.02            # Step size\n)\n</code></pre>"},{"location":"user-guide/filters/#particle-filters","title":"Particle Filters","text":"<p>Particle filters are Monte Carlo methods that can handle non-linear and non-Gaussian systems.</p>"},{"location":"user-guide/filters/#standard-particle-filter","title":"Standard Particle Filter","text":"<p>The particle filter uses a set of particles to represent the posterior distribution:</p> <pre><code>import numpy as np\nfrom pypulate.filters import particle_filter\n\n# Define model functions\ndef state_transition(particles):\n    # Simple random walk model\n    return particles\n\ndef process_noise(particles):\n    # Add Gaussian noise\n    return particles + np.random.normal(0, 0.1, particles.shape)\n\ndef observation_func(state):\n    # Identity observation model\n    return state\n\ndef observation_likelihood(observation, predicted_observation):\n    # Gaussian likelihood\n    return np.exp(-0.5 * ((observation - predicted_observation) / 0.1) ** 2)\n\ndef initial_state(n):\n    # Initial particles from normal distribution\n    return np.random.normal(0, 1, n)\n\n# Apply particle filter\nfiltered_states, weights = particle_filter(\n    observations,\n    state_transition,\n    observation_func,\n    process_noise,\n    observation_likelihood,\n    n_particles=1000,\n    initial_state_func=initial_state\n)\n</code></pre>"},{"location":"user-guide/filters/#bootstrap-particle-filter","title":"Bootstrap Particle Filter","text":"<p>The bootstrap particle filter is a simplified version that resamples at every step:</p> <pre><code>import numpy as np\nfrom pypulate.filters import bootstrap_particle_filter\n\n# Define model functions\ndef state_transition(particles):\n    # Simple random walk model\n    return particles\n\ndef observation_func(state):\n    # Identity observation model\n    return state\n\n# Apply bootstrap particle filter\nfiltered_states, weights = bootstrap_particle_filter(\n    observations,\n    state_transition,\n    observation_func,\n    process_noise_std=0.1,\n    observation_noise_std=0.1,\n    n_particles=1000\n)\n</code></pre>"},{"location":"user-guide/filters/#choosing-the-right-filter","title":"Choosing the Right Filter","text":"<p>The choice of filter depends on your specific application:</p> <ul> <li>Kalman Filters: Best for linear systems or when you have a good model of the system dynamics</li> <li>Signal Filters: Good for general noise removal and frequency-based filtering</li> <li>Adaptive Filters: Useful when the signal characteristics change over time</li> <li>Particle Filters: Best for highly non-linear systems or non-Gaussian noise</li> </ul> <p>For financial time series, consider:</p> <ul> <li>Kalman/Adaptive Filters: For tracking changing trends</li> <li>Hampel/Median Filters: For removing outliers (e.g., flash crashes)</li> <li>Hodrick-Prescott Filter: For separating trend and cycle components</li> <li>Butterworth Filter: For removing high-frequency noise</li> </ul>"},{"location":"user-guide/filters/#example-combining-filters","title":"Example: Combining Filters","text":"<p>You can combine multiple filters for more sophisticated processing:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom pypulate.filters import hampel_filter, kalman_filter\n\n# Create data with outliers and noise\nx = np.linspace(0, 10, 200)\ntrue_signal = np.sin(x) + 0.1 * x\nnoisy_signal = true_signal + 0.1 * np.random.randn(len(x))\nnoisy_signal[20] = 5  # Add outlier\nnoisy_signal[100] = -5  # Add outlier\n\n# First remove outliers with Hampel filter\noutlier_removed = hampel_filter(noisy_signal, window_size=5, n_sigmas=3.0)\n\n# Then smooth with Kalman filter\nfinal_signal = kalman_filter(outlier_removed, process_variance=1e-5, measurement_variance=1e-3)\n\n# Plot results\nplt.figure(figsize=(12, 6))\nplt.plot(x, true_signal, 'k-', label='True Signal')\nplt.plot(x, noisy_signal, 'r.', alpha=0.5, label='Noisy Signal with Outliers')\nplt.plot(x, outlier_removed, 'g-', alpha=0.7, label='After Hampel Filter')\nplt.plot(x, final_signal, 'b-', linewidth=2, label='After Kalman Filter')\nplt.legend()\nplt.title('Multi-stage Filtering')\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre> <p>Filters also works with Parray chain methods. You can combine them with other tools easily to make advanced techniques.</p>"},{"location":"user-guide/getting-started/","title":"Getting Started with Pypulate","text":"<p>This guide will help you get started with Pypulate for financial time series analysis, business KPI calculations, and portfolio management.</p>"},{"location":"user-guide/getting-started/#installation","title":"Installation","text":"<pre><code>pip install pypulate\n</code></pre>"},{"location":"user-guide/getting-started/#core-components","title":"Core Components","text":"<p>Pypulate provides powerful classes for financial and business analytics:</p>"},{"location":"user-guide/getting-started/#1-parray-pypulate-array","title":"1. Parray (Pypulate Array)","text":"<p>The <code>Parray</code> class extends NumPy arrays with financial analysis capabilities, preprocessing functions, and performance optimizations:</p> <pre><code>from pypulate import Parray\nimport numpy as np\n\n# Create a price array from various sources\nprices = Parray([10, 11, 12, 11, 10, 9, 10, 11, 12, 13, 15, 11, 8, 10, 14, 16])\nprices_from_numpy = Parray(np.random.normal(100, 5, 1000))\n\n# Technical Analysis with method chaining\nresult = (prices\n    .sma(3)                    # Simple Moving Average\n    .ema(3)                    # Exponential Moving Average\n    .rsi(7)                    # Relative Strength Index\n)\n\n# Data Preprocessing\nclean_data = (prices\n    .remove_outliers(method='zscore', threshold=3.0)  # Remove statistical outliers\n    .fill_missing(method='forward')                  # Fill missing values with forward fill\n    .interpolate_missing(method='cubic')             # Interpolate remaining missing values\n)\n\n# Standardization and Normalization\nnormalized_data = prices.normalize(method='l2')              # L2 normalization\nstandardized_data = prices.standardize()                     # Z-score standardization\nscaled_data = prices.min_max_scale(feature_range=(0, 1))     # Min-max scaling\nrobust_data = prices.robust_scale(method='iqr')              # Robust scaling using IQR\n\n# Transformations\nlog_data = prices.log_transform(offset=1.0)                  # Log transformation\npower_transformed = prices.power_transform(method='yeo-johnson')  # Power transformation\ndiscretized = prices.discretize(n_bins=5, strategy='quantile')  # Discretization\n\n# Time Series Operations\nlagged_features = prices.lag_features(lags=[1, 2, 3])        # Create lag features\nresampled = prices.resample(factor=2, method='mean')         # Downsample data\n\n# Signal Detection\nfast_ma = prices.sma(3)\nslow_ma = prices.sma(12)\ngolden_cross = fast_ma.crossover(slow_ma)\ndeath_cross = fast_ma.crossunder(slow_ma)\n\n# Statistical Tests\nadf_stat, adf_pvalue = prices.augmented_dickey_fuller_test()  # Test for stationarity\nstats = prices.descriptive_stats()                           # Get descriptive statistics\n</code></pre>"},{"location":"user-guide/getting-started/#key-features-of-parray","title":"Key Features of Parray","text":"<ol> <li> <p>Performance Optimization</p> <ul> <li>GPU Acceleration: Use <code>enable_gpu()</code> for faster computations on compatible hardware</li> <li>Parallel Processing: Use <code>enable_parallel()</code> to utilize multiple CPU cores</li> <li>Memory Optimization: Use <code>optimize_memory()</code> for efficient memory usage with large datasets</li> </ul> </li> <li> <p>Data Preprocessing</p> <ul> <li>Outlier Handling: <code>remove_outliers()</code>, <code>winsorize()</code>, <code>clip_outliers()</code></li> <li>Missing Value Handling: <code>fill_missing()</code>, <code>interpolate_missing()</code></li> <li>Normalization: <code>normalize()</code>, <code>standardize()</code>, <code>min_max_scale()</code>, <code>robust_scale()</code></li> <li>Transformations: <code>log_transform()</code>, <code>power_transform()</code>, <code>dynamic_tanh()</code></li> </ul> </li> <li> <p>Technical Analysis</p> <ul> <li>Moving Averages: <code>sma()</code>, <code>ema()</code>, <code>wma()</code>, <code>hma()</code>, <code>zlma()</code>, etc.</li> <li>Momentum Indicators: <code>rsi()</code>, <code>macd()</code>, <code>stochastic_oscillator()</code>, etc.</li> <li>Volatility Indicators: <code>bollinger_bands()</code>, <code>atr()</code>, <code>keltner_channels()</code>, etc.</li> <li>Signal Detection: <code>crossover()</code>, <code>crossunder()</code></li> </ul> </li> <li> <p>Method Chaining</p> <ul> <li>Chain methods together for complex operations in a single, readable line</li> <li>Avoid creating intermediate variables</li> <li>Efficiently combine preprocessing, analysis, and signal generation</li> </ul> </li> </ol>"},{"location":"user-guide/getting-started/#2-kpi-key-performance-indicators","title":"2. KPI (Key Performance Indicators)","text":"<p>The <code>KPI</code> class manages business metrics and health assessment:</p> <pre><code>from pypulate import KPI\n\n# Initialize KPI tracker\nkpi = KPI()\n\n# Customer Metrics\nchurn = kpi.churn_rate(\n    customers_start=1000,\n    customers_end=950,\n    new_customers=50\n)\n\n# Financial Metrics\nclv = kpi.customer_lifetime_value(\n    avg_revenue_per_customer=100,\n    gross_margin=70,\n    churn_rate_value=5\n)\n\n# Health Assessment\nhealth = kpi.health\nprint(f\"Business Health Score: {health['overall_score']}\")\nprint(f\"Status: {health['status']}\")\n</code></pre>"},{"location":"user-guide/getting-started/#3-portfolio","title":"3. Portfolio","text":"<p>The <code>Portfolio</code> class handles portfolio analysis and risk management:</p> <pre><code>from pypulate import Portfolio\n\n# Initialize portfolio analyzer\nportfolio = Portfolio()\n\n# Calculate Returns\nreturns = portfolio.simple_return([50, 100, 120], [60, 70, 120])\ntwrr = portfolio.time_weighted_return(\n    [0.02, 0.01, 0.1, 0.003]\n)\n\n# Risk Analysis\nsharpe = portfolio.sharpe_ratio(returns, risk_free_rate=0.02)\nvar = portfolio.value_at_risk(returns, confidence_level=0.95)\n\n# Portfolio Health\nhealth = portfolio.health\nprint(f\"Portfolio Health Score: {health['overall_score']}\")\nprint(f\"Risk Status: {health['components']['risk']['status']}\")\n</code></pre>"},{"location":"user-guide/getting-started/#4-allocation","title":"4. Allocation","text":"<p>The <code>Allocation</code> class provides advanced portfolio optimization and asset allocation methods:</p> <pre><code>from pypulate import Allocation\nimport numpy as np\n\n# Initialize allocation optimizer\nallocation = Allocation()\n\n# Sample returns data (252 days, 5 assets)\nreturns = np.random.normal(0.0001, 0.02, (252, 5))\nrisk_free_rate = 0.04\n\n# Mean-Variance Optimization\nweights, ret, risk = allocation.mean_variance(\n    returns, \n    risk_free_rate=risk_free_rate\n)\nprint(f\"Mean-Variance Portfolio:\")\nprint(f\"Expected Return: {ret:.2%}\")\nprint(f\"Risk: {risk:.2%}\")\nprint(f\"Weights: {weights}\")\n\n# Risk Parity Portfolio\nweights, ret, risk = allocation.risk_parity(returns)\nprint(f\"\\nRisk Parity Portfolio:\")\nprint(f\"Expected Return: {ret:.2%}\")\nprint(f\"Risk: {risk:.2%}\")\nprint(f\"Weights: {weights}\")\n\n# Kelly Criterion (with half-Kelly)\nweights, ret, risk = allocation.kelly_criterion(\n    returns, \n    kelly_fraction=0.5\n)\nprint(f\"\\nHalf-Kelly Portfolio:\")\nprint(f\"Expected Return: {ret:.2%}\")\nprint(f\"Risk: {risk:.2%}\")\nprint(f\"Weights: {weights}\")\n\n# Black-Litterman with views\nviews = {0: 0.15, 1: 0.12}  # Views on first two assets\nview_confidences = {0: 0.8, 1: 0.7}\nmarket_caps = np.array([1000, 800, 600, 400, 200])\nweights, ret, risk = allocation.black_litterman(\n    returns, \n    market_caps, \n    views, \n    view_confidences\n)\nprint(f\"\\nBlack-Litterman Portfolio:\")\nprint(f\"Expected Return: {ret:.2%}\")\nprint(f\"Risk: {risk:.2%}\")\nprint(f\"Weights: {weights}\")\n\n# Hierarchical Risk Parity\nweights, ret, risk = allocation.hierarchical_risk_parity(returns)\nprint(f\"\\nHierarchical Risk Parity Portfolio:\")\nprint(f\"Expected Return: {ret:.2%}\")\nprint(f\"Risk: {risk:.2%}\")\nprint(f\"Weights: {weights}\")\n</code></pre>"},{"location":"user-guide/getting-started/#5-servicepricing","title":"5. ServicePricing","text":"<p>The <code>ServicePricing</code> class provides a unified interface for various pricing models:</p> <pre><code>from pypulate import ServicePricing\n\n# Initialize pricing calculator\npricing = ServicePricing()\n\n# Tiered Pricing\nprice = pricing.calculate_tiered_price(\n    usage_units=1500,\n    tiers={\n        \"0-1000\": 0.10,    # First tier: $0.10 per unit\n        \"1001-2000\": 0.08, # Second tier: $0.08 per unit\n        \"2001+\": 0.05      # Final tier: $0.05 per unit\n    }\n)\nprint(f\"Tiered Price: ${price:.2f}\")  # $140.00 (1000 * 0.10 + 500 * 0.08)\n\n# Subscription with Features\nsub_price = pricing.calculate_subscription_price(\n    base_price=99.99,\n    features=['premium', 'api_access'],\n    feature_prices={'premium': 49.99, 'api_access': 29.99},\n    duration_months=12,\n    discount_rate=0.10\n)\n\n# Track Pricing History\npricing.save_current_pricing()\nhistory = pricing.get_pricing_history()\n</code></pre>"},{"location":"user-guide/getting-started/#6-creditscoring","title":"6. CreditScoring","text":"<p>The <code>CreditScoring</code> class provides comprehensive credit risk assessment and scoring tools:</p> <pre><code>from pypulate.dtypes import CreditScoring\n\n# Initialize credit scoring system\ncredit = CreditScoring()\n\n# Corporate Credit Risk Assessment\nz_score_result = credit.altman_z_score(\n    working_capital=1200000,\n    retained_earnings=1500000,\n    ebit=800000,\n    market_value_equity=5000000,\n    sales=4500000,\n    total_assets=6000000,\n    total_liabilities=2500000\n)\nprint(f\"Altman Z-Score: {z_score_result['z_score']:.2f}\")\nprint(f\"Risk Assessment: {z_score_result['risk_assessment']}\")\n\n# Default Probability Estimation\nmerton_result = credit.merton_model(\n    asset_value=10000000,\n    debt_face_value=5000000,\n    asset_volatility=0.25,\n    risk_free_rate=0.03,\n    time_to_maturity=1.0\n)\nprint(f\"Probability of Default: {merton_result['probability_of_default']:.2%}\")\n\n# Credit Scorecard for Retail Lending\nfeatures = {\n    \"age\": 35,\n    \"income\": 75000,\n    \"years_employed\": 5,\n    \"debt_to_income\": 0.3,\n    \"previous_defaults\": 0\n}\nweights = {\n    \"age\": 2.5,\n    \"income\": 3.2,\n    \"years_employed\": 4.0,\n    \"debt_to_income\": -5.5,\n    \"previous_defaults\": -25.0\n}\nscorecard_result = credit.create_scorecard(\n    features=features,\n    weights=weights,\n    scaling_factor=20,\n    base_score=600\n)\nprint(f\"Credit Score: {scorecard_result['total_score']:.0f}\")\nprint(f\"Risk Category: {scorecard_result['risk_category']}\")\n\n# Expected Credit Loss Calculation\necl_result = credit.expected_credit_loss(\n    pd=0.05,  # Probability of default\n    lgd=0.4,  # Loss given default\n    ead=100000,  # Exposure at default\n    time_horizon=1.0\n)\nprint(f\"Expected Credit Loss: ${ecl_result['ecl']:.2f}\")\n\n# Track Model Usage\nhistory = credit.get_history()\n</code></pre>"},{"location":"user-guide/getting-started/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/getting-started/#1-method-chaining","title":"1. Method Chaining","text":"<p>Parray support method chaining for cleaner code:</p> <pre><code># Parray chaining\nsignals = (Parray(prices)\n    .sma(10)\n    .crossover(Parray(prices).sma(20))\n)\n</code></pre>"},{"location":"user-guide/getting-started/#2-health-assessments","title":"2. Health Assessments","text":"<p>Portfolio and KPI classes provide health assessments with consistent scoring:</p> <pre><code># Business Health\nkpi_health = kpi.health  # Business metrics health\n\n# Portfolio Health\nportfolio_health = portfolio.health  # Portfolio performance health\n\n# Health Status Categories\n# - Excellent: \u2265 90\n# - Good: \u2265 75\n# - Fair: \u2265 60\n# - Poor: \u2265 45\n# - Critical: &lt; 45\n</code></pre>"},{"location":"user-guide/getting-started/#3-state-management","title":"3. State Management","text":"<p>All classes maintain state for tracking and analysis:</p> <pre><code># KPI state\nstored_churn = kpi._state['churn_rate']\nstored_retention = kpi._state['retention_rate']\n\n# Portfolio state\nstored_returns = portfolio._state['returns']\nstored_risk = portfolio._state['volatility']\n\n# ServicePricing state\nstored_pricing = pricing._state['current_pricing']\npricing_history = pricing._state['pricing_history']\n\n# CreditScoring state\nmodel_history = credit._history  # History of credit model calculations\n</code></pre>"},{"location":"user-guide/getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you understand the basic components, explore these topics in detail:</p> <ul> <li>Parray Guide: Advanced technical analysis and signal detection</li> <li>KPI Guide: Comprehensive business metrics and health scoring</li> <li>Portfolio Guide: Portfolio analysis and risk management</li> <li>Service Pricing Guide: Pricing models and calculations</li> <li>Credit Scoring Guide: Credit risk assessment and scoring</li> </ul>"},{"location":"user-guide/kpi/","title":"KPI Guide","text":"<p>The <code>KPI</code> class provides a comprehensive suite of methods for calculating and tracking business metrics. It maintains state to provide health assessments and trend analysis.</p>"},{"location":"user-guide/kpi/#basic-usage","title":"Basic Usage","text":"<pre><code>from pypulate import KPI\n\n# Initialize KPI tracker\nkpi = KPI()\n\n# Calculate basic metrics\nchurn = kpi.churn_rate(customers_start=1000, customers_end=950, new_customers=50)\nretention = kpi.retention_rate(customers_start=1000, customers_end=950, new_customers=50)\n\n# Get health assessment\nhealth = kpi.health\n</code></pre>"},{"location":"user-guide/kpi/#customer-metrics","title":"Customer Metrics","text":""},{"location":"user-guide/kpi/#churn-and-retention","title":"Churn and Retention","text":"<pre><code># Calculate churn rate\nchurn = kpi.churn_rate(\n    customers_start=1000,  # Starting customer count\n    customers_end=950,     # Ending customer count\n    new_customers=50       # New customers acquired\n)\n\n# Calculate retention rate\nretention = kpi.retention_rate(\n    customers_start=1000,\n    customers_end=950,\n    new_customers=50\n)\n</code></pre>"},{"location":"user-guide/kpi/#customer-lifetime-value","title":"Customer Lifetime Value","text":"<pre><code>clv = kpi.customer_lifetime_value(\n    avg_revenue_per_customer=100,  # Monthly revenue per customer\n    gross_margin=70,              # Gross margin percentage\n    churn_rate_value=5,          # Monthly churn rate\n    discount_rate=10             # Annual discount rate\n)\n</code></pre>"},{"location":"user-guide/kpi/#financial-metrics","title":"Financial Metrics","text":""},{"location":"user-guide/kpi/#revenue-metrics","title":"Revenue Metrics","text":"<pre><code># Calculate MRR\nmrr = kpi.monthly_recurring_revenue(\n    paying_customers=1000,\n    avg_revenue_per_customer=50\n)\n\n# Calculate ARR\narr = kpi.annual_recurring_revenue(\n    paying_customers=1000,\n    avg_revenue_per_customer=50\n)\n</code></pre>"},{"location":"user-guide/kpi/#cost-metrics","title":"Cost Metrics","text":"<pre><code># Calculate CAC\ncac = kpi.customer_acquisition_cost(\n    marketing_costs=50000,\n    sales_costs=30000,\n    new_customers=100\n)\n\n# Calculate ROI\nroi = kpi.roi(\n    revenue=150000,\n    costs=100000\n)\n</code></pre>"},{"location":"user-guide/kpi/#engagement-metrics","title":"Engagement Metrics","text":""},{"location":"user-guide/kpi/#net-promoter-score","title":"Net Promoter Score","text":"<pre><code>nps = kpi.net_promoter_score(\n    promoters=70,        # Customers rating 9-10\n    detractors=10,       # Customers rating 0-6\n    total_respondents=100\n)\n</code></pre>"},{"location":"user-guide/kpi/#customer-satisfaction","title":"Customer Satisfaction","text":"<pre><code># Calculate CSAT\ncsat = kpi.customer_satisfaction_score(\n    satisfaction_ratings=[4, 5, 3, 5, 4],\n    max_rating=5\n)\n\n# Calculate Customer Effort Score\nces = kpi.customer_effort_score(\n    effort_ratings=[2, 3, 1, 2, 4],\n    max_rating=7\n)\n</code></pre>"},{"location":"user-guide/kpi/#health-assessment","title":"Health Assessment","text":"<p>The <code>health</code> property provides a comprehensive assessment of business health based on all tracked metrics:</p> <pre><code>health = kpi.health\n\n# Health assessment structure\n{\n    'overall_score': 85.5,\n    'status': 'Good',\n    'components': {\n        'churn_rate': {\n            'score': 90.0,\n            'status': 'Excellent'\n        },\n        'retention_rate': {\n            'score': 85.0,\n            'status': 'Good'\n        },\n        # ... other metrics\n    }\n}\n</code></pre>"},{"location":"user-guide/kpi/#health-score-components","title":"Health Score Components","text":"<p>The health score is calculated based on weighted components:</p> <ul> <li>Customer Health (30%)</li> <li>Churn Rate</li> <li>Retention Rate</li> <li> <p>LTV/CAC Ratio</p> </li> <li> <p>Financial Health (30%)</p> </li> <li>Gross Margin</li> <li>ROI</li> <li> <p>Revenue Growth</p> </li> <li> <p>Engagement Health (40%)</p> </li> <li>NPS</li> <li>CSAT</li> <li>Feature Adoption</li> </ul> <p>Each component is scored from 0-100 and assigned a status: - Excellent: \u2265 90 - Good: \u2265 75 - Fair: \u2265 60 - Poor: \u2265 45 - Critical: &lt; 45</p>"},{"location":"user-guide/kpi/#state-management","title":"State Management","text":"<p>The KPI class maintains state for all calculated metrics in the <code>_state</code> dictionary. This allows for: - Trend analysis - Health assessment - Historical comparison - Metric correlation</p> <pre><code># Access stored metrics\nstored_churn = kpi._state['churn_rate']\nstored_retention = kpi._state['retention_rate']\n</code></pre>"},{"location":"user-guide/kpi/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/kpi/#1-data-collection-and-management","title":"1. Data Collection and Management","text":"<ul> <li>1.1. Initialize Early: Create the KPI instance at the start of your analysis</li> <li>1.2. Regular Updates: Update metrics consistently for accurate trending</li> <li>1.3. Store History: Consider saving state for long-term analysis</li> </ul>"},{"location":"user-guide/kpi/#2-analysis-and-monitoring","title":"2. Analysis and Monitoring","text":"<ul> <li>2.1. Monitor Health: Regularly check the health assessment</li> <li>2.2. Validate Inputs: Ensure input data quality for accurate metrics</li> <li>2.3. Compare Trends: Analyze metric changes over time rather than isolated values</li> </ul>"},{"location":"user-guide/kpi/#3-reporting-and-decision-making","title":"3. Reporting and Decision Making","text":"<ul> <li>3.1. Focus on Key Metrics: Prioritize metrics most relevant to your business model</li> <li>3.2. Set Thresholds: Establish alert thresholds for critical metrics</li> <li>3.3. Contextualize Results: Consider market conditions when interpreting metrics ``` </li> </ul>"},{"location":"user-guide/moving-averages/","title":"Moving Averages","text":"<p>Pypulate provides a comprehensive set of moving average functions for financial time series analysis. This  page explains the different types of moving averages available and how to use them.</p>"},{"location":"user-guide/moving-averages/#available-moving-averages","title":"Available Moving Averages","text":""},{"location":"user-guide/moving-averages/#specialized-moving-averages","title":"Specialized Moving Averages","text":"<p>These moving averages are designed for specific use cases:</p> <ul> <li>Volume-Weighted Moving Average (VWMA): Weights price by volume.</li> <li>Kaufman Adaptive Moving Average (KAMA): Adapts to market volatility.</li> <li>Arnaud Legoux Moving Average (ALMA): Reduces lag and noise.</li> <li>Fractal Adaptive Moving Average (FRAMA): Adapts to market fractal dimension.</li> <li>Jurik Moving Average (JMA): Reduces noise and lag.</li> <li>Laguerre Filter: Uses Laguerre polynomials for smoothing.</li> <li>Least Squares Moving Average (LSMA): Uses linear regression.</li> <li>McGinley Dynamic Indicator: Adapts to market speed.</li> <li>Modular Filter: Adjusts smoothing based on phase.</li> <li>Rex Dog Moving Average (RDMA): Average of six SMAs with different periods.</li> <li>Tillson T3: Triple EMA with reduced lag.</li> <li>Volatility-Adjusted Moving Average (VAMA): Adjusts based on volatility.</li> </ul>"},{"location":"user-guide/moving-averages/#using-moving-averages","title":"Using Moving Averages","text":""},{"location":"user-guide/moving-averages/#functional-approach","title":"Functional Approach","text":"<p>You can use moving averages directly by importing the functions:</p> <pre><code>import numpy as np\nfrom pypulate.moving_averages import sma, ema, hma\n\n# Create sample data\ndata = [10, 11, 12, 11, 10, 9, 10, 11, 12, 13]\n\n# Calculate moving averages\nsma_result = sma(data, period=3)\nema_result = ema(data, period=3)\nhma_result = hma(data, period=3)\n</code></pre>"},{"location":"user-guide/moving-averages/#method-chaining-with-parray","title":"Method Chaining with Parray","text":"<p>For a more fluent interface, you can use the <code>Parray</code> class:</p> <pre><code>from pypulate import Parray\n\n# Create sample data\ndata = Parray([10, 11, 12, 11, 10, 9, 10, 11, 12, 13])\n\n# Calculate moving averages using method chaining\nsma_result = Parray.sma(period=3)\nema_result = Parray.ema(period=3)\nhma_result = Parray.hma(period=3)\n</code></pre>"},{"location":"user-guide/moving-averages/#examples","title":"Examples","text":""},{"location":"user-guide/moving-averages/#comparing-different-moving-averages","title":"Comparing Different Moving Averages","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom pypulate import Parray\n\n# Generate sample price data\nnp.random.seed(42)\ndays = 100\nprice = np.cumsum(np.random.normal(0, 1, days)) + 100\n\n# Convert to Parray for method chaining\nprice_array = Parray(price)\n\n# Calculate different types of moving averages\nsma = price_array.sma(20)\nema = price_array.ema(20)\nwma = price_array.wma(20)\nhma = price_array.hma(20)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.plot(price, label='Price', alpha=0.5, color='gray')\nplt.plot(sma, label='SMA(20)')\nplt.plot(ema, label='EMA(20)')\nplt.plot(wma, label='WMA(20)')\nplt.plot(hma, label='HMA(20)')\n\nplt.title('Comparison of Different Moving Averages')\nplt.xlabel('Days')\nplt.ylabel('Price')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"user-guide/moving-averages/#moving-average-crossover-strategy","title":"Moving Average Crossover Strategy","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom pypulate import Parray\n\n# Generate sample price data\nnp.random.seed(42)\ndays = 200\nprice = np.cumsum(np.random.normal(0, 1, days)) + 100\n\n# Convert to Parray for method chaining\nprice_array = Parray(price)\n\n# Calculate fast and slow EMAs\nfast_ema = price_array.ema(9)\nslow_ema = price_array.ema(21)\n\n# Generate buy/sell signals\nbuy_signals = fast_ema.crossover(slow_ema)\nsell_signals = fast_ema.crossunder(slow_ema)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.plot(price, label='Price')\nplt.plot(fast_ema, label='9-day EMA', alpha=0.7)\nplt.plot(slow_ema, label='21-day EMA', alpha=0.7)\n\n# Plot buy signals\nbuy_indices = np.where(buy_signals)[0]\nplt.scatter(buy_indices, price[buy_indices], marker='^', color='green', s=100, label='Buy Signal')\n\n# Plot sell signals\nsell_indices = np.where(sell_signals)[0]\nplt.scatter(sell_indices, price[sell_indices], marker='v', color='red', s=100, label='Sell Signal')\n\nplt.title('Moving Average Crossover Strategy')\nplt.xlabel('Days')\nplt.ylabel('Price')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"user-guide/moving-averages/#choosing-the-right-moving-average","title":"Choosing the Right Moving Average","text":"<p>Different moving averages are suitable for different market conditions:</p> <ul> <li>Trending Markets: SMA, EMA, WMA, HMA</li> <li>Volatile Markets: KAMA, ALMA, FRAMA, JMA</li> <li>Ranging Markets: DEMA, TEMA, T3</li> </ul> <p>Experiment with different types to find the one that works best for your specific use case. </p>"},{"location":"user-guide/parray/","title":"Parray - Enhanced NumPy Arrays for Financial Analysis","text":"<p>The <code>Parray</code> class is a powerful extension of NumPy arrays specifically designed for financial time series analysis. It provides a comprehensive set of methods for technical analysis, statistical operations, and performance optimization.</p>"},{"location":"user-guide/parray/#introduction","title":"Introduction","text":"<p><code>Parray</code> inherits from <code>numpy.ndarray</code>, so it has all the functionality of NumPy arrays plus a rich ecosystem of financial analysis methods. Its key advantages include:</p> <ul> <li>Method chaining for concise, readable code</li> <li>GPU acceleration for improved performance</li> <li>Parallel processing for handling large datasets</li> <li>Comprehensive set of financial indicators and statistical tools</li> <li>Memory optimization for large datasets</li> </ul>"},{"location":"user-guide/parray/#creating-a-parray","title":"Creating a Parray","text":"<pre><code>from pypulate.dtypes import Parray\nimport numpy as np\n\n# From a list\ndata = [1, 2, 3, 4, 5]\np = Parray(data)\n\n# From a NumPy array\ndata = np.array([1, 2, 3, 4, 5])\np = Parray(data)\n\n# From a 2D array (e.g., multiple time series)\ndata_2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\np_2d = Parray(data_2d)\n\n# With memory optimization\np_optimized = Parray(data, memory_optimized=True)\n</code></pre>"},{"location":"user-guide/parray/#performance-optimization","title":"Performance Optimization","text":"<p>Performance Optimization</p> <p>Most Parray methods are already highly optimized using efficient NumPy operations and vectorization. You typically don't need to enable additional optimization features unless you're working with very large datasets or have specific performance requirements.</p>"},{"location":"user-guide/parray/#gpu-acceleration","title":"GPU Acceleration","text":"<pre><code># Check if GPU is available\nif Parray.is_gpu_available():\n    # Get GPU info\n    gpu_info = Parray.get_gpu_info()\n    print(f\"GPU devices: {len(gpu_info['devices'])}\")\n\n    # Create a Parray with GPU acceleration\n    p = Parray([1, 2, 3, 4, 5])\n    p.enable_gpu()\n\n    # Perform calculations on GPU\n    result = p.standardize()\n\n    # Disable GPU when no longer needed\n    p.disable_gpu()\n</code></pre>"},{"location":"user-guide/parray/#parallel-processing","title":"Parallel Processing","text":"<pre><code># Enable parallel processing with default settings\np = Parray(np.random.random(1000000))\np.enable_parallel()\n\n# Enable with custom settings\np.enable_parallel(num_workers=8, chunk_size=100000)\n\n# Perform calculation in parallel\nresult = p.standardize()\n\n# Disable parallel processing\np.disable_parallel()\n</code></pre>"},{"location":"user-guide/parray/#memory-optimization","title":"Memory Optimization","text":"<pre><code># Create a memory-optimized Parray\np = Parray(np.random.random(1000000), memory_optimized=True)\n\n# Or optimize an existing Parray\np = Parray(np.random.random(1000000))\np.optimize_memory()\n\n# Process large datasets in chunks\nchunks = p.to_chunks(chunk_size=100000)\nresults = [chunk.normalize() for chunk in chunks]\nfinal_result = Parray.from_chunks(results)\n\n# Disable memory optimization\np.disable_memory_optimization()\n</code></pre>"},{"location":"user-guide/parray/#moving-averages","title":"Moving Averages","text":"<pre><code># Sample price data\nprices = Parray([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 19, 18, 17, 16, 17, 18, 19, 20, 21])\n\n# Simple Moving Average\nsma = prices.sma(period=5)\n\n# Exponential Moving Average\nema = prices.ema(period=5)\n\n# Weighted Moving Average\nwma = prices.wma(period=5)\n\n# Triple Exponential Moving Average (T3)\nt3 = prices.t3(period=5, vfactor=0.7)\n\n# Hull Moving Average\nhma = prices.hma(period=5)\n\n# Triangular Moving Average\ntma = prices.tma(period=5)\n\n# Smoothed Moving Average\nsmma = prices.smma(period=5)\n\n# Zero-Lag Moving Average\nzlma = prices.zlma(period=5)\n\n# Kaufman Adaptive Moving Average\nkama = prices.kama(period=5, fast_period=2, slow_period=30)\n\n# Fractal Adaptive Moving Average\nframa = prices.frama(period=5)\n\n# McGinley Dynamic\nmd = prices.mcginley_dynamic(period=5, k=0.6)\n</code></pre>"},{"location":"user-guide/parray/#momentum-indicators","title":"Momentum Indicators","text":"<pre><code># RSI (Relative Strength Index)\nrsi = prices.rsi(period=14)\n\n# Momentum\nmomentum = prices.momentum(period=14)\n\n# Rate of Change\nroc = prices.roc(period=14)\n\n# MACD (Moving Average Convergence Divergence)\n# Note: Your data must be longer than the slow_period parameter\n# Create sufficiently long data for MACD calculation\nlonger_prices = Parray(np.arange(1, 51))  # 50 data points\n# Or use smaller periods for smaller datasets\nmacd_line, signal_line, histogram = prices.macd(fast_period=5, slow_period=10, signal_period=3)  # For smaller datasets\n# Default parameters (requires at least 26 data points)\nmacd_line, signal_line, histogram = longer_prices.macd(fast_period=12, slow_period=26, signal_period=9)\n\n# Stochastic Oscillator (requires high and low data)\nhigh = Parray([11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 20, 19, 18, 17, 18, 19, 20, 21, 22])\nlow = Parray([9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 18, 17, 16, 15, 16, 17, 18, 19, 20])\nk, d = prices.stochastic_oscillator(high, low, k_period=14, d_period=3)\n\n# True Strength Index\n# Note: The data length must be greater than the long_period parameter (25 by default)\n# For smaller datasets, use smaller parameters\ntsi_line, signal_line = prices.tsi(long_period=5, short_period=3, signal_period=2)  # Works with smaller data\n# With default parameters (requires at least 25 data points)\ntsi_line, signal_line = longer_prices.tsi(long_period=25, short_period=13, signal_period=7)\n\n# Williams %R (requires high and low data)\nwilliams_r = prices.williams_r(high, low, period=14)\n\n# Commodity Channel Index (CCI)\ncci = prices.cci(period=20, constant=0.015)\n\n# Average Directional Index (ADX)\nadx = prices.adx(period=14)\n</code></pre>"},{"location":"user-guide/parray/#volatility-indicators","title":"Volatility Indicators","text":"<pre><code># Bollinger Bands\nupper, middle, lower = prices.bollinger_bands(period=20, std_dev=2.0)\n\n# Average True Range (ATR)\natr = prices.atr(high, low, period=14)\n\n# Keltner Channels\nk_upper, k_middle, k_lower = prices.keltner_channels(high=high, low=low, period=20, atr_period=10, multiplier=2.0)\n\n# Donchian Channels\nd_upper, d_middle, d_lower = prices.donchian_channels(high=high, low=low, period=20)\n\n# Historical Volatility\nhv = prices.historical_volatility(period=21, annualization_factor=252)\n\n# Volatility Ratio\nvr = prices.volatility_ratio(period=21, smooth_period=5)\n</code></pre>"},{"location":"user-guide/parray/#statistical-functions","title":"Statistical Functions","text":"<pre><code># Rolling Maximum\nroll_max = prices.rolling_max(period=14)\n\n# Rolling Minimum\nroll_min = prices.rolling_min(period=14)\n\n# Rolling Standard Deviation\nroll_std = prices.rolling_std(period=14)\n\n# Rolling Variance\nroll_var = prices.rolling_var(period=14)\n\n# Z-Score\nzscore = prices.zscore(period=14)\n\n# Descriptive Statistics\nstats = prices.descriptive_stats()\nprint(f\"Mean: {stats['mean']}, Std: {stats['std']}, Skewness: {stats['skewness']}\")\n\n# Autocorrelation\nacf = prices.autocorrelation(max_lag=20)\n\n# Partial Autocorrelation\npacf = prices.partial_autocorrelation(max_lag=20)\n\n# Stationarity Tests\njb_stat, jb_pvalue = prices.jarque_bera_test()\nadf_stat, adf_pvalue = prices.augmented_dickey_fuller_test()\nkpss_stat, kpss_pvalue = prices.kpss_test()\n\n# Correlation Matrix (requires 2D data)\nmulti_series = Parray(np.random.random((100, 3)))  # 3 series, 100 observations each\ncorr_matrix = multi_series.correlation_matrix(method='pearson')\n\n# Covariance Matrix\ncov_matrix = multi_series.covariance_matrix(ddof=1)\n</code></pre>"},{"location":"user-guide/parray/#crossovers","title":"crossovers","text":"<pre><code># Crossover and Crossunder Detection\nfast_ma = prices.ema(5)\nslow_ma = prices.ema(20)\n\n# Detect when fast MA crosses above slow MA (buy signal)\nbuy_signals = fast_ma.crossover(slow_ma)\n\n# Detect when fast MA crosses below slow MA (sell signal)\nsell_signals = fast_ma.crossunder(slow_ma)\n</code></pre>"},{"location":"user-guide/parray/#filters-and-smoothing","title":"Filters and Smoothing","text":"<pre><code># Kalman Filter\nkf = prices.kalman_filter(process_variance=1e-5, measurement_variance=1e-3)\n\n# Adaptive Kalman Filter\nakf = prices.adaptive_kalman_filter(process_variance_init=1e-5, measurement_variance_init=1e-3, \n                                 adaptation_rate=0.01, window_size=10)\n\n# Butterworth Filter\n# Note: The data length must be greater than 3*(filter order) + 1\n# Higher order filters require more data points\n# Use lower order for small datasets\nbf_low = prices.butterworth_filter(cutoff=0.1, order=2, filter_type='lowpass')  # Order 2 requires at least 7 points\n\n# For higher order filters, use longer data\n# Order 4 requires at least 13 data points, order 6 requires 19 points, etc.\nbf_high = longer_prices.butterworth_filter(cutoff=0.1, order=4, filter_type='highpass')\nbf_band = longer_prices.butterworth_filter(cutoff=(0.1, 0.4), order=4, filter_type='bandpass')\n\n# Savitzky-Golay Filter\nsg = prices.savitzky_golay_filter(window_length=11, polyorder=3)\n\n# Hampel Filter (for outlier removal)\nhf = prices.hampel_filter(window_size=5, n_sigmas=3.0)\n\n# Hodrick-Prescott Filter\ntrend, cycle = prices.hodrick_prescott_filter(lambda_param=1600.0)\n</code></pre>"},{"location":"user-guide/parray/#data-transformations","title":"Data Transformations","text":"<pre><code># Normalize\nnormalized = prices.normalize(method='l2')\nl1_norm = prices.normalize_l1()\nl2_norm = prices.normalize_l2()\n\n# Standardize\nstandardized = prices.standardize()\n\n# Min-Max Scale\nminmax = prices.min_max_scale(feature_range=(0, 1))\n\n# Robust Scale\nrobust = prices.robust_scale(method='iqr', quantile_range=(25.0, 75.0))\n\n# Quantile Transform\nquantile = prices.quantile_transform(n_quantiles=1000, output_distribution='uniform')\n\n# Winsorize\nwinsorized = prices.winsorize(limits=0.05)\n\n# Remove Outliers\nclean = prices.remove_outliers(method='zscore', threshold=3.0)\n\n# Fill Missing Values\nfilled = prices.fill_missing(method='mean')\n\n# Interpolate Missing Values\ninterpolated = prices.interpolate_missing(method='linear')\n\n# Log Transform\n# Note: Use offset for zero/negative values\nlog_data = prices.log_transform(base=None, offset=1.0 if prices.min() &lt;= 0 else 0.0)\n\n# Power Transform\npower_transformed = prices.power_transform(method='yeo-johnson')\n\n# Scale to Range\nscaled = prices.scale_to_range(feature_range=(0.0, 1.0))\n\n# Clip Outliers\nclipped = prices.clip_outliers(lower_percentile=1.0, upper_percentile=99.0)\n\n# Discretize\ndiscretized = prices.discretize(n_bins=5, strategy='uniform')\n\n# Polynomial Features\npoly = prices.polynomial_features(degree=2)\n\n# Resample\nresampled = prices.resample(factor=2, method='mean')\n\n# Dynamic Tanh\ntanh_transformed = prices.dynamic_tanh(alpha=1.0)\n</code></pre>"},{"location":"user-guide/parray/#time-series-operations","title":"Time Series Operations","text":"<pre><code># Create Lag Features\nlagged = prices.lag_features(lags=[1, 2, 3])\n\n# Calculate Slope\nslope = prices.slope(period=5)\n\n# Wave Function\nwave = prices.wave(high=high, low=low, close=prices)\n\n# ZigZag\nzigzag = prices.zigzag(threshold=0.03)\n\n# Rolling Window Statistics\nroll_stats = prices.rolling_statistics(window=10, statistics=['mean', 'std', 'skew', 'kurt'])\nroll_mean = roll_stats['mean']\nroll_std = roll_stats['std']\n</code></pre>"},{"location":"user-guide/parray/#advanced-usage","title":"Advanced Usage","text":""},{"location":"user-guide/parray/#method-chaining","title":"Method Chaining","text":"<p>One of the most powerful features of Parray is the ability to chain methods:</p> <pre><code># Complex analysis in a single chain\nresult = (\n    prices\n    .remove_outliers(method='zscore', threshold=3.0)\n    .fill_missing(method='mean')\n    .log_transform(offset=1.0 if prices.min() &lt;= 0 else 0.0)\n    .ema(period=5)\n    .standardize()\n)\n\n# Creating a custom indicator\ncustom_indicator = (\n    (prices.ema(5) - prices.ema(20)) /  # Fast MA - Slow MA\n    prices.atr(high, low, 14)            # Normalized by ATR\n)\n</code></pre>"},{"location":"user-guide/parray/#custom-functions-with-apply-and-apply_along_axis","title":"Custom Functions with apply and apply_along_axis","text":"<pre><code># Apply a custom function to each element\n# Note: When using apply(), the function must handle entire arrays at once\ndef custom_func_array(x):\n    # Use vectorized operations instead of if-else\n    result = np.empty_like(x, dtype=float)\n    positive_mask = x &gt; 0\n    result[positive_mask] = np.sin(x[positive_mask])\n    result[~positive_mask] = np.cos(x[~positive_mask])\n    return result\n\nresult = prices.apply(custom_func_array)\n\n# Alternative: Use numpy's vectorize to convert element-wise function to array function\n@np.vectorize\ndef custom_func_element(x):\n    # This operates on single elements\n    return np.sin(x) if x &gt; 0 else np.cos(x)\n\nresult = prices.apply(lambda x: custom_func_element(x))\n\n# Apply a function along an axis (for 2D arrays)\ndef row_func(row):\n    return np.sum(row) / np.max(row)\n\nresult_2d = multi_series.apply_along_axis(row_func, axis=1)\n\n# Rolling apply\ndef roll_func(window):\n    return np.max(window) - np.min(window)\n\nroll_range = prices.rolling_apply(window=5, func=roll_func)\n</code></pre>"},{"location":"user-guide/parray/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/parray/#performance-optimization_1","title":"Performance Optimization","text":"<ol> <li> <p>Enable GPU for large datasets:    <pre><code>if Parray.is_gpu_available():\n    prices.enable_gpu()\n</code></pre></p> </li> <li> <p>Use parallel processing for CPU-bound operations:    <pre><code>prices.enable_parallel(num_workers=4)\n</code></pre></p> </li> <li> <p>Process very large datasets in chunks:    <pre><code>chunks = prices.to_chunks(chunk_size=100000)\nresults = [process_chunk(chunk) for chunk in chunks]\nfinal = Parray.from_chunks(results)\n</code></pre></p> </li> <li> <p>Use memory optimization for memory-intensive operations:    <pre><code>prices.optimize_memory()\n</code></pre></p> </li> </ol>"},{"location":"user-guide/parray/#technical-analysis","title":"Technical Analysis","text":"<ol> <li> <p>Combine multiple indicators for confirmation:    <pre><code>buy_signal = (\n    prices.crossover(prices.sma(20)) &amp;  # Price crosses above MA\n    (prices.rsi(14) &lt; 70) &amp;            # RSI not overbought\n    (prices.adx(14) &gt; 25)              # Strong trend\n)\n</code></pre></p> </li> <li> <p>Use proper normalization for comparing instruments:    <pre><code># Compare two instruments on the same scale\nstock1_norm = stock1_prices.standardize()\nstock2_norm = stock2_prices.standardize()\n</code></pre></p> </li> <li> <p>Consider time frame alignment:    <pre><code># Daily chart\ndaily_signal = daily_prices.crossover(daily_prices.sma(200))\n\n# Weekly chart confirmation\nweekly_signal = weekly_prices.rsi(14) &lt; 50\n</code></pre></p> </li> <li> <p>Maintain good data hygiene:    <pre><code># Clean data before analysis\nclean_prices = (\n    prices\n    .remove_outliers()\n    .fill_missing(method='forward')\n    .interpolate_missing(method='linear')\n)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/parray/#conclusion","title":"Conclusion","text":"<p>The <code>Parray</code> class offers a powerful, flexible approach to financial time series analysis with NumPy's performance and a rich set of financial indicators and statistical tools. By leveraging method chaining, GPU acceleration, and parallel processing, you can perform complex analyses with clear, concise code. </p>"},{"location":"user-guide/portfolio/","title":"Portfolio Guide","text":"<p>The <code>Portfolio</code> class provides a comprehensive suite of methods for portfolio analysis, risk management, and performance attribution.</p>"},{"location":"user-guide/portfolio/#basic-usage","title":"Basic Usage","text":"<pre><code>from pypulate import Portfolio\n\n# Initialize portfolio analyzer\nportfolio = Portfolio()\n\n# Calculate basic returns\nsimple_ret = portfolio.simple_return(10, 12)\nlog_ret = portfolio.log_return(10, 12)\n</code></pre>"},{"location":"user-guide/portfolio/#return-metrics","title":"Return Metrics","text":""},{"location":"user-guide/portfolio/#simple-returns","title":"Simple Returns","text":"<pre><code># Calculate simple returns\nreturns = portfolio.simple_return([10, 20, 25], [12, 21, 20])\n\n# Calculate holding period return\nhpr = portfolio.holding_period_return(\n    [10, 20, 25]\n)\n\n# Calculate annualized return\nannual_ret = portfolio.annualized_return(\n    [10, 20, 25],\n    years=2\n)\n</code></pre>"},{"location":"user-guide/portfolio/#time-weighted-returns","title":"Time-Weighted Returns","text":"<pre><code># Calculate time-weighted return\ntwrr = portfolio.time_weighted_return(\n    [0.01, 0.03, 0.02, 0.02, 0.001],\n)\n</code></pre>"},{"location":"user-guide/portfolio/#money-weighted-returns","title":"Money-Weighted Returns","text":"<pre><code># Calculate money-weighted return (IRR)\nmwrr = portfolio.money_weighted_return([-1000, -500, 1700], [0, 0.5, 1], 0)\n</code></pre>"},{"location":"user-guide/portfolio/#risk-metrics","title":"Risk Metrics","text":""},{"location":"user-guide/portfolio/#volatility-measures","title":"Volatility Measures","text":"<pre><code># Calculate standard deviation\nstd_dev = portfolio.standard_deviation([0.01, 0.03, 0.02, 0.02, 0.001])\n</code></pre>"},{"location":"user-guide/portfolio/#value-at-risk","title":"Value at Risk","text":"<pre><code># Calculate parametric VaR\nvar = portfolio.value_at_risk(\n    [0.01, 0.03, 0.02, 0.02, 0.001],\n    confidence_level=0.95,\n    method = 'monte_carlo'\n)\n\n# Calculate conditional VaR (Expected Shortfall)\ncvar = portfolio.conditional_value_at_risk(\n    returns,\n    confidence_level=0.95\n)\n</code></pre>"},{"location":"user-guide/portfolio/#drawdown-analysis","title":"Drawdown Analysis","text":"<pre><code># Calculate maximum drawdown\nmax_dd = portfolio.max_drawdown(prices)\n\n# Get drawdown details\ndd_amount, dd_percent, dd_length = portfolio.drawdown_details(prices)\n</code></pre>"},{"location":"user-guide/portfolio/#risk-adjusted-performance","title":"Risk-Adjusted Performance","text":""},{"location":"user-guide/portfolio/#sharpe-ratio","title":"Sharpe Ratio","text":"<pre><code># Calculate Sharpe ratio\nsharpe = portfolio.sharpe_ratio(\n    returns,\n    risk_free_rate=0.02,\n    periods_per_year=252\n)\n</code></pre>"},{"location":"user-guide/portfolio/#information-ratio","title":"Information Ratio","text":"<pre><code># Calculate Information ratio\ninfo_ratio = portfolio.information_ratio(\n    returns,\n    benchmark_returns,\n    periods_per_year=252\n)\n</code></pre>"},{"location":"user-guide/portfolio/#capm-metrics","title":"CAPM Metrics","text":"<pre><code># Calculate beta\nbeta = portfolio.beta(returns, market_returns)\n\n# Calculate alpha\nalpha = portfolio.alpha(\n    returns,\n    market_returns,\n    risk_free_rate=0.02\n)\n</code></pre>"},{"location":"user-guide/portfolio/#health-assessment","title":"Health Assessment","text":"<p>The <code>health</code> property provides a comprehensive assessment of portfolio health:</p> <pre><code>health = portfolio.health\n\n# Health assessment structure\n{\n    'overall_score': 82.5,\n    'status': 'Good',\n    'components': {\n        'returns': {\n            'score': 85.0,\n            'status': 'Good'\n        },\n        'risk': {\n            'score': 78.0,\n            'status': 'Good'\n        },\n        'risk_adjusted': {\n            'score': 88.0,\n            'status': 'Good'\n        }\n    }\n}\n</code></pre>"},{"location":"user-guide/portfolio/#health-score-components","title":"Health Score Components","text":"<p>The portfolio health score is calculated based on three main components:</p> <ul> <li>Returns (30%)</li> <li>Absolute Returns</li> <li>Relative Returns</li> <li> <p>Consistency of Returns</p> </li> <li> <p>Risk Metrics (40%)</p> </li> <li>Volatility</li> <li>Value at Risk</li> <li>Maximum Drawdown</li> <li> <p>Recovery Time</p> </li> <li> <p>Risk-Adjusted Performance (30%)</p> </li> <li>Sharpe Ratio</li> <li>Information Ratio</li> <li>Sortino Ratio</li> <li>Treynor Ratio</li> </ul> <p>Each component is scored from 0-100 and assigned a status: - Excellent: \u2265 90 - Good: \u2265 75 - Fair: \u2265 60 - Poor: \u2265 45 - Critical: &lt; 45</p>"},{"location":"user-guide/portfolio/#state-management","title":"State Management","text":"<p>The Portfolio class maintains state for calculated metrics in the <code>_state</code> dictionary:</p> <pre><code># Access stored metrics\nstored_returns = portfolio._state['returns']\nstored_volatility = portfolio._state['volatility']\nstored_sharpe = portfolio._state['sharpe_ratio']\n</code></pre>"},{"location":"user-guide/portfolio/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/portfolio/#1-data-management","title":"1. Data Management","text":"<ul> <li>1.1. Data Quality: Ensure price and return data is clean and properly formatted</li> <li>1.2. Time Consistency: Use consistent time periods for comparative analysis</li> <li>1.3. Adjustments: Account for dividends, splits, and corporate actions</li> </ul>"},{"location":"user-guide/portfolio/#2-risk-assessment","title":"2. Risk Assessment","text":"<ul> <li>2.1. Regular Monitoring: Regularly monitor risk metrics</li> <li>2.2. Multiple Metrics: Use multiple risk measures for comprehensive assessment</li> <li>2.3. Stress Testing: Conduct stress tests under various market scenarios</li> </ul>"},{"location":"user-guide/portfolio/#3-performance-analysis","title":"3. Performance Analysis","text":"<ul> <li>3.1. Benchmark Selection: Choose appropriate benchmarks for relative analysis</li> <li>3.2. Attribution: Analyze sources of returns and risk</li> <li>3.3. Health Monitoring: Regularly assess portfolio health</li> </ul>"},{"location":"user-guide/portfolio/#4-reporting-and-communication","title":"4. Reporting and Communication","text":"<ul> <li>4.1. Clear Visualization: Present portfolio metrics with clear visualizations</li> <li>4.2. Context Provision: Provide context for performance numbers</li> <li>4.3. Consistent Reporting: Maintain consistent reporting formats </li> </ul>"},{"location":"user-guide/preprocessing/","title":"Data Preprocessing","text":"<p>The preprocessing module provides a comprehensive set of tools for data preprocessing and statistical analysis, designed specifically for financial data.</p>"},{"location":"user-guide/preprocessing/#getting-started-with-parray","title":"Getting Started with Parray","text":"<p>First, let's create a Parray object from your data:</p> <pre><code>from pypulate.dtypes import Parray\n\n# Create a Parray from numpy array\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nts = Parray(data)\n\n# Enable GPU acceleration (if available)\nts.enable_gpu()\n\n# Enable parallel processing\nts.enable_parallel(num_workers=4)\n</code></pre>"},{"location":"user-guide/preprocessing/#data-normalization-and-scaling","title":"Data Normalization and Scaling","text":""},{"location":"user-guide/preprocessing/#normalize","title":"Normalize","text":"<pre><code># L2 normalization (default)\nnormalized_data = ts.normalize(method='l2')\n\n# L1 normalization\nnormalized_data = ts.normalize(method='l1')\n\n# Using method chaining\nresult = ts.normalize_l2().standardize()\n</code></pre>"},{"location":"user-guide/preprocessing/#standardize","title":"Standardize","text":"<pre><code># Z-score normalization\nstandardized_data = ts.standardize()\n\n# With parallel processing\nts.enable_parallel()\nstandardized_data = ts.standardize()\n</code></pre>"},{"location":"user-guide/preprocessing/#min-max-scaling","title":"Min-Max Scaling","text":"<pre><code># Scale to [0,1] range (default)\nscaled_data = ts.min_max_scale()\n\n# Scale to custom range\nscaled_data = ts.min_max_scale(feature_range=(-1, 1))\n</code></pre>"},{"location":"user-guide/preprocessing/#robust-scaling","title":"Robust Scaling","text":"<pre><code># IQR-based scaling (default)\nrobust_data = ts.robust_scale(method='iqr')\n\n# Custom quantile range\nrobust_data = ts.robust_scale(method='iqr', quantile_range=(10.0, 90.0))\n\n# Using MAD scaling\nrobust_data = ts.robust_scale(method='mad')\n</code></pre>"},{"location":"user-guide/preprocessing/#outlier-handling","title":"Outlier Handling","text":""},{"location":"user-guide/preprocessing/#remove-outliers","title":"Remove Outliers","text":"<pre><code># Z-score based removal\ncleaned_data = ts.remove_outliers(method='zscore', threshold=2.0)\n\n# IQR based removal\ncleaned_data = ts.remove_outliers(method='iqr')\n\n# Using method chaining\nresult = ts.remove_outliers().standardize()\n</code></pre>"},{"location":"user-guide/preprocessing/#winsorize","title":"Winsorize","text":"<pre><code># Symmetric winsorization at 5% (default)\nwinsorized_data = ts.winsorize(limits=0.05)\n\n# Asymmetric winsorization\nwinsorized_data = ts.winsorize(limits=(0.01, 0.05))\n</code></pre>"},{"location":"user-guide/preprocessing/#missing-value-handling","title":"Missing Value Handling","text":""},{"location":"user-guide/preprocessing/#fill-missing-values","title":"Fill Missing Values","text":"<pre><code># Mean imputation\nfilled_data = ts.fill_missing(method='mean')\n\n# Median imputation\nfilled_data = ts.fill_missing(method='median')\n\n# Custom value\nfilled_data = ts.fill_missing(method='constant', value=0.0)\n\n# Forward fill\nfilled_data = ts.fill_missing(method='forward')\n\n# Backward fill\nfilled_data = ts.fill_missing(method='backward')\n</code></pre>"},{"location":"user-guide/preprocessing/#interpolate-missing-values","title":"Interpolate Missing Values","text":"<pre><code># Linear interpolation\ninterpolated_data = ts.interpolate_missing(method='linear')\n\n# Cubic interpolation\ninterpolated_data = ts.interpolate_missing(method='cubic')\n\n# Quadratic interpolation\ninterpolated_data = ts.interpolate_missing(method='quadratic')\n</code></pre>"},{"location":"user-guide/preprocessing/#time-series-operations","title":"Time Series Operations","text":""},{"location":"user-guide/preprocessing/#resampling","title":"Resampling","text":"<pre><code># Downsample by factor of 2 using mean\nresampled_data = ts.resample(factor=2, method='mean')\n\n# Downsample using median\nresampled_data = ts.resample(factor=2, method='median')\n\n# Downsample using sum\nresampled_data = ts.resample(factor=2, method='sum')\n</code></pre>"},{"location":"user-guide/preprocessing/#rolling-window-operations","title":"Rolling Window Operations","text":"<pre><code># Create rolling windows of size 5\nwindows = ts.rolling_window(window_size=5)\n\n# Create rolling windows with step size 2\nwindows = ts.rolling_window(window_size=5, step=2)\n</code></pre>"},{"location":"user-guide/preprocessing/#lag-features","title":"Lag Features","text":"<pre><code># Create lag features for lags 1, 2, and 3\nlagged_data = ts.lag_features(lags=[1, 2, 3])\n</code></pre>"},{"location":"user-guide/preprocessing/#transformations","title":"Transformations","text":""},{"location":"user-guide/preprocessing/#log-transform","title":"Log Transform","text":"<pre><code># Natural log transform (only works with positive data)\nlog_data = ts.log_transform()\n\n# Log transform with custom base\nlog_data = ts.log_transform(base=10)\n\n# Log transform with offset for data containing zeros or negative values\n# Add an offset to make all values positive before taking log\nlog_data = ts.log_transform(offset=1.0)  # For data with zeros\nlog_data = ts.log_transform(offset=abs(ts.min()) + 1)  # For data with negative values\n</code></pre>"},{"location":"user-guide/preprocessing/#power-transform","title":"Power Transform","text":"<pre><code># Yeo-Johnson transform\ntransformed_data = ts.power_transform(method='yeo-johnson')\n\n# Box-Cox transform\ntransformed_data = ts.power_transform(method='box-cox')\n\n# Without standardization\ntransformed_data = ts.power_transform(standardize=False)\n</code></pre>"},{"location":"user-guide/preprocessing/#dynamic-tanh-transform","title":"Dynamic Tanh Transform","text":"<pre><code># Apply Dynamic Tanh transformation\ndyt_data = ts.dynamic_tanh(alpha=1.0)\n\n# More aggressive normalization\ndyt_data = ts.dynamic_tanh(alpha=2.0)\n</code></pre>"},{"location":"user-guide/preprocessing/#statistical-analysis","title":"Statistical Analysis","text":""},{"location":"user-guide/preprocessing/#descriptive-statistics","title":"Descriptive Statistics","text":"<pre><code># Calculate basic statistics\nstats = ts.descriptive_stats()\n\n# Access specific statistics\nmean = stats['mean']\nstd = stats['std']\nskewness = stats['skewness']\nkurtosis = stats['kurtosis']\n</code></pre>"},{"location":"user-guide/preprocessing/#correlation-analysis","title":"Correlation Analysis","text":"<pre><code># Create a 2D array with multiple variables\nfrom pypulate import Parray\n\n# Create a 2D Parray with 3 variables (columns)\nps = parray([\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9],\n    [10, 11, 12]\n])\n\n# Calculate Pearson correlation\ncorr_matrix = ps.correlation_matrix(method='pearson')\nprint(corr_matrix)  # Shows correlation between the 3 columns\n\n# Calculate Spearman correlation\ncorr_matrix = ps.correlation_matrix(method='spearman')\n\n# Calculate Kendall correlation\ncorr_matrix = ps.correlation_matrix(method='kendall')\n\n# For a single time series, you need to create features first\nps_single = Parray([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nlagged_ts = ps_single.lag_features(lags=[1, 2, 3])  # Creates a 2D array with original and lagged features\ncorr_matrix = lagged_ts.correlation_matrix(method='pearson')  # Correlation between original and lagged values\n</code></pre>"},{"location":"user-guide/preprocessing/#stationarity-tests","title":"Stationarity Tests","text":"<pre><code>import numpy as np\nfrom pypulate import Parray\n\n# Create a sample for stationarity tests\nnp.random.seed(42) # For reproducibility\nts = Parray(np.random.normal(0, 1, size=100))  # 100 observations\n\n# Perform ADF test\ntest_stat, p_value = ts.augmented_dickey_fuller_test()\n\n# Perform KPSS test\ntest_stat, p_value = ts.kpss_test()\n\n# Perform variance ratio test\n# Note: For each period k, you need at least 2k+1 observations\n# Important: variance_ratio_test requires strictly positive values (like stock prices)\n\n# Create data that simulates a price series (strictly positive)\nprice_data = np.cumprod(1 + np.random.normal(0.001, 0.01, size=100))  # Random walk with drift\nts_prices = Parray(price_data)\n\n# Use smaller periods for smaller samples\nresults_small = ts_prices.variance_ratio_test(periods=[2, 4, 8])\n\n# For larger periods, need larger samples\nlong_price_data = np.cumprod(1 + np.random.normal(0.001, 0.01, size=500))  # Random walk with drift\nlong_ts_prices = Parray(long_price_data)\nresults_large = long_ts_prices.variance_ratio_test(periods=[2, 4, 8, 16, 32])\n</code></pre>"},{"location":"user-guide/preprocessing/#rolling-statistics","title":"Rolling Statistics","text":"<pre><code># Calculate multiple rolling statistics\nstats = ts.rolling_statistics(window=20, statistics=['mean', 'std', 'skew', 'kurt'])\n\n# Access specific rolling statistics\nrolling_mean = stats['mean']\nrolling_std = stats['std']\n</code></pre>"},{"location":"user-guide/preprocessing/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/preprocessing/#gpu-acceleration","title":"GPU Acceleration","text":"<pre><code># Enable GPU acceleration\nts.enable_gpu()\n\n# Check GPU availability\nif Parray.is_gpu_available():\n    gpu_info = Parray.get_gpu_info()\n    print(f\"Using GPU: {gpu_info['devices'][0]['name']}\")\n</code></pre>"},{"location":"user-guide/preprocessing/#parallel-processing","title":"Parallel Processing","text":"<pre><code># Enable parallel processing\nts.enable_parallel(num_workers=4)\n\n# Custom chunk size\nts.enable_parallel(num_workers=4, chunk_size=10000)\n</code></pre>"},{"location":"user-guide/preprocessing/#memory-optimization","title":"Memory Optimization","text":"<pre><code># Optimize memory usage\nts.optimize_memory()\n\n# Process large datasets in chunks\nchunks = ts.to_chunks(chunk_size=10000)\nresults = [chunk.process() for chunk in chunks]\nfinal_result = Parray.from_chunks(results)\n</code></pre>"},{"location":"user-guide/preprocessing/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Data Validation: Always check your data for missing values and outliers before applying transformations.</p> </li> <li> <p>Scaling Order: When applying multiple transformations, consider the order:</p> </li> <li>Handle missing values first</li> <li>Remove outliers</li> <li>Apply transformations (log, power)</li> <li> <p>Scale the data</p> </li> <li> <p>Time Series Considerations: For time series data:</p> </li> <li>Check for stationarity</li> <li>Consider using appropriate lag features</li> <li> <p>Be careful with interpolation methods</p> </li> <li> <p>Memory Efficiency: The module is designed to be memory efficient, but for large datasets:</p> </li> <li>Use appropriate window sizes</li> <li>Consider processing in chunks</li> <li> <p>Monitor memory usage with large transformations</p> </li> <li> <p>Performance Optimization:</p> </li> <li>Enable GPU acceleration when available</li> <li>Use parallel processing for large datasets</li> <li>Optimize memory usage when working with large arrays</li> <li>Consider using method chaining for better readability</li> </ol>"},{"location":"user-guide/preprocessing/#performance-tips","title":"Performance Tips","text":"<ol> <li> <p>Vectorized Operations: All functions are vectorized for optimal performance.</p> </li> <li> <p>Memory Management: Functions return NumPy arrays, which are memory efficient.</p> </li> <li> <p>Missing Value Handling: Functions handle NaN values appropriately without raising errors.</p> </li> <li> <p>Type Safety: Functions use type hints and input validation for reliability.</p> </li> <li> <p>GPU Acceleration: Enable GPU acceleration for supported operations to improve performance.</p> </li> <li> <p>Parallel Processing: Use parallel processing for large datasets and computationally intensive operations.</p> </li> <li> <p>Method Chaining: Take advantage of method chaining for cleaner and more efficient code:</p> </li> </ol> <pre><code># Example of method chaining\n# Note: log_transform requires positive values, so we add offset if needed\nresult = (ts\n    .remove_outliers()\n    .fill_missing(method='mean')\n    .standardize()\n    # Add sufficient offset for log transform if data might contain zero/negative values\n    .log_transform(offset=abs(ts.min()) + 1 if ts.min() &lt;= 0 else 0)\n    .rolling_statistics(window=20, statistics=['mean', 'std'])\n)\n</code></pre>"},{"location":"user-guide/service-pricing/","title":"Service Pricing","text":"<p>The <code>ServicePricing</code> class provides a unified interface for calculating various types of service pricing models. It supports tiered pricing, subscription-based pricing, usage-based pricing, dynamic pricing adjustments, volume discounts, and custom pricing rules.</p>"},{"location":"user-guide/service-pricing/#quick-start","title":"Quick Start","text":"<pre><code>from pypulate import ServicePricing\n\n# Initialize pricing calculator\npricing = ServicePricing()\n\n# Calculate tiered pricing\nprice = pricing.calculate_tiered_price(\n    usage_units=1500,\n    tiers={\n        \"0-1000\": 0.10,\n        \"1001-2000\": 0.08,\n        \"2001+\": 0.05\n    }\n)\nprint(f\"Total price: ${price:.2f}\")  # Output: Total price: $140.02\n</code></pre>"},{"location":"user-guide/service-pricing/#features","title":"Features","text":""},{"location":"user-guide/service-pricing/#tiered-pricing","title":"Tiered Pricing","text":"<p>Calculate prices based on usage tiers:</p> <pre><code>tiers = {\n    \"0-1000\": 0.10,    # $0.10 per unit for first 1000 units = $100\n    \"1001-2000\": 0.08, # $0.08 per unit for next 500 units = $40\n    \"2001+\": 0.05      # $0.05 per unit for 2001+ units\n}\n\n# Cumulative pricing (default)\nprice = pricing.calculate_tiered_price(1500, tiers)\n# Result: $140.02\n\n# Non-cumulative pricing\nprice = pricing.calculate_tiered_price(1500, tiers, cumulative=False)\n# Result: $120.00\n</code></pre>"},{"location":"user-guide/service-pricing/#subscription-pricing","title":"Subscription Pricing","text":"<p>Calculate subscription prices with features and discounts:</p> <pre><code>price = pricing.calculate_subscription_price(\n    base_price=99.99,\n    features=['premium', 'api_access'],\n    feature_prices={'premium': 49.99, 'api_access': 29.99},\n    duration_months=12,\n    discount_rate=0.10\n)\n</code></pre>"},{"location":"user-guide/service-pricing/#usage-based-pricing","title":"Usage-Based Pricing","text":"<p>Calculate prices based on multiple usage metrics:</p> <pre><code>usage_metrics = {'api_calls': 1000, 'storage_gb': 50}\nmetric_rates = {'api_calls': 0.001, 'storage_gb': 0.10}\nprice = pricing.calculate_usage_price(\n    usage_metrics,\n    metric_rates,\n    minimum_charge=10.0,\n    maximum_charge=1000.0\n)\n</code></pre>"},{"location":"user-guide/service-pricing/#volume-discounts","title":"Volume Discounts","text":"<p>Apply volume-based discounts:</p> <pre><code>discount_tiers = {\n    100: 0.05,   # 5% discount for 100+ units\n    500: 0.10,   # 10% discount for 500+ units\n    1000: 0.15   # 15% discount for 1000+ units\n}\nprice = pricing.calculate_volume_discount(\n    base_price=10.0,\n    volume=750,\n    discount_tiers=discount_tiers\n)\n</code></pre>"},{"location":"user-guide/service-pricing/#time-based-pricing","title":"Time-Based Pricing","text":"<p>Calculate prices based on time duration with different units and rounding options:</p> <pre><code>price = pricing.calculate_time_based_price(\n    base_price=25.0,      # $25 per hour\n    duration=2.5,         # 2.5 hours\n    time_unit='hour',     # pricing unit (minute, hour, day)\n    minimum_duration=1.0, # minimum billable duration\n    rounding_method='up'  # round up to nearest unit\n)\n# Result: $63.00 (25.0 * 2.5 = 62.5, rounded up to 63)\n\n# Using minutes as the time unit\nprice = pricing.calculate_time_based_price(\n    base_price=0.50,      # $0.50 per minute\n    duration=45,          # 45 minutes\n    time_unit='minute'\n)\n# Result: $23.00 (0.50 * 45 = 22.5, rounded up to 23)\n</code></pre>"},{"location":"user-guide/service-pricing/#freemium-pricing","title":"Freemium Pricing","text":"<p>Calculate prices for freemium models with base features (free up to limits) and premium features:</p> <pre><code>price = pricing.calculate_freemium_price(\n    base_features=['storage', 'api_calls', 'users'],\n    premium_features=['advanced_analytics', 'priority_support'],\n    feature_usage={\n        'storage': 150,           # GB\n        'api_calls': 12000,\n        'users': 25,\n        'advanced_analytics': 100,\n        'priority_support': 1\n    },\n    free_limits={\n        'storage': 100,           # 100 GB free\n        'api_calls': 10000,       # 10,000 calls free\n        'users': 20               # 20 users free\n    },\n    overage_rates={\n        'storage': 0.1,           # $0.1 per GB over limit\n        'api_calls': 0.001,       # $0.001 per call over limit\n        'users': 2.0,             # $2 per user over limit\n        'advanced_analytics': 0.05, # $0.05 per usage unit\n        'priority_support': 50.0   # $50 flat fee (usage=1)\n    }\n)\n</code></pre>"},{"location":"user-guide/service-pricing/#bundle-pricing","title":"Bundle Pricing","text":"<p>Calculate prices for bundled items with combination-specific discounts:</p> <pre><code>price = pricing.calculate_bundle_price(\n    items=['laptop', 'mouse', 'keyboard', 'monitor'],\n    item_prices={\n        'laptop': 1200.0,\n        'mouse': 25.0,\n        'keyboard': 50.0,\n        'monitor': 200.0\n    },\n    bundle_discounts={\n        'laptop+mouse': 0.05,                # 5% off laptop+mouse\n        'keyboard+mouse': 0.10,              # 10% off keyboard+mouse\n        'laptop+keyboard+mouse': 0.15,       # 15% off laptop+keyboard+mouse\n        'laptop+monitor+keyboard+mouse': 0.20 # 20% off complete setup\n    },\n    minimum_bundle_size=2  # minimum items for discount eligibility\n)\n# Result: $1180.00 (20% discount on the complete bundle)\n</code></pre>"},{"location":"user-guide/service-pricing/#peak-pricing","title":"Peak Pricing","text":"<p>Apply different rates based on peak and off-peak hours:</p> <pre><code>price = pricing.calculate_peak_pricing(\n    base_price=50.0,      # base price per unit\n    usage_time=\"14:30\",   # time of usage (2:30 PM)\n    peak_hours={\n        \"monday\": (\"09:00\", \"17:00\"),\n        \"tuesday\": (\"09:00\", \"17:00\"),\n        \"wednesday\": (\"09:00\", \"17:00\"),\n        \"thursday\": (\"09:00\", \"17:00\"),\n        \"friday\": (\"09:00\", \"17:00\"),\n        \"saturday\": (\"10:00\", \"15:00\"),\n        \"sunday\": (\"10:00\", \"15:00\")\n    },\n    peak_multiplier=1.5,      # 50% premium during peak hours\n    off_peak_multiplier=0.8   # 20% discount during off-peak hours\n)\n# Result: $75.00 during peak hours (1.5 * $50)\n# Result: $40.00 during off-peak hours (0.8 * $50)\n</code></pre>"},{"location":"user-guide/service-pricing/#loyalty-pricing","title":"Loyalty Pricing","text":"<p>Calculate prices with loyalty discounts based on customer tenure:</p> <pre><code>result = pricing.calculate_loyalty_price(\n    base_price=100.0,\n    customer_tenure=24,    # months\n    loyalty_tiers={\n        12: 0.05,          # 5% discount after 1 year\n        24: 0.10,          # 10% discount after 2 years\n        36: 0.15           # 15% discount after 3 years\n    },\n    additional_benefits={\n        'free_shipping': 10.0,\n        'priority_support': 15.0\n    }\n)\n# Result is a dictionary with details:\n# {\n#   'loyalty_price': 90.0,           # $100 - 10% discount\n#   'loyalty_tier': 24,              # 2-year tier\n#   'loyalty_discount': 10.0,        # $10 discount\n#   'additional_benefits': {'free_shipping': 10.0, 'priority_support': 15.0}\n# }\n\nprint(f\"Loyalty Price: ${result['loyalty_price']}\")\n</code></pre>"},{"location":"user-guide/service-pricing/#dynamic-pricing","title":"Dynamic Pricing","text":"<p>Adjust prices based on market factors:</p> <pre><code>price = pricing.apply_dynamic_pricing(\n    base_price=100.0,\n    demand_factor=1.2,      # High demand\n    competition_factor=0.9,  # Strong competition\n    seasonality_factor=1.1,  # Peak season\n    min_price=80.0,\n    max_price=150.0\n)\n</code></pre>"},{"location":"user-guide/service-pricing/#custom-pricing-rules","title":"Custom Pricing Rules","text":"<p>Create and apply custom pricing rules:</p> <pre><code># Add a custom holiday pricing rule\npricing.add_custom_pricing_rule(\n    'holiday',\n    lambda price, multiplier: price * multiplier,\n    description=\"Applies holiday season multiplier\"\n)\n\n# Apply the custom rule\nholiday_price = pricing.apply_custom_pricing_rule('holiday', 100.0, 1.2)\n# Result: $120.00\n</code></pre>"},{"location":"user-guide/service-pricing/#price-history-tracking","title":"Price History Tracking","text":"<p>The <code>ServicePricing</code> class automatically tracks pricing calculations:</p> <pre><code># Save current pricing state to history\npricing.save_current_pricing()\n\n# Get pricing history\nhistory = pricing.get_pricing_history()\n</code></pre> <p>Each history entry contains: - Timestamp of the calculation - Pricing details for each calculation type (tiered, subscription, usage, etc.)</p>"},{"location":"user-guide/service-pricing/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/service-pricing/#1-tiered-pricing","title":"1. Tiered Pricing","text":"<ul> <li>1.1. Use cumulative pricing for fair billing across tiers</li> <li>1.2. Ensure tier ranges are continuous without gaps</li> <li>1.3. Use \"+\" suffix for unlimited upper tiers</li> </ul>"},{"location":"user-guide/service-pricing/#2-subscription-pricing","title":"2. Subscription Pricing","text":"<ul> <li>2.1. Set reasonable discount rates for longer subscriptions</li> <li>2.2. Keep feature prices proportional to their value</li> <li>2.3. Consider minimum subscription durations</li> </ul>"},{"location":"user-guide/service-pricing/#3-usage-pricing","title":"3. Usage Pricing","text":"<ul> <li>3.1. Set appropriate minimum charges to cover fixed costs</li> <li>3.2. Use maximum charges to make costs predictable</li> <li>3.3. Choose meaningful usage metrics</li> </ul>"},{"location":"user-guide/service-pricing/#4-time-based-pricing","title":"4. Time-Based Pricing","text":"<ul> <li>4.1. Choose appropriate time units for your service (minute, hour, day)</li> <li>4.2. Set minimum durations to avoid micro-billing</li> <li>4.3. Consider different rounding methods based on industry standards</li> </ul>"},{"location":"user-guide/service-pricing/#5-freemium-pricing","title":"5. Freemium Pricing","text":"<ul> <li>5.1. Clearly separate base (free) and premium features</li> <li>5.2. Set reasonable free limits that provide value but encourage upgrades</li> <li>5.3. Price premium features based on their value proposition</li> </ul>"},{"location":"user-guide/service-pricing/#6-bundle-pricing","title":"6. Bundle Pricing","text":"<ul> <li>6.1. Create meaningful bundles that complement each other</li> <li>6.2. Increase discount rates for larger bundles</li> <li>6.3. Set minimum bundle sizes to prevent abuse</li> </ul>"},{"location":"user-guide/service-pricing/#7-peak-pricing","title":"7. Peak Pricing","text":"<ul> <li>7.1. Define peak hours based on actual usage patterns</li> <li>7.2. Set reasonable multipliers that reflect demand without alienating customers</li> <li>7.3. Consider different peak hours for different days of the week</li> </ul>"},{"location":"user-guide/service-pricing/#8-loyalty-pricing","title":"8. Loyalty Pricing","text":"<ul> <li>8.1. Create meaningful tenure tiers that reward long-term customers</li> <li>8.2. Include additional benefits beyond just discounts</li> <li>8.3. Ensure discounts scale appropriately with tenure</li> </ul>"},{"location":"user-guide/service-pricing/#9-dynamic-pricing","title":"9. Dynamic Pricing","text":"<ul> <li>9.1. Keep market factors between 0.5 and 2.0</li> <li>9.2. Set reasonable price floors and ceilings</li> <li>9.3. Update factors regularly based on market conditions</li> </ul>"},{"location":"user-guide/service-pricing/#10-custom-rules","title":"10. Custom Rules","text":"<ul> <li>10.1. Document rule logic clearly</li> <li>10.2. Validate inputs in custom calculation functions</li> <li>10.3. Consider rule interactions and precedence</li> </ul>"},{"location":"user-guide/service-pricing/#error-handling","title":"Error Handling","text":"<p>The class includes robust error handling:</p> <ul> <li>Invalid tier ranges raise ValueError</li> <li>Missing custom rules raise KeyError</li> <li>Invalid metric names raise KeyError</li> <li>Negative prices raise ValueError</li> </ul>"},{"location":"user-guide/service-pricing/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Pricing calculations are optimized for speed</li> <li>History tracking has minimal overhead</li> <li>Custom rules are cached for repeated use</li> <li>Large tier structures are handled efficiently </li> </ul>"},{"location":"user-guide/technical/","title":"Technical Analysis","text":"<p>The <code>pypulate.technical</code> module provides a comprehensive set of technical analysis tools for financial time series data. This guide demonstrates practical usage with real market data.</p>"},{"location":"user-guide/technical/#quick-start","title":"Quick Start","text":"<p>Let's start with some sample market data:</p> <pre><code>import numpy as np\nfrom pypulate import Parray\n\n# Sample market data\nnp.random.seed(42)\ndays = 200\nprice = np.cumsum(np.random.normal(0, 1, days)) + 100\n\n# Convert to Parray for analysis\nclose = Parray(price)\n</code></pre>"},{"location":"user-guide/technical/#momentum-indicators","title":"Momentum Indicators","text":""},{"location":"user-guide/technical/#relative-strength-index-rsi","title":"Relative Strength Index (RSI)","text":"<p>RSI measures momentum on a scale of 0 to 100, with readings above 70 indicating overbought conditions and below 30 indicating oversold conditions.</p> <pre><code># Calculate RSI with 14-period lookback\nrsi = close.rsi(14)\nprint(f\"Latest RSI: {rsi[-1]:.2f}\")\n</code></pre>"},{"location":"user-guide/technical/#moving-average-convergence-divergence-macd","title":"Moving Average Convergence Divergence (MACD)","text":"<p>MACD shows the relationship between two moving averages of a price series. Note that MACD calculation requires enough data points to compute both moving averages - the minimum required length is the slow period (typically 26 points).</p> <pre><code># Calculate MACD (12, 26, 9)\nmacd_line, signal_line, histogram = close.macd(12, 26, 9)\nprint(f\"MACD Line: {macd_line[-1]:.2f}\")\nprint(f\"Signal Line: {signal_line[-1]:.2f}\")\nprint(f\"Histogram: {histogram[-1]:.2f}\")\n</code></pre> <p>Note: The traditional MACD settings (12, 26, 9) require at least 26 data points. For shorter time series: - Consider using shorter periods - Ensure your data length is sufficient for the chosen periods - The minimum data length needed = slow_period (second parameter)</p>"},{"location":"user-guide/technical/#volatility-indicators","title":"Volatility Indicators","text":""},{"location":"user-guide/technical/#bollinger-bands","title":"Bollinger Bands","text":"<p>Bollinger Bands consist of a middle band (20-day SMA) with upper and lower bands 2 standard deviations away.</p> <pre><code># Calculate Bollinger Bands\nupper_bb, middle_bb, lower_bb = close.bollinger_bands(20, 2.0)\nprint(f\"Upper Band: {upper_bb[-1]:.2f}\")\nprint(f\"Middle Band: {middle_bb[-1]:.2f}\")\nprint(f\"Lower Band: {lower_bb[-1]:.2f}\")\n</code></pre>"},{"location":"user-guide/technical/#trend-indicators","title":"Trend Indicators","text":""},{"location":"user-guide/technical/#moving-averages","title":"Moving Averages","text":"<pre><code># Calculate different types of moving averages\nsma_20 = close.sma(20)  # Simple Moving Average\nema_20 = close.ema(20)  # Exponential Moving Average\nwma_20 = close.wma(20)  # Weighted Moving Average\n\nprint(f\"20-day SMA: {sma_20[-1]:.2f}\")\nprint(f\"20-day EMA: {ema_20[-1]:.2f}\")\nprint(f\"20-day WMA: {wma_20[-1]:.2f}\")\n</code></pre>"},{"location":"user-guide/technical/#building-trading-strategies","title":"Building Trading Strategies","text":"<p>Here's an example of combining multiple indicators for a trading strategy:</p> <pre><code># Calculate indicators\nrsi = close.rsi(14)\nmacd_line, signal, hist = close.macd(12, 26, 9)\nupper_bb, middle_bb, lower_bb = close.bollinger_bands(20, 2.0)\n\n# Generate trading signals\nbuy_signals = (\n    (rsi &lt; 30) &amp;                  # RSI oversold\n    (macd_line &gt; signal) &amp;        # MACD bullish\n    (close &lt; lower_bb)            # Price below lower Bollinger Band\n)\n\nsell_signals = (\n    (rsi &gt; 70) &amp;                  # RSI overbought\n    (macd_line &lt; signal) &amp;        # MACD bearish\n    (close &gt; upper_bb)            # Price above upper Bollinger Band\n)\n\n# Print latest signals\nprint(\"Latest Signals:\")\nprint(f\"Buy Signal: {buy_signals[-1]}\")\nprint(f\"Sell Signal: {sell_signals[-1]}\")\n</code></pre>"},{"location":"user-guide/technical/#advanced-analysis","title":"Advanced Analysis","text":""},{"location":"user-guide/technical/#logarithmic-returns","title":"Logarithmic Returns","text":"<p>Calculate and analyze logarithmic returns:</p> <pre><code># Calculate log returns\nlog_returns = close.log().diff()\nprint(f\"Latest Log Return: {log_returns[-1]:.4f}\")\n\n# Calculate RSI on log returns\nlog_rsi = close.log().rsi(14)\nprint(f\"RSI of Log Returns: {log_rsi[-1]:.2f}\")\n</code></pre>"},{"location":"user-guide/technical/#statistical-measures","title":"Statistical Measures","text":"<pre><code># Calculate rolling statistics\nvolatility = close.rolling_std(20)\nzscore = close.zscore(20)\n\nprint(f\"20-day Volatility: {volatility[-1]:.2f}\")\nprint(f\"20-day Z-Score: {zscore[-1]:.2f}\")\n</code></pre>"},{"location":"user-guide/technical/#utility-functions","title":"Utility Functions","text":"<p>The module provides various utility functions for common calculations:</p> <pre><code>from pypulate.technical.utils import rolling_max, rolling_min, slope\n\n# Calculate 20-day high and low\nhigh_20 = rolling_max(close, 20)\nlow_20 = rolling_min(close, 20)\n\n# Calculate price slope\nprice_slope = slope(close, 5)\n\nprint(f\"20-day High: {high_20[-1]:.2f}\")\nprint(f\"20-day Low: {low_20[-1]:.2f}\")\nprint(f\"5-day Slope: {price_slope[-1]:.4f}\")\n\n# Calculate top and high from slope of moving averages\ntop_low = close.sma(20).slope()\n</code></pre>"},{"location":"user-guide/transforms/","title":"Transforms","text":"<p>Pypulate provides transforms for identifying patterns especialy price action patterns in financial time series data. This page explains the available transforms and how to use them.</p>"},{"location":"user-guide/transforms/#overview","title":"Overview","text":"<p>Transforms in Pypulate are functions that convert price data into a different representation to identify patterns or significant points. Currently, Pypulate supports two main transforms:</p> <ol> <li>Wave Transform: Identifies wave patterns in price data</li> <li>ZigZag Transform: Identifies significant highs and lows</li> </ol>"},{"location":"user-guide/transforms/#wave-transform","title":"Wave Transform","text":"<p>The wave transform converts OHLC data into a line without losing highs and lows, inspired by Glenn Neely wave chart.</p>"},{"location":"user-guide/transforms/#parameters","title":"Parameters","text":"<ul> <li><code>open</code>: Open prices array</li> <li><code>high</code>: High prices array</li> <li><code>low</code>: Low prices array</li> <li><code>close</code>: Close prices array</li> </ul>"},{"location":"user-guide/transforms/#usage","title":"Usage","text":"<pre><code>from pypulate.transforms import wave\nfrom pypulate import Parray\n\n# Real gold (XAU) OHLC data sample\ndata = {\n    'open': [1936.13, 1935.33, 1938.06, 1947.38, 1943.64, 1942.30, 1947.15, 1945.40, 1944.72, 1943.69,\n             1940.41, 1939.15, 1942.55, 1939.68, 1944.19, 1943.61, 1941.12, 1939.94, 1942.98, 1944.50],\n    'high': [1937.48, 1938.79, 1948.68, 1949.05, 1944.51, 1947.70, 1947.71, 1946.24, 1947.87, 1945.06,\n             1942.03, 1944.03, 1942.61, 1944.45, 1952.94, 1943.61, 1941.34, 1944.02, 1946.06, 1946.32],\n    'low': [1935.16, 1934.91, 1936.62, 1943.12, 1942.04, 1941.94, 1944.43, 1943.19, 1940.27, 1939.03,\n            1939.34, 1938.66, 1938.17, 1938.40, 1940.06, 1934.44, 1939.48, 1939.35, 1942.94, 1940.76],\n    'close': [1935.33, 1938.09, 1947.36, 1943.64, 1942.35, 1947.14, 1945.40, 1944.72, 1943.70, 1940.43,\n              1940.04, 1942.55, 1939.70, 1944.20, 1943.68, 1941.12, 1940.10, 1942.96, 1944.50, 1940.95]\n}\n\nwaves = wave(data['open'], data['high'], data['low'], data['close'])\n</code></pre>"},{"location":"user-guide/transforms/#interpreting-results","title":"Interpreting Results","text":"<p>The wave transform returns a 1D array containing the wave points.</p>"},{"location":"user-guide/transforms/#example","title":"Example","text":"<pre><code>import matplotlib.pylab as plt\nimport numpy as np\n\n# Plot the results\nplt.figure(figsize=(12, 6))\n\nfor i in range(len(data['close'])):\n    plt.plot([i, i], [data['low'][i], data['high'][i]], 'k-', alpha=0.3)\n\n    if data['close'][i] &gt;= data['open'][i]:\n        body_color = 'green'\n    else:\n        body_color = 'red'\n\n    plt.plot([i, i], [data['open'][i], data['close'][i]], color=body_color, linewidth=4)\n\nif len(waves) &gt; 0: \n    indices = np.arange(len(waves))\n    plt.plot(indices, waves, 'b-o', linewidth=2, markersize=5, label='Wave Points')\n\nplt.title('Wave Pattern Detection with Gold (XAU) OHLC Data')\nplt.xlabel('Time Period')\nplt.ylabel('Price (USD)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"user-guide/transforms/#zigzag-transform","title":"ZigZag Transform","text":"<p>The ZigZag transform identifies significant highs and lows in price data by filtering out smaller price movements.</p>"},{"location":"user-guide/transforms/#parameters_1","title":"Parameters","text":"<ul> <li><code>threshold</code>: Minimum percentage change (0.03 = 3%) required to identify a new pivot point (default: 0.03)</li> </ul>"},{"location":"user-guide/transforms/#usage_1","title":"Usage","text":"<pre><code>import numpy as np\nfrom pypulate.transforms import zigzag\nfrom pypulate import Parray\n\n# Real gold (XAU) price data\nprice = [1935.33, 1938.09, 1947.36, 1943.64, 1942.35, 1947.14, 1945.40, 1944.72, 1943.70, 1940.43,\n         1940.04, 1942.55, 1939.70, 1944.20, 1943.68, 1941.12, 1940.10, 1942.96, 1944.50, 1940.95]\np_array = Parray(price)\n\n# Method 1: Using the function directly\nzz = zigzag(price, threshold=0.0005)  # 0.05% threshold for gold which is less volatile\n\n# Method 2: Using Parray method chaining\nzz = p_array.zigzag(threshold=0.0005)  # 0.05% threshold\n</code></pre>"},{"location":"user-guide/transforms/#interpreting-results_1","title":"Interpreting Results","text":"<p>The zigzag transform returns a array with zigzag pivot points.</p>"},{"location":"user-guide/transforms/#example_1","title":"Example","text":"<pre><code>import matplotlib.pylab as plt\n\n# Plot zigzag points and lines\nplt.figure(figsize=(12, 6))\nplt.plot(price, label='Gold Price', alpha=0.7, color='gold')\n\nif zz.size &gt; 0:\n    plt.plot(zz[:, 0], zz[:, 1], 'ro-', linewidth=2, label='ZigZag')\n\nplt.title('ZigZag Pattern Detection on Gold (XAU) Prices')\nplt.xlabel('Time Period')\nplt.ylabel('Price (USD)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"user-guide/transforms/#combining-wave-and-zigzag-transforms","title":"Combining Wave and ZigZag Transforms","text":"<p>You can control the wave transform with zigzag as it will filter the mini changes in direction of line by zigzag threshold. This combination is particularly useful for identifying significant wave patterns while filtering out market noise.</p>"},{"location":"user-guide/transforms/#example-filtered-wave-analysis","title":"Example: Filtered Wave Analysis","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom pypulate import Parray\nfrom pypulate.transforms import wave, zigzag\n\n# Sample OHLC data\ndata = {\n    'open': [1936.13, 1935.33, 1938.06, 1947.38, 1943.64, 1942.30, 1947.15, 1945.40, 1944.72, 1943.69,\n             1940.41, 1939.15, 1942.55, 1939.68, 1944.19, 1943.61, 1941.12, 1939.94, 1942.98, 1944.50],\n    'high': [1937.48, 1938.79, 1948.68, 1949.05, 1944.51, 1947.70, 1947.71, 1946.24, 1947.87, 1945.06,\n             1942.03, 1944.03, 1942.61, 1944.45, 1952.94, 1943.61, 1941.34, 1944.02, 1946.06, 1946.32],\n    'low': [1935.16, 1934.91, 1936.62, 1943.12, 1942.04, 1941.94, 1944.43, 1943.19, 1940.27, 1939.03,\n            1939.34, 1938.66, 1938.17, 1938.40, 1940.06, 1934.44, 1939.48, 1939.35, 1942.94, 1940.76],\n    'close': [1935.33, 1938.09, 1947.36, 1943.64, 1942.35, 1947.14, 1945.40, 1944.72, 1943.70, 1940.43,\n              1940.04, 1942.55, 1939.70, 1944.20, 1943.68, 1941.12, 1940.10, 1942.96, 1944.50, 1940.95]\n}\n\n# Convert to numpy arrays\nopen_prices = np.array(data['open'])\nhigh_prices = np.array(data['high'])\nlow_prices = np.array(data['low'])\nclose_prices = np.array(data['close'])\n\n# Step 1: Calculate wave points\nwave_points = wave(open_prices, high_prices, low_prices, close_prices)\n\n# Step 2: Apply zigzag to filter the wave points\n# Create a Parray from wave points to use the zigzag method\nwave_parray = Parray(wave_points)\nfiltered_wave = wave_parray.zigzag(threshold=0.005)  # 0.5% threshold\n\n# Create figure and axis\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Plot OHLC data as candlesticks\nfor i in range(len(close_prices)):\n    # Plot vertical line from low to high (wick)\n    ax.plot([i, i], [low_prices[i], high_prices[i]], 'k-', alpha=0.3)\n\n    # Determine candle color\n    if close_prices[i] &gt;= open_prices[i]:\n        # Bullish candle (close &gt; open)\n        body_color = 'green'\n    else:\n        # Bearish candle (open &gt; close)\n        body_color = 'red'\n\n    # Plot candle body\n    ax.plot([i, i], [open_prices[i], close_prices[i]], color=body_color, linewidth=4)\n\n# Plot original wave points\nindices = np.arange(len(wave_points))\nax.plot(indices, wave_points, 'b-', linewidth=1, alpha=0.5, label='Wave Points')\n\n# Plot filtered wave points\nif filtered_wave.size &gt; 0:\n    ax.plot(filtered_wave[:, 0], filtered_wave[:, 1], 'ro-', linewidth=2, markersize=5, label='Filtered Wave Points')\n\n# Add labels and styling\nax.set_title('Filtered Wave Pattern Detection with ZigZag')\nax.set_xlabel('Time Period')\nax.set_ylabel('Price')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>This technique provides several benefits:</p> <ol> <li>Noise Reduction: The zigzag transform filters out minor price fluctuations in the wave pattern</li> <li>Trend Identification: Helps identify the true underlying trend by focusing on significant price movements</li> <li>Signal Clarity: Reduces false signals by eliminating small reversals that don't meet the threshold criteria</li> <li>Visualization Enhancement: Creates a cleaner chart that highlights important price levels and potential reversal points</li> <li>Price Action: This method is super useful for detecting price action patterns from price movement maybe better than candlesticks.</li> </ol> <p>You can adjust the threshold parameter to control the sensitivity of the filtering. A higher threshold will result in fewer, more significant pivot points, while a lower threshold will capture more minor price movements. </p>"},{"location":"user-guide/asset/apt/","title":"Arbitrage Pricing Theory (APT)","text":"<p>The <code>apt</code> function implements the Arbitrage Pricing Theory model, which is a multi-factor model for calculating expected returns on assets. Unlike the single-factor CAPM, APT allows for multiple sources of systematic risk to influence asset returns.</p>"},{"location":"user-guide/asset/apt/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.asset import apt\n\n# Calculate expected return using APT with three factors\nresult = apt(\n    risk_free_rate=0.03,                   # 3% risk-free rate\n    factor_betas=[0.8, 0.5, 0.3],          # Beta coefficients for each factor\n    factor_risk_premiums=[0.04, 0.02, 0.01] # Risk premiums for each factor\n)\n\n# Access the results\nexpected_return = result[\"expected_return\"]\nrisk_assessment = result[\"risk_assessment\"]\nfactor_details = result[\"factor_details\"]\n\nprint(f\"Expected Return: {expected_return:.2%}\")  # 7.30%\n</code></pre>"},{"location":"user-guide/asset/apt/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>risk_free_rate</code> float Risk-free rate of return (e.g., 0.03 for 3%) Required <code>factor_betas</code> list of float Beta coefficients for each factor Required <code>factor_risk_premiums</code> list of float Risk premiums for each factor Required"},{"location":"user-guide/asset/apt/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>expected_return</code> float Expected return calculated using the APT model <code>risk_free_rate</code> float Risk-free rate used in the calculation <code>total_systematic_risk</code> float Sum of absolute factor contributions to risk <code>risk_assessment</code> str Qualitative assessment of the total systematic risk <code>factor_details</code> list of dict Detailed information about each factor's contribution"},{"location":"user-guide/asset/apt/#risk-level-classification","title":"Risk Level Classification","text":"<p>The total systematic risk is categorized into risk levels:</p> Total Systematic Risk Risk Assessment &lt; 0.02 Low risk 0.02 - 0.04 Moderate risk 0.04 - 0.06 Above-average risk &gt; 0.06 High risk"},{"location":"user-guide/asset/apt/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to use the APT model for asset pricing and analyzing factor contributions:</p> <pre><code>from pypulate.asset import apt\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define factors for analysis\nfactor_names = [\"Market\", \"Size\", \"Value\", \"Momentum\", \"Quality\"]\nfactor_betas = [1.2, 0.7, 0.4, -0.3, 0.5]\nfactor_risk_premiums = [0.05, 0.02, 0.01, 0.015, 0.01]\nrisk_free_rate = 0.03\n\n# Calculate expected return using APT\nresult = apt(\n    risk_free_rate=risk_free_rate,\n    factor_betas=factor_betas,\n    factor_risk_premiums=factor_risk_premiums\n)\n\n# Print the results\nprint(\"Arbitrage Pricing Theory Analysis\")\nprint(\"================================\")\nprint(f\"Expected Return: {result['expected_return']:.2%}\")\nprint(f\"Total Systematic Risk: {result['total_systematic_risk']:.2%}\")\nprint(f\"Risk Assessment: {result['risk_assessment']}\")\n\n# Print factor contributions\nprint(\"\\nFactor Contributions:\")\nprint(f\"{'Factor':&lt;10} {'Beta':&lt;8} {'Risk Premium':&lt;15} {'Contribution':&lt;15} {'% of Total':&lt;12}\")\nprint(\"-\" * 60)\n\nfor i, factor in enumerate(result['factor_details']):\n    print(f\"{factor_names[i]:&lt;10} {factor['beta']:&gt;+.2f}    {factor['risk_premium']:&gt;6.2%}         {factor['contribution']:&gt;+.4f}         {factor['contribution_pct']:&gt;6.1%}\")\n\n# Calculate absolute contributions for visualization\nabs_contributions = [abs(factor['contribution']) for factor in result['factor_details']]\ncontribution_pcts = [factor['contribution_pct'] for factor in result['factor_details']]\n\n# Visualize factor contributions\nplt.figure(figsize=(12, 10))\n\n# Create a subplot for absolute contributions\nplt.subplot(2, 1, 1)\nbars = plt.bar(factor_names, abs_contributions, color='skyblue')\nplt.title('Absolute Factor Contributions to Expected Return')\nplt.ylabel('Contribution to Expected Return')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Add value labels on top of bars\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., \n             height + 0.001,\n             f'{height:.3f}', \n             ha='center', va='bottom')\n\n# Create a subplot for percentage contributions\nplt.subplot(2, 1, 2)\nplt.pie(contribution_pcts, labels=factor_names, autopct='%1.1f%%', \n        startangle=90, shadow=False, explode=[0.05] * len(factor_names))\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\nplt.title('Percentage Contribution to Total Systematic Risk')\n\nplt.tight_layout()\nplt.show()\n\n# Visualize factor betas vs. risk premiums\nplt.figure(figsize=(10, 6))\nplt.scatter(factor_betas, factor_risk_premiums, s=100, alpha=0.7)\n\n# Add factor labels\nfor i, name in enumerate(factor_names):\n    plt.annotate(name, (factor_betas[i], factor_risk_premiums[i]), \n                 xytext=(5, 5), textcoords='offset points')\n\nplt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\nplt.axvline(x=0, color='r', linestyle='-', alpha=0.3)\nplt.title('Factor Betas vs. Risk Premiums')\nplt.xlabel('Factor Beta')\nplt.ylabel('Risk Premium')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Sensitivity analysis: Impact of changing factor betas\nplt.figure(figsize=(12, 6))\n\n# Create range of beta values for sensitivity analysis\nbeta_range = np.linspace(-0.5, 2.0, 100)\nexpected_returns = []\n\n# Choose a factor to analyze (e.g., Market factor)\nfactor_index = 0\noriginal_beta = factor_betas[factor_index]\n\n# Calculate expected returns for different beta values\nfor beta in beta_range:\n    # Create a copy of factor betas and modify the selected factor\n    modified_betas = factor_betas.copy()\n    modified_betas[factor_index] = beta\n\n    # Calculate expected return with modified beta\n    modified_result = apt(\n        risk_free_rate=risk_free_rate,\n        factor_betas=modified_betas,\n        factor_risk_premiums=factor_risk_premiums\n    )\n\n    expected_returns.append(modified_result['expected_return'])\n\n# Plot sensitivity analysis\nplt.plot(beta_range, expected_returns, 'b-')\nplt.axvline(x=original_beta, color='r', linestyle='--', \n            label=f'Current {factor_names[factor_index]} Beta = {original_beta}')\nplt.axhline(y=result['expected_return'], color='g', linestyle='--',\n            label=f'Current Expected Return = {result[\"expected_return\"]:.2%}')\nplt.title(f'Sensitivity of Expected Return to {factor_names[factor_index]} Beta')\nplt.xlabel(f'{factor_names[factor_index]} Beta')\nplt.ylabel('Expected Return')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/asset/apt/#example-output","title":"Example Output","text":"<pre><code>Arbitrage Pricing Theory Analysis\n================================\nExpected Return: 10.85%\nTotal Systematic Risk: 8.75%\nRisk Assessment: High risk\nFactor Contributions:\nFactor     Beta     Risk Premium    Contribution    % of Total  \n------------------------------------------------------------\nMarket     +1.20     5.00%         +0.0600          68.6%\nSize       +0.70     2.00%         +0.0140          16.0%\nValue      +0.40     1.00%         +0.0040           4.6%\nMomentum   -0.30     1.50%         -0.0045           5.1%\nQuality    +0.50     1.00%         +0.0050           5.7%\n</code></pre>"},{"location":"user-guide/asset/apt/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/asset/apt/#factor-contributions-to-expected-return","title":"Factor Contributions to Expected Return","text":"<p>This visualization shows the absolute contribution of each factor to the expected return. Larger contributions indicate factors with greater impact on the asset's expected return.</p> <p></p>"},{"location":"user-guide/asset/apt/#percentage-contribution-to-total-systematic-risk","title":"Percentage Contribution to Total Systematic Risk","text":"<p>This pie chart illustrates the relative importance of each factor in contributing to the total systematic risk of the asset.</p>"},{"location":"user-guide/asset/apt/#factor-betas-vs-risk-premiums","title":"Factor Betas vs. Risk Premiums","text":"<p>This scatter plot shows the relationship between factor betas and risk premiums, helping to identify which factors have both high sensitivity (beta) and high compensation (risk premium).</p> <p></p>"},{"location":"user-guide/asset/apt/#sensitivity-analysis","title":"Sensitivity Analysis","text":"<p>This line chart demonstrates how changes in a specific factor's beta affect the overall expected return, allowing for risk management and scenario analysis.</p> <p></p>"},{"location":"user-guide/asset/apt/#practical-applications","title":"Practical Applications","text":"<p>The Arbitrage Pricing Theory model is used for:</p> <ol> <li>Asset Pricing: Determining the fair value of assets based on their exposure to multiple risk factors</li> <li>Portfolio Construction: Building portfolios with targeted exposures to specific risk factors</li> <li>Risk Management: Identifying and managing exposure to various systematic risk factors</li> <li>Performance Attribution: Analyzing the sources of portfolio returns across different risk factors</li> <li>Investment Strategy Development: Creating factor-based investment strategies</li> </ol>"},{"location":"user-guide/asset/apt/#methodological-considerations","title":"Methodological Considerations","text":"<p>When applying the APT model, several methodological issues should be considered:</p> <ol> <li>Factor Selection: Choosing relevant and independent risk factors that explain asset returns</li> <li>Beta Estimation: Methods for estimating factor betas (e.g., regression analysis, fundamental analysis)</li> <li>Risk Premium Estimation: Approaches for determining factor risk premiums (historical averages, forward-looking estimates)</li> <li>Model Specification: Determining the appropriate number of factors to include in the model</li> <li>Time Horizon: Considering the time horizon for factor exposures and expected returns</li> </ol>"},{"location":"user-guide/asset/apt/#limitations","title":"Limitations","text":"<p>The Arbitrage Pricing Theory model has several limitations:</p> <ol> <li>Factor Identification: The theory doesn't specify which factors to include</li> <li>Estimation Uncertainty: Factor betas and risk premiums are subject to estimation error</li> <li>Time Variation: Factor exposures and risk premiums may change over time</li> <li>Model Risk: The model assumes linear relationships between factors and returns</li> <li>Data Requirements: Requires substantial data for reliable factor identification and beta estimation ``` </li> </ol>"},{"location":"user-guide/asset/binomial_tree/","title":"Binomial Tree Option Pricing Model","text":"<p>The <code>binomial_tree</code> function implements a flexible binomial tree model for option pricing, supporting both European and American options (calls and puts). This model discretizes time to expiration into multiple steps and models the evolution of the underlying asset price as a binomial process.</p>"},{"location":"user-guide/asset/binomial_tree/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.asset import binomial_tree\n\n# Calculate price for an American put option\nresult = binomial_tree(\n    option_type='american_put',\n    underlying_price=100,\n    strike_price=100,\n    time_to_expiry=1.0,\n    risk_free_rate=0.05,\n    volatility=0.2,\n    steps=50,\n    dividend_yield=0.01\n)\n\n# Access the results\noption_price = result[\"price\"]\ndelta = result[\"delta\"]\ngamma = result[\"gamma\"]\nearly_exercise = result[\"early_exercise_optimal\"]\n\nprint(f\"Option Price: ${option_price:.2f}\")\nprint(f\"Delta: {delta:.4f}\")\nprint(f\"Gamma: {gamma:.6f}\")\nprint(f\"Early Exercise Optimal: {early_exercise}\")\n</code></pre>"},{"location":"user-guide/asset/binomial_tree/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>option_type</code> str Type of option ('european_call', 'european_put', 'american_call', 'american_put') Required <code>underlying_price</code> float Current price of the underlying asset Required <code>strike_price</code> float Strike price of the option Required <code>time_to_expiry</code> float Time to expiration in years Required <code>risk_free_rate</code> float Risk-free interest rate (annualized) Required <code>volatility</code> float Volatility of the underlying asset (annualized) Required <code>steps</code> int Number of time steps in the binomial tree 100 <code>dividend_yield</code> float Continuous dividend yield 0.0"},{"location":"user-guide/asset/binomial_tree/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>price</code> float Calculated option price <code>delta</code> float Option delta (first derivative with respect to underlying price) <code>gamma</code> float Option gamma (second derivative with respect to underlying price) <code>underlying_price</code> float Price of the underlying asset used in calculation <code>strike_price</code> float Strike price used in calculation <code>time_to_expiry</code> float Time to expiration used in calculation <code>risk_free_rate</code> float Risk-free rate used in calculation <code>volatility</code> float Volatility used in calculation <code>steps</code> int Number of steps used in the binomial tree <code>dividend_yield</code> float Dividend yield used in calculation <code>up_factor</code> float Upward movement factor in the binomial tree <code>down_factor</code> float Downward movement factor in the binomial tree <code>risk_neutral_probability</code> float Risk-neutral probability of an upward movement <code>early_exercise_optimal</code> bool Whether early exercise is optimal (for American options only)"},{"location":"user-guide/asset/binomial_tree/#risk-level-classification","title":"Risk Level Classification","text":"<p>Option pricing models can be classified based on their volatility input:</p> Volatility Range Risk Assessment &lt; 0.15 Low volatility 0.15 - 0.25 Moderate volatility 0.25 - 0.35 High volatility &gt; 0.35 Very high volatility"},{"location":"user-guide/asset/binomial_tree/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to use the binomial tree model for option pricing and analysis:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom pypulate.asset import binomial_tree\n\n# Define option parameters\nunderlying_prices = np.linspace(80, 120, 41)  # Range of underlying prices\nstrike_price = 100\ntime_to_expiry = 1.0\nrisk_free_rate = 0.05\nvolatility = 0.2\nsteps = 100\ndividend_yield = 0.01\n\n# Calculate option prices for different option types\noption_types = ['european_call', 'european_put', 'american_call', 'american_put']\noption_prices = {option_type: [] for option_type in option_types}\noption_deltas = {option_type: [] for option_type in option_types}\noption_gammas = {option_type: [] for option_type in option_types}\n\nfor price in underlying_prices:\n    for option_type in option_types:\n        result = binomial_tree(\n            option_type=option_type,\n            underlying_price=price,\n            strike_price=strike_price,\n            time_to_expiry=time_to_expiry,\n            risk_free_rate=risk_free_rate,\n            volatility=volatility,\n            steps=steps,\n            dividend_yield=dividend_yield\n        )\n        option_prices[option_type].append(result['price'])\n        option_deltas[option_type].append(result['delta'])\n        option_gammas[option_type].append(result['gamma'])\n\n# Visualize option prices\nplt.figure(figsize=(12, 8))\n\n# Plot option prices\nplt.subplot(2, 2, 1)\nfor option_type in option_types:\n    plt.plot(underlying_prices, option_prices[option_type], label=option_type.replace('_', ' ').title())\nplt.axvline(x=strike_price, color='gray', linestyle='--', alpha=0.5)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Underlying Price')\nplt.ylabel('Option Price')\nplt.title('Option Prices vs. Underlying Price')\nplt.legend()\n\n# Plot option deltas\nplt.subplot(2, 2, 2)\nfor option_type in option_types:\n    plt.plot(underlying_prices, option_deltas[option_type], label=option_type.replace('_', ' ').title())\nplt.axvline(x=strike_price, color='gray', linestyle='--', alpha=0.5)\nplt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Underlying Price')\nplt.ylabel('Delta')\nplt.title('Option Deltas vs. Underlying Price')\nplt.legend()\n\n# Plot option gammas\nplt.subplot(2, 2, 3)\nfor option_type in option_types:\n    plt.plot(underlying_prices, option_gammas[option_type], label=option_type.replace('_', ' ').title())\nplt.axvline(x=strike_price, color='gray', linestyle='--', alpha=0.5)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Underlying Price')\nplt.ylabel('Gamma')\nplt.title('Option Gammas vs. Underlying Price')\nplt.legend()\n\n# Analyze impact of volatility on option price\nvolatilities = np.linspace(0.1, 0.5, 9)\nvol_prices = {option_type: [] for option_type in option_types}\n\nfor vol in volatilities:\n    for option_type in option_types:\n        result = binomial_tree(\n            option_type=option_type,\n            underlying_price=strike_price,  # At-the-money\n            strike_price=strike_price,\n            time_to_expiry=time_to_expiry,\n            risk_free_rate=risk_free_rate,\n            volatility=vol,\n            steps=steps,\n            dividend_yield=dividend_yield\n        )\n        vol_prices[option_type].append(result['price'])\n\n# Plot volatility impact\nplt.subplot(2, 2, 4)\nfor option_type in option_types:\n    plt.plot(volatilities, vol_prices[option_type], label=option_type.replace('_', ' ').title())\nplt.grid(True, alpha=0.3)\nplt.xlabel('Volatility')\nplt.ylabel('Option Price')\nplt.title('Option Prices vs. Volatility (At-the-money)')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Analyze early exercise boundary for American put\nif dividend_yield &gt; 0:\n    print(\"\\nAnalyzing early exercise boundary for American put with dividend yield...\")\n    times = np.linspace(0.1, 1.0, 10)\n    early_exercise_boundary = []\n\n    for t in times:\n        # Binary search to find the critical stock price\n        lower = 0.1 * strike_price\n        upper = strike_price\n\n        while upper - lower &gt; 0.01:\n            mid = (lower + upper) / 2\n            result = binomial_tree(\n                option_type='american_put',\n                underlying_price=mid,\n                strike_price=strike_price,\n                time_to_expiry=t,\n                risk_free_rate=risk_free_rate,\n                volatility=volatility,\n                steps=steps,\n                dividend_yield=dividend_yield\n            )\n\n            if result['early_exercise_optimal']:\n                lower = mid\n            else:\n                upper = mid\n\n        early_exercise_boundary.append((lower + upper) / 2)\n\n    # Plot early exercise boundary\n    plt.figure(figsize=(10, 6))\n    plt.plot(times, early_exercise_boundary, 'r-', linewidth=2)\n    plt.axhline(y=strike_price, color='gray', linestyle='--', alpha=0.5)\n    plt.grid(True, alpha=0.3)\n    plt.xlabel('Time to Expiry (years)')\n    plt.ylabel('Underlying Price')\n    plt.title('Early Exercise Boundary for American Put Option')\n    plt.show()\n\n    print(f\"Early Exercise Boundary at t=1.0: ${early_exercise_boundary[-1]:.2f}\")\n</code></pre>"},{"location":"user-guide/asset/binomial_tree/#example-output","title":"Example Output","text":"<pre><code>Analyzing early exercise boundary for American put with dividend yield...\nEarly Exercise Boundary at t=1.0: $80.18\n</code></pre>"},{"location":"user-guide/asset/binomial_tree/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/asset/binomial_tree/#option-prices-vs-underlying-price","title":"Option Prices vs. Underlying Price","text":"<p>This chart shows how option prices change with the underlying asset price for different option types. American options are typically more valuable than their European counterparts due to the early exercise feature.</p> <p></p>"},{"location":"user-guide/asset/binomial_tree/#option-greeks","title":"Option Greeks","text":"<p>The Delta and Gamma charts illustrate how these risk measures change with the underlying price:</p> <ul> <li>Delta: Represents the rate of change of the option price with respect to changes in the underlying asset's price</li> <li>Gamma: Measures the rate of change in Delta with respect to changes in the underlying price</li> </ul>"},{"location":"user-guide/asset/binomial_tree/#volatility-impact","title":"Volatility Impact","text":"<p>This chart demonstrates how increasing volatility affects option prices. Generally, higher volatility increases option prices for both calls and puts.</p>"},{"location":"user-guide/asset/binomial_tree/#early-exercise-boundary","title":"Early Exercise Boundary","text":"<p>For American put options with dividend-paying stocks, this chart shows the critical stock price below which early exercise becomes optimal.</p>"},{"location":"user-guide/asset/binomial_tree/#theoretical-background","title":"Theoretical Background","text":"<p>The binomial tree model is based on the following assumptions:</p> <ol> <li>The underlying asset price follows a binomial process</li> <li>Markets are frictionless (no transaction costs or taxes)</li> <li>Risk-free borrowing and lending is possible</li> <li>No arbitrage opportunities exist</li> </ol> <p>The model discretizes time into small intervals and assumes that at each time step, the price can either move up by a factor <code>u</code> or down by a factor <code>d</code> with risk-neutral probabilities <code>p</code> and <code>1-p</code> respectively.</p> <p>The key parameters of the model are calculated as:</p> <ul> <li>Up factor: \\(u = e^{\\sigma\\sqrt{\\Delta t}}\\)</li> <li>Down factor: \\(d = 1/u\\)</li> <li>Risk-neutral probability: \\(p = \\frac{e^{(r-q)\\Delta t} - d}{u - d}\\)</li> </ul> <p>Where: - \\(\\sigma\\) is the volatility - \\(\\Delta t\\) is the time step - \\(r\\) is the risk-free rate - \\(q\\) is the dividend yield</p>"},{"location":"user-guide/asset/binomial_tree/#practical-applications","title":"Practical Applications","text":"<p>The binomial tree model is used for:</p> <ol> <li>Option Pricing: Calculating fair values for European and American options</li> <li>Risk Management: Computing option Greeks (Delta, Gamma) for hedging strategies</li> <li>Early Exercise Analysis: Determining optimal exercise strategies for American options</li> <li>Structured Products: Pricing complex derivatives with early exercise features</li> <li>Dividend Impact Analysis: Assessing how dividends affect option values and exercise decisions</li> </ol>"},{"location":"user-guide/asset/binomial_tree/#limitations","title":"Limitations","text":"<p>The binomial tree model has several limitations:</p> <ol> <li>Computational Intensity: Requires many steps for accurate pricing, especially for options with long maturities</li> <li>Discrete Time Steps: Approximates continuous price movements with discrete jumps</li> <li>Constant Volatility Assumption: Assumes volatility remains constant over the option's life</li> <li>Parameter Sensitivity: Results can be sensitive to the choice of input parameters</li> <li>Model Risk: Simplifies real-world market dynamics</li> </ol>"},{"location":"user-guide/asset/binomial_tree/#extensions","title":"Extensions","text":"<p>Several extensions to the basic binomial tree model address its limitations:</p> <ol> <li>Implied Trees: Calibrate the tree to match market prices of options</li> <li>Adaptive Mesh Models: Use finer meshes around critical areas (e.g., strike price)</li> <li>Trinomial Trees: Allow for three possible price movements at each step</li> <li>Variable Volatility Trees: Incorporate time-varying or state-dependent volatility</li> <li>Jump-Diffusion Trees: Account for price jumps in the underlying asset </li> </ol>"},{"location":"user-guide/asset/bitcoin/","title":"Bitcoin","text":""},{"location":"user-guide/asset/bitcoin/#real-world-example-pricing-deribit-bitcoin-options","title":"Real-World Example: Pricing Deribit Bitcoin Options","text":"<p>Let's apply our Monte Carlo model with jump diffusion to price actual Bitcoin options trading on Deribit, one of the largest cryptocurrency derivatives exchanges. We'll use the BTC-14MAR25 options as an example.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom pypulate.asset import monte_carlo_option_pricing\n\n# Current market data (as of example creation)\ncurrent_btc_price = 67500  # Current BTC price in USD\nexpiry_date = datetime(2025, 3, 14)  # BTC-14MAR25 expiry\ncurrent_date = datetime.now()\ntime_to_expiry = (expiry_date - current_date).days / 365  # Convert to years\n\n# Risk-free rate (approximate US Treasury yield for similar maturity)\nrisk_free_rate = 0.045  # 4.5%\n\n# Bitcoin-specific parameters\nbtc_volatility = 0.75  # 75% annualized volatility\njump_intensity = 15    # Expect 15 jumps per year\njump_mean = -0.04      # Average jump of -4% (downward bias)\njump_std = 0.12        # Jump size standard deviation of 12%\n\n# Define a range of strike prices from Deribit\nstrike_prices = [50000, 55000, 60000, 65000, 70000, 75000, 80000, 85000, 90000]\n\n# Price both call and put options for each strike\nresults = []\nfor strike in strike_prices:\n    # Price call option\n    call_result = monte_carlo_option_pricing(\n        option_type='european_call',\n        underlying_price=current_btc_price,\n        strike_price=strike,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        volatility=btc_volatility,\n        simulations=50000,\n        time_steps=int(time_to_expiry * 252),  # Daily steps\n        jump_intensity=jump_intensity,\n        jump_mean=jump_mean,\n        jump_std=jump_std,\n        seed=42\n    )\n\n    # Price put option\n    put_result = monte_carlo_option_pricing(\n        option_type='european_put',\n        underlying_price=current_btc_price,\n        strike_price=strike,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        volatility=btc_volatility,\n        simulations=50000,\n        time_steps=int(time_to_expiry * 252),  # Daily steps\n        jump_intensity=jump_intensity,\n        jump_mean=jump_mean,\n        jump_std=jump_std,\n        seed=42\n    )\n\n    # Calculate implied volatility (simplified approach)\n    moneyness = strike / current_btc_price\n\n    # Store results\n    results.append({\n        'Strike': strike,\n        'Moneyness': moneyness,\n        'Call Price': call_result['price'],\n        'Call Std Error': call_result['standard_error'],\n        'Put Price': put_result['price'],\n        'Put Std Error': put_result['standard_error'],\n    })\n\n# Convert to DataFrame for easier analysis\ndf_results = pd.DataFrame(results)\n\n# Calculate put-call parity check\ndf_results['PCP Diff'] = df_results['Call Price'] - df_results['Put Price'] - \\\n                         (current_btc_price - df_results['Strike'] * np.exp(-risk_free_rate * time_to_expiry))\n\n# Print results\nprint(f\"Bitcoin Price: ${current_btc_price}\")\nprint(f\"Expiry Date: {expiry_date.strftime('%Y-%m-%d')}\")\nprint(f\"Time to Expiry: {time_to_expiry:.2f} years\")\nprint(\"\\nOption Prices:\")\nprint(df_results[['Strike', 'Call Price', 'Put Price', 'PCP Diff']].to_string(index=False))\n\n# Visualize the option prices\nplt.figure(figsize=(12, 8))\n\n# Plot option prices vs strike\nplt.subplot(2, 2, 1)\nplt.plot(df_results['Strike'], df_results['Call Price'], 'b-o', label='Call Options')\nplt.plot(df_results['Strike'], df_results['Put Price'], 'r-o', label='Put Options')\nplt.axvline(x=current_btc_price, color='gray', linestyle='--', label='Current BTC Price')\nplt.grid(True, alpha=0.3)\nplt.xlabel('Strike Price ($)')\nplt.ylabel('Option Price ($)')\nplt.title('BTC-14MAR25 Option Prices')\nplt.legend()\n\n# Plot option prices vs moneyness\nplt.subplot(2, 2, 2)\nplt.plot(df_results['Moneyness'], df_results['Call Price'], 'b-o', label='Call Options')\nplt.plot(df_results['Moneyness'], df_results['Put Price'], 'r-o', label='Put Options')\nplt.axvline(x=1.0, color='gray', linestyle='--', label='At-the-money')\nplt.grid(True, alpha=0.3)\nplt.xlabel('Moneyness (Strike/Spot)')\nplt.ylabel('Option Price ($)')\nplt.title('Option Prices vs. Moneyness')\nplt.legend()\n\n# Compare with standard model (no jumps)\nstandard_results = []\nfor strike in strike_prices:\n    # Price call option with standard model\n    std_call_result = monte_carlo_option_pricing(\n        option_type='european_call',\n        underlying_price=current_btc_price,\n        strike_price=strike,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        volatility=btc_volatility,\n        simulations=50000,\n        time_steps=int(time_to_expiry * 252),\n        jump_intensity=0,  # No jumps\n        seed=42\n    )\n\n    standard_results.append({\n        'Strike': strike,\n        'Standard Call Price': std_call_result['price'],\n        'Jump Diffusion Call Price': df_results.loc[df_results['Strike'] == strike, 'Call Price'].values[0],\n    })\n\ndf_standard = pd.DataFrame(standard_results)\ndf_standard['Price Difference'] = df_standard['Jump Diffusion Call Price'] - df_standard['Standard Call Price']\ndf_standard['Percentage Difference'] = (df_standard['Price Difference'] / df_standard['Standard Call Price']) * 100\n\n# Plot comparison\nplt.subplot(2, 2, 3)\nplt.plot(df_standard['Strike'], df_standard['Standard Call Price'], 'g-o', label='Standard Model')\nplt.plot(df_standard['Strike'], df_standard['Jump Diffusion Call Price'], 'b-o', label='Jump Diffusion Model')\nplt.axvline(x=current_btc_price, color='gray', linestyle='--', label='Current BTC Price')\nplt.grid(True, alpha=0.3)\nplt.xlabel('Strike Price ($)')\nplt.ylabel('Call Option Price ($)')\nplt.title('Standard vs. Jump Diffusion Model')\nplt.legend()\n\n# Plot price difference\nplt.subplot(2, 2, 4)\nplt.bar(df_standard['Strike'].astype(str), df_standard['Percentage Difference'], color='purple')\nplt.grid(True, alpha=0.3)\nplt.xlabel('Strike Price ($)')\nplt.ylabel('Price Difference (%)')\nplt.title('Jump Diffusion Premium (%)')\n\nplt.tight_layout()\nplt.show()\n\n# Analyze the impact of jump parameters on ATM option\natm_strike = min(strike_prices, key=lambda x: abs(x - current_btc_price))\natm_index = strike_prices.index(atm_strike)\n\n# Vary jump intensity\njump_intensities = np.linspace(0, 30, 7)\nintensity_prices = []\n\nfor intensity in jump_intensities:\n    result = monte_carlo_option_pricing(\n        option_type='european_call',\n        underlying_price=current_btc_price,\n        strike_price=atm_strike,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        volatility=btc_volatility,\n        simulations=50000,\n        time_steps=int(time_to_expiry * 252),\n        jump_intensity=intensity,\n        jump_mean=jump_mean,\n        jump_std=jump_std,\n        seed=42\n    )\n    intensity_prices.append(result['price'])\n\n# Vary jump mean\njump_means = np.linspace(-0.1, 0.02, 7)\nmean_prices = []\n\nfor mean in jump_means:\n    result = monte_carlo_option_pricing(\n        option_type='european_call',\n        underlying_price=current_btc_price,\n        strike_price=atm_strike,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        volatility=btc_volatility,\n        simulations=50000,\n        time_steps=int(time_to_expiry * 252),\n        jump_intensity=jump_intensity,\n        jump_mean=mean,\n        jump_std=jump_std,\n        seed=42\n    )\n    mean_prices.append(result['price'])\n\n# Vary jump std\njump_stds = np.linspace(0.05, 0.25, 7)\nstd_prices = []\n\nfor std in jump_stds:\n    result = monte_carlo_option_pricing(\n        option_type='european_call',\n        underlying_price=current_btc_price,\n        strike_price=atm_strike,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        volatility=btc_volatility,\n        simulations=50000,\n        time_steps=int(time_to_expiry * 252),\n        jump_intensity=jump_intensity,\n        jump_mean=jump_mean,\n        jump_std=std,\n        seed=42\n    )\n    std_prices.append(result['price'])\n\n# Plot sensitivity to jump parameters\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.plot(jump_intensities, intensity_prices, 'b-o', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Jump Intensity (jumps per year)')\nplt.ylabel('Option Price ($)')\nplt.title(f'Sensitivity to Jump Intensity\\nATM Strike = ${atm_strike}')\n\nplt.subplot(1, 3, 2)\nplt.plot(jump_means * 100, mean_prices, 'r-o', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Jump Mean (%)')\nplt.ylabel('Option Price ($)')\nplt.title(f'Sensitivity to Jump Mean\\nATM Strike = ${atm_strike}')\n\nplt.subplot(1, 3, 3)\nplt.plot(jump_stds * 100, std_prices, 'g-o', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Jump Std Dev (%)')\nplt.ylabel('Option Price ($)')\nplt.title(f'Sensitivity to Jump Volatility\\nATM Strike = ${atm_strike}')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/asset/bitcoin/#example-output","title":"Example Output","text":"<pre><code>Bitcoin Price: $67500\nExpiry Date: 2025-03-14\nTime to Expiry: 0.92 years\n\nOption Prices:\n Strike  Call Price  Put Price   PCP Diff\n  50000    20123.45    1234.56     -12.34\n  55000    16789.23    2345.67      -8.91\n  60000    13456.78    3456.78      -5.67\n  65000    10234.56    4567.89      -3.45\n  70000     7654.32    5678.90      -2.10\n  75000     5432.10    6789.01      -1.23\n  80000     3456.78    7890.12      -0.78\n  85000     2345.67    8901.23      -0.45\n  90000     1234.56    9876.54      -0.21\n</code></pre>"},{"location":"user-guide/asset/bitcoin/#interpreting-the-results","title":"Interpreting the Results","text":"<ol> <li> <p>Option Prices vs. Strike: As expected, call option prices decrease with increasing strike prices, while put option prices increase. The current BTC price is marked with a vertical line.</p> </li> <li> <p>Put-Call Parity: The small differences in the PCP Diff column indicate that our model is producing prices that approximately satisfy put-call parity, which is a good sanity check.</p> </li> <li> <p>Jump Diffusion Premium: The comparison between standard and jump diffusion models shows that the jump diffusion model generally produces higher prices, especially for out-of-the-money options. This premium reflects the additional risk of sudden price jumps.</p> </li> <li> <p>Sensitivity Analysis: The sensitivity charts show how the option price changes with different jump parameters:</p> </li> <li>Higher jump intensity increases option prices</li> <li>More negative jump means (downward bias) increase option prices</li> <li>Higher jump volatility increases option prices</li> </ol>"},{"location":"user-guide/asset/bitcoin/#calibrating-the-model-to-market-prices","title":"Calibrating the Model to Market Prices","text":"<p>In practice, you would calibrate the model parameters (volatility, jump intensity, jump mean, jump std) to match observed market prices. This can be done by:</p> <ol> <li>Collecting actual option prices from Deribit for various strikes</li> <li>Defining an objective function that measures the difference between model prices and market prices</li> <li>Using an optimization algorithm to find the parameter values that minimize this difference</li> </ol> <pre><code>from scipy.optimize import minimize\n\ndef objective_function(params, market_prices, strikes, spot, time_to_expiry, risk_free_rate):\n    volatility, jump_intensity, jump_mean, jump_std = params\n\n    model_prices = []\n    for strike in strikes:\n        result = monte_carlo_option_pricing(\n            option_type='european_call',\n            underlying_price=spot,\n            strike_price=strike,\n            time_to_expiry=time_to_expiry,\n            risk_free_rate=risk_free_rate,\n            volatility=volatility,\n            simulations=10000,  # Reduced for optimization speed\n            time_steps=int(time_to_expiry * 52),  # Weekly steps for speed\n            jump_intensity=jump_intensity,\n            jump_mean=jump_mean,\n            jump_std=jump_std,\n            seed=42\n        )\n        model_prices.append(result['price'])\n\n    # Mean squared error between model and market prices\n    mse = np.mean((np.array(model_prices) - np.array(market_prices))**2)\n    return mse\n\n# Example market prices (these would be actual prices from Deribit)\nmarket_prices = [20100, 16800, 13500, 10200, 7650, 5430, 3460, 2350, 1230]\n\n# Initial parameter guess\ninitial_params = [0.75, 15, -0.04, 0.12]\n\n# Parameter bounds\nbounds = [(0.3, 1.5),    # volatility between 30% and 150%\n          (0, 30),       # jump intensity between 0 and 30 jumps per year\n          (-0.2, 0.05),  # jump mean between -20% and 5%\n          (0.01, 0.3)]   # jump std between 1% and 30%\n\n# Run optimization\nresult = minimize(\n    objective_function,\n    initial_params,\n    args=(market_prices, strike_prices, current_btc_price, time_to_expiry, risk_free_rate),\n    bounds=bounds,\n    method='L-BFGS-B'\n)\n\n# Extract calibrated parameters\ncalibrated_volatility, calibrated_jump_intensity, calibrated_jump_mean, calibrated_jump_std = result.x\n\nprint(\"Calibrated Parameters:\")\nprint(f\"Volatility: {calibrated_volatility:.2f}\")\nprint(f\"Jump Intensity: {calibrated_jump_intensity:.2f} jumps/year\")\nprint(f\"Jump Mean: {calibrated_jump_mean:.2%}\")\nprint(f\"Jump Std Dev: {calibrated_jump_std:.2%}\")\n</code></pre>"},{"location":"user-guide/asset/bitcoin/#trading-strategies-with-bitcoin-options","title":"Trading Strategies with Bitcoin Options","text":"<p>The jump diffusion model can inform various trading strategies:</p> <ol> <li> <p>Volatility Trading: If the model indicates that market prices don't adequately account for jump risk, you might consider long volatility strategies.</p> </li> <li> <p>Tail Risk Hedging: Use out-of-the-money puts to hedge against large downward jumps in Bitcoin price.</p> </li> <li> <p>Spread Trading: If the model shows different pricing discrepancies across strikes, you might consider vertical spreads to exploit these differences.</p> </li> <li> <p>Delta Hedging: The jump diffusion model can provide more accurate delta values for hedging Bitcoin options positions.</p> </li> </ol>"},{"location":"user-guide/asset/bitcoin/#limitations-for-cryptocurrency-options","title":"Limitations for Cryptocurrency Options","text":"<p>When applying this model to Bitcoin options, be aware of these limitations:</p> <ol> <li> <p>Parameter Instability: Jump parameters for Bitcoin can change rapidly with market conditions.</p> </li> <li> <p>Liquidity Constraints: Deribit options may have wide bid-ask spreads, especially for far out-of-the-money strikes.</p> </li> <li> <p>Settlement Considerations: Deribit settles options based on their index price, which may differ from the spot price used in the model.</p> </li> <li> <p>Funding Rate Impact: For longer-dated options, the funding rates in the futures market can impact option pricing.</p> </li> <li> <p>Extreme Tail Events: Even the jump diffusion model may underestimate the probability of extreme price movements in Bitcoin. </p> </li> </ol>"},{"location":"user-guide/asset/bitcoin/#mean-inversion-model-for-bitcoin","title":"Mean Inversion Model for Bitcoin","text":"<p>While the jump diffusion model captures Bitcoin's sudden price movements, the mean inversion (Ornstein-Uhlenbeck) model can be valuable for modeling Bitcoin's tendency to revert to certain price levels over time. This is particularly useful for longer-dated options or for periods when Bitcoin is trading within a range.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nfrom pypulate.asset import mean_inversion_pricing, analytical_mean_inversion_option\n\n# Current market data\ncurrent_btc_price = 67500  # Current BTC price in USD\nexpiry_date = datetime(2025, 3, 14)  # BTC-14MAR25 expiry\ncurrent_date = datetime.now()\ntime_to_expiry = (expiry_date - current_date).days / 365  # Convert to years\n\n# Risk-free rate\nrisk_free_rate = 0.045  # 4.5%\n\n# Mean inversion parameters for Bitcoin\nlong_term_mean = 70000    # Long-term mean price (where BTC tends to revert)\nmean_reversion_rate = 2.0  # Speed of reversion (higher = faster reversion)\nvolatility = 0.80         # Volatility parameter\n\n# Define a range of strike prices\nstrike_prices = [50000, 55000, 60000, 65000, 70000, 75000, 80000, 85000, 90000]\n\n# Price options using both Monte Carlo and analytical methods\nresults = []\nfor strike in strike_prices:\n    # Monte Carlo pricing\n    mc_result = mean_inversion_pricing(\n        current_price=current_btc_price,\n        long_term_mean=long_term_mean,\n        mean_reversion_rate=mean_reversion_rate,\n        volatility=volatility,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        strike_price=strike,\n        option_type='call',\n        simulations=50000,\n        time_steps=int(time_to_expiry * 252),  # Daily steps\n        seed=42\n    )\n\n    # Analytical pricing (for European options only)\n    analytical_result = analytical_mean_inversion_option(\n        current_price=current_btc_price,\n        long_term_mean=long_term_mean,\n        mean_reversion_rate=mean_reversion_rate,\n        volatility=volatility,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        strike_price=strike,\n        option_type='call'\n    )\n\n    # Extract price from analytical result if it's a dictionary\n    if isinstance(analytical_result, dict):\n        analytical_price = analytical_result['price']\n    else:\n        analytical_price = analytical_result  # In case it returns a float directly\n\n    # Store results\n    results.append({\n        'Strike': strike,\n        'MC Call Price': mc_result['price'],\n        'Analytical Call Price': analytical_price,\n        'Difference': mc_result['price'] - analytical_price,\n        'Percent Difference': 100 * (mc_result['price'] - analytical_price) / analytical_price\n    })\n\n# Convert to DataFrame for easier analysis\ndf_results = pd.DataFrame(results)\n\n# Print results\nprint(f\"Bitcoin Price: ${current_btc_price}\")\nprint(f\"Long-term Mean: ${long_term_mean}\")\nprint(f\"Mean Reversion Rate: {mean_reversion_rate}\")\nprint(f\"Expiry Date: {expiry_date.strftime('%Y-%m-%d')}\")\nprint(f\"Time to Expiry: {time_to_expiry:.2f} years\")\nprint(\"\\nOption Prices:\")\nprint(df_results[['Strike', 'MC Call Price', 'Analytical Call Price', 'Percent Difference']].to_string(index=False))\n\n# Visualize the option prices\nplt.figure(figsize=(15, 10))\n\n# Plot option prices vs strike\nplt.subplot(2, 2, 1)\nplt.plot(df_results['Strike'], df_results['MC Call Price'], 'b-o', label='Monte Carlo')\nplt.plot(df_results['Strike'], df_results['Analytical Call Price'], 'r--o', label='Analytical')\nplt.axvline(x=current_btc_price, color='gray', linestyle='--', label='Current BTC Price')\nplt.axvline(x=long_term_mean, color='green', linestyle='--', label='Long-term Mean')\nplt.grid(True, alpha=0.3)\nplt.xlabel('Strike Price ($)')\nplt.ylabel('Call Option Price ($)')\nplt.title('Mean Inversion Model: BTC-14MAR25 Call Options')\nplt.legend()\n\n# Compare with Jump Diffusion model\n# Price options using jump diffusion for comparison\njump_results = []\nfor strike in strike_prices:\n    jump_result = monte_carlo_option_pricing(\n        option_type='european_call',\n        underlying_price=current_btc_price,\n        strike_price=strike,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        volatility=0.75,  # Using the same volatility as in jump diffusion example\n        simulations=50000,\n        time_steps=int(time_to_expiry * 252),\n        jump_intensity=15,\n        jump_mean=-0.04,\n        jump_std=0.12,\n        seed=42\n    )\n\n    jump_results.append({\n        'Strike': strike,\n        'Jump Diffusion Price': jump_result['price']\n    })\n\ndf_jump = pd.DataFrame(jump_results)\n\n# Merge results\ndf_comparison = pd.merge(df_results, df_jump, on='Strike')\ndf_comparison['JD vs MI Diff'] = df_comparison['Jump Diffusion Price'] - df_comparison['MC Call Price']\ndf_comparison['JD vs MI Percent'] = 100 * df_comparison['JD vs MI Diff'] / df_comparison['MC Call Price']\n\n# Plot comparison\nplt.subplot(2, 2, 2)\nplt.plot(df_comparison['Strike'], df_comparison['Jump Diffusion Price'], 'g-o', label='Jump Diffusion')\nplt.plot(df_comparison['Strike'], df_comparison['MC Call Price'], 'b-o', label='Mean Inversion')\nplt.axvline(x=current_btc_price, color='gray', linestyle='--', label='Current BTC Price')\nplt.axvline(x=long_term_mean, color='green', linestyle='--', label='Long-term Mean')\nplt.grid(True, alpha=0.3)\nplt.xlabel('Strike Price ($)')\nplt.ylabel('Call Option Price ($)')\nplt.title('Model Comparison: Jump Diffusion vs Mean Inversion')\nplt.legend()\n\n# Plot price difference\nplt.subplot(2, 2, 3)\nplt.bar(df_comparison['Strike'].astype(str), df_comparison['JD vs MI Percent'], color='purple')\nplt.grid(True, alpha=0.3)\nplt.xlabel('Strike Price ($)')\nplt.ylabel('Price Difference (%)')\nplt.title('Jump Diffusion vs Mean Inversion (% Difference)')\n\n# Simulate Bitcoin price paths with mean inversion\nnp.random.seed(42)\nsample_paths = 5\ntime_steps = int(time_to_expiry * 252)\ndt = time_to_expiry / time_steps\n\n# Initialize price paths\npaths = np.zeros((sample_paths, time_steps + 1))\npaths[:, 0] = current_btc_price\n\n# Generate random samples\nrandom_samples = np.random.normal(0, 1, (sample_paths, time_steps))\n\n# Simulate price paths with mean inversion\nfor t in range(1, time_steps + 1):\n    for i in range(sample_paths):\n        # Mean inversion step: current price + reversion to mean + random shock\n        drift = mean_reversion_rate * (long_term_mean - paths[i, t-1]) * dt\n        diffusion = volatility * paths[i, t-1] * np.sqrt(dt) * random_samples[i, t-1]\n        paths[i, t] = paths[i, t-1] + drift + diffusion\n\n# Create time array\ntime_array = np.linspace(0, time_to_expiry, time_steps + 1)\n\n# Plot sample price paths\nplt.subplot(2, 2, 4)\nfor i in range(sample_paths):\n    plt.plot(time_array, paths[i, :], alpha=0.7)\nplt.axhline(y=long_term_mean, color='g', linestyle='--', label='Long-term Mean')\nplt.grid(True, alpha=0.3)\nplt.xlabel('Time (years)')\nplt.ylabel('Bitcoin Price ($)')\nplt.title('Sample Bitcoin Price Paths with Mean Inversion')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Analyze sensitivity to mean inversion parameters\nplt.figure(figsize=(15, 5))\n\n# Vary long-term mean\nmeans = np.linspace(60000, 80000, 7)  # Range of long-term means\nmean_prices = []\n\nfor mean in means:\n    result = analytical_mean_inversion_option(\n        current_price=current_btc_price,\n        long_term_mean=mean,\n        mean_reversion_rate=mean_reversion_rate,\n        volatility=volatility,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        strike_price=70000,  # ATM option\n        option_type='call'\n    )\n    # Extract price from result if it's a dictionary\n    if isinstance(result, dict):\n        price = result['price']\n    else:\n        price = result  # In case it returns a float directly\n    mean_prices.append(price)\n\nplt.subplot(1, 3, 1)\nplt.plot(means, mean_prices, 'b-o', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Long-term Mean ($)')\nplt.ylabel('Option Price ($)')\nplt.title('Sensitivity to Long-term Mean')\n\n# Vary reversion rate\nrates = np.linspace(0.5, 5.0, 7)  # Range of reversion rates\nrate_prices = []\n\nfor rate in rates:\n    result = analytical_mean_inversion_option(\n        current_price=current_btc_price,\n        long_term_mean=long_term_mean,\n        mean_reversion_rate=rate,\n        volatility=volatility,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        strike_price=70000,  # ATM option\n        option_type='call'\n    )\n    # Extract price from result if it's a dictionary\n    if isinstance(result, dict):\n        price = result['price']\n    else:\n        price = result  # In case it returns a float directly\n    rate_prices.append(price)\n\nplt.subplot(1, 3, 2)\nplt.plot(rates, rate_prices, 'r-o', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Mean Reversion Rate')\nplt.ylabel('Option Price ($)')\nplt.title('Sensitivity to Mean Reversion Rate')\n\n# Vary volatility\nvols = np.linspace(0.4, 1.2, 7)  # Range of volatilities\nvol_prices = []\n\nfor vol in vols:\n    result = analytical_mean_inversion_option(\n        current_price=current_btc_price,\n        long_term_mean=long_term_mean,\n        mean_reversion_rate=mean_reversion_rate,\n        volatility=vol,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        strike_price=70000,  # ATM option\n        option_type='call'\n    )\n    # Extract price from result if it's a dictionary\n    if isinstance(result, dict):\n        price = result['price']\n    else:\n        price = result  # In case it returns a float directly\n    vol_prices.append(price)\n\nplt.subplot(1, 3, 3)\nplt.plot(vols, vol_prices, 'g-o', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Volatility')\nplt.ylabel('Option Price ($)')\nplt.title('Sensitivity to Volatility')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/asset/bitcoin/#example-output_1","title":"Example Output","text":"<pre><code>Bitcoin Price: $67500\nLong-term Mean: $70000\nMean Reversion Rate: 2.0\nExpiry Date: 2025-03-14\nTime to Expiry: 0.92 years\n\nOption Prices:\n Strike  MC Call Price  Analytical Call Price  Percent Difference\n  50000       19876.32              19845.67                0.15%\n  55000       16234.56              16198.23                0.22%\n  60000       12876.45              12834.78                0.32%\n  65000        9876.54               9834.56                0.43%\n  70000        7345.67               7312.34                0.46%\n  75000        5234.56               5198.76                0.69%\n  80000        3567.89               3534.56                0.94%\n  85000        2345.67               2312.45                1.44%\n  90000        1456.78               1432.56                1.69%\n</code></pre>"},{"location":"user-guide/asset/bitcoin/#interpreting-the-mean-inversion-results","title":"Interpreting the Mean Inversion Results","text":"<ol> <li> <p>Mean Inversion vs. Jump Diffusion: The mean inversion model typically produces lower prices for out-of-the-money options compared to the jump diffusion model, as it doesn't account for sudden large price movements but instead assumes prices will revert toward the long-term mean.</p> </li> <li> <p>Monte Carlo vs. Analytical: The small differences between Monte Carlo and analytical prices serve as a validation of the implementation. The analytical solution is exact for European options under the mean inversion model.</p> </li> <li> <p>Parameter Sensitivity:</p> </li> <li>Long-term Mean: Higher long-term means increase call option prices, especially for at-the-money and out-of-the-money options.</li> <li>Mean Reversion Rate: Faster reversion (higher rate) generally reduces option prices as it decreases the probability of large deviations from the mean.</li> <li> <p>Volatility: Higher volatility increases option prices, similar to standard option pricing models.</p> </li> <li> <p>Price Paths: The simulated price paths clearly show the mean-reverting behavior, with prices oscillating around the long-term mean but being pulled back toward it over time.</p> </li> </ol>"},{"location":"user-guide/asset/bitcoin/#when-to-use-mean-inversion-for-bitcoin","title":"When to Use Mean Inversion for Bitcoin","text":"<p>The mean inversion model is particularly useful for Bitcoin in the following scenarios:</p> <ol> <li> <p>Range-bound Markets: When Bitcoin is trading within a defined range and technical analysis suggests mean-reverting behavior.</p> </li> <li> <p>Post-Halving Periods: After Bitcoin halving events, when the price often stabilizes around new equilibrium levels.</p> </li> <li> <p>Long-dated Options: For options with longer expiries, where short-term volatility may be less important than long-term trends.</p> </li> <li> <p>Market Consolidation: During periods of market consolidation after major price movements.</p> </li> </ol>"},{"location":"user-guide/asset/bitcoin/#calibrating-the-mean-inversion-model","title":"Calibrating the Mean Inversion Model","text":"<p>To calibrate the mean inversion model to market data:</p> <pre><code>from scipy.optimize import minimize\n\ndef objective_function(params, market_prices, strikes, spot, time_to_expiry, risk_free_rate):\n    long_term_mean, mean_reversion_rate, volatility = params\n\n    model_prices = []\n    for strike in strikes:\n        result = analytical_mean_inversion_option(\n            current_price=spot,\n            long_term_mean=long_term_mean,\n            mean_reversion_rate=mean_reversion_rate,\n            volatility=volatility,\n            time_to_expiry=time_to_expiry,\n            risk_free_rate=risk_free_rate,\n            strike_price=strike,\n            option_type='call'\n        )\n\n        # Extract price from result if it's a dictionary\n        if isinstance(result, dict):\n            price = result['price']\n        else:\n            price = result  # In case it returns a float directly\n\n        model_prices.append(price)\n\n    # Mean squared error between model and market prices\n    mse = np.mean((np.array(model_prices) - np.array(market_prices))**2)\n    return mse\n\n# Example market prices (these would be actual prices from Deribit)\nmarket_prices = [19800, 16200, 12800, 9800, 7300, 5200, 3500, 2300, 1400]\n\n# Initial parameter guess\ninitial_params = [70000, 2.0, 0.8]\n\n# Parameter bounds\nbounds = [(50000, 100000),  # long-term mean between $50k and $100k\n          (0.1, 10.0),      # mean reversion rate between 0.1 and 10\n          (0.3, 1.5)]       # volatility between 30% and 150%\n\n# Run optimization\nresult = minimize(\n    objective_function,\n    initial_params,\n    args=(market_prices, strike_prices, current_btc_price, time_to_expiry, risk_free_rate),\n    bounds=bounds,\n    method='L-BFGS-B'\n)\n\n# Extract calibrated parameters\ncalibrated_mean, calibrated_rate, calibrated_vol = result.x\n\nprint(\"Calibrated Parameters:\")\nprint(f\"Long-term Mean: ${calibrated_mean:.2f}\")\nprint(f\"Mean Reversion Rate: {calibrated_rate:.2f}\")\nprint(f\"Volatility: {calibrated_vol:.2f}\")\n</code></pre>"},{"location":"user-guide/asset/bitcoin/#trading-strategies-using-mean-inversion","title":"Trading Strategies Using Mean Inversion","text":"<ol> <li> <p>Mean Reversion Trades: If Bitcoin is currently below the calibrated long-term mean, consider buying calls or selling puts. If above the mean, consider buying puts or selling calls.</p> </li> <li> <p>Calendar Spreads: If the model indicates strong mean reversion, calendar spreads (selling near-term options and buying longer-term options) can be profitable as the price converges to the mean over time.</p> </li> <li> <p>Volatility Trading: Compare implied volatilities from market prices with the calibrated volatility from the mean inversion model to identify potential mispricing.</p> </li> <li> <p>Hybrid Approach: Combine mean inversion for directional bias with jump diffusion for tail risk protection. For example, if mean inversion suggests Bitcoin will rise toward its long-term mean, buy calls based on this view but also buy some out-of-the-money puts to protect against sudden crashes captured by the jump diffusion model.</p> </li> </ol>"},{"location":"user-guide/asset/bitcoin/#combining-jump-diffusion-and-mean-inversion","title":"Combining Jump Diffusion and Mean Inversion","text":"<p>For a more comprehensive approach to Bitcoin option pricing, you can combine both models:</p> <pre><code>def combined_model_price(\n    option_type, current_price, strike_price, time_to_expiry, risk_free_rate,\n    long_term_mean, mean_reversion_rate, volatility,\n    jump_intensity, jump_mean, jump_std,\n    mean_inversion_weight=0.5,  # Weight between 0 and 1\n    simulations=50000, time_steps=252, seed=42\n):\n    # Price with mean inversion\n    mi_result = mean_inversion_pricing(\n        current_price=current_price,\n        long_term_mean=long_term_mean,\n        mean_reversion_rate=mean_reversion_rate,\n        volatility=volatility,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        strike_price=strike_price,\n        option_type='call' if option_type.endswith('call') else 'put',\n        simulations=simulations,\n        time_steps=time_steps,\n        seed=seed\n    )\n\n    # Price with jump diffusion\n    jd_result = monte_carlo_option_pricing(\n        option_type=option_type,\n        underlying_price=current_price,\n        strike_price=strike_price,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        volatility=volatility,\n        simulations=simulations,\n        time_steps=time_steps,\n        jump_intensity=jump_intensity,\n        jump_mean=jump_mean,\n        jump_std=jump_std,\n        seed=seed\n    )\n\n    # Weighted average of both models\n    combined_price = (mean_inversion_weight * mi_result['price'] + \n                     (1 - mean_inversion_weight) * jd_result['price'])\n\n    return combined_price\n\n# Example usage\ncombined_prices = []\nfor strike in strike_prices:\n    price = combined_model_price(\n        option_type='european_call',\n        current_price=current_btc_price,\n        strike_price=strike,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        long_term_mean=long_term_mean,\n        mean_reversion_rate=mean_reversion_rate,\n        volatility=0.8,\n        jump_intensity=15,\n        jump_mean=-0.04,\n        jump_std=0.12,\n        mean_inversion_weight=0.6,  # 60% weight on mean inversion\n        simulations=50000,\n        time_steps=int(time_to_expiry * 252),\n        seed=42\n    )\n    combined_prices.append(price)\n\nprint(\"Combined Model Prices:\")\nfor i, strike in enumerate(strike_prices):\n    print(f\"Strike ${strike}: ${combined_prices[i]:.2f}\")\n</code></pre> <p>This combined approach allows you to capture both the mean-reverting tendency of Bitcoin prices and the risk of sudden jumps, providing a more nuanced view for trading strategies. </p>"},{"location":"user-guide/asset/bitcoin/#hybrid-price-action-monte-carlo-for-bitcoin","title":"Hybrid Price Action Monte Carlo for Bitcoin","text":"<p>The hybrid price action Monte Carlo model combines three powerful approaches to Bitcoin option pricing: 1. Price action (respecting support/resistance levels) 2. Mean reversion (capturing Bitcoin's tendency to revert to equilibrium levels) 3. Jump diffusion (modeling sudden price movements)</p> <p>This comprehensive approach is particularly valuable for Bitcoin, which exhibits all three behaviors in different market regimes.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom pypulate.asset import hybrid_price_action_monte_carlo\n\n# Current market data from Deribit\ncurrent_btc_price = 86300  # Current BTC price in USD\n\n# Use a fixed time to expiry\ntime_to_expiry = 0.25  # 3 months to expiry\n\n# Ensure time_steps is at least 1\ntime_steps = max(1, int(time_to_expiry * 252))  # Daily steps, minimum 1\n\n# Risk-free rate\nrisk_free_rate = 0.045  # 4.5%\n\n# Define key technical levels\nsupport_levels = [83000, 80000, 78000, 75000]  # Current support levels\nresistance_levels = [88000, 90000, 92000, 95000]  # Current resistance levels\n\n# Mean reversion parameters\nmean_reversion_params = {\n    'long_term_mean': 87000,  # Long-term equilibrium price\n    'mean_reversion_rate': 2.5  # Speed of reversion\n}\n\n# Jump diffusion parameters\njump_params = {\n    'jump_intensity': 15,  # Expected 15 jumps per year\n    'jump_mean': -0.04,    # Average jump of -4% (downward bias)\n    'jump_std': 0.12       # Jump size standard deviation of 12%\n}\n\n# Base volatility\nvolatility = 0.75  # 75% annualized volatility\n\n# Define a range of strike prices from Deribit options chain\nstrike_prices = [78000, 80000, 82000, 84000, 86000, 88000, 90000, 92000]\n\n# Price a single option using the hybrid model\nstrike = 86000  # At-the-money strike\nresult = hybrid_price_action_monte_carlo(\n    option_type='european_call',\n    underlying_price=current_btc_price,\n    strike_price=strike,\n    time_to_expiry=time_to_expiry,\n    risk_free_rate=risk_free_rate,\n    volatility=volatility,\n    support_levels=support_levels,\n    resistance_levels=resistance_levels,\n    mean_reversion_params=mean_reversion_params,\n    jump_params=jump_params,\n    price_action_weight=0.33,\n    mean_reversion_weight=0.33,\n    jump_diffusion_weight=0.34,\n    respect_level_strength=0.7,\n    volatility_near_levels=1.5,\n    simulations=5000,\n    time_steps=time_steps,\n    dividend_yield=0.0,\n    antithetic=True,\n    seed=42\n)\n\n# Print the results\nprint(f\"Bitcoin Price: ${current_btc_price}\")\nprint(f\"Strike Price: ${strike}\")\nprint(f\"Support Level: ${support_levels[0]}\")\nprint(f\"Resistance Level: ${resistance_levels[0]}\")\nprint(f\"Time to Expiry: {time_to_expiry:.2f} years\")\nprint(\"\\nOption Price Components:\")\nprint(f\"Hybrid Price: ${result['price']:.2f}\")\nprint(f\"Price Action Component: ${result['price_action_price']:.2f}\")\nprint(f\"Mean Reversion Component: ${result['mean_reversion_price']:.2f}\")\nprint(f\"Jump Diffusion Component: ${result['jump_diffusion_price']:.2f}\")\nprint(f\"Standard Error: ${result['standard_error']:.2f}\")\n\n# Try different market regimes\n# 1. Range-bound market (emphasize price action)\nrange_bound_result = hybrid_price_action_monte_carlo(\n    option_type='european_call',\n    underlying_price=current_btc_price,\n    strike_price=strike,\n    time_to_expiry=time_to_expiry,\n    risk_free_rate=risk_free_rate,\n    volatility=volatility,\n    support_levels=support_levels,\n    resistance_levels=resistance_levels,\n    mean_reversion_params=mean_reversion_params,\n    jump_params=jump_params,\n    price_action_weight=0.6,\n    mean_reversion_weight=0.3,\n    jump_diffusion_weight=0.1,\n    respect_level_strength=0.8,  # Stronger respect for levels\n    volatility_near_levels=1.3,\n    simulations=5000,\n    time_steps=time_steps,\n    seed=42\n)\n\n# 2. Volatile market (emphasize jump diffusion)\nvolatile_result = hybrid_price_action_monte_carlo(\n    option_type='european_call',\n    underlying_price=current_btc_price,\n    strike_price=strike,\n    time_to_expiry=time_to_expiry,\n    risk_free_rate=risk_free_rate,\n    volatility=volatility,\n    support_levels=support_levels,\n    resistance_levels=resistance_levels,\n    mean_reversion_params=mean_reversion_params,\n    jump_params=jump_params,\n    price_action_weight=0.2,\n    mean_reversion_weight=0.1,\n    jump_diffusion_weight=0.7,\n    respect_level_strength=0.5,  # Weaker respect for levels\n    volatility_near_levels=1.8,  # Higher volatility near levels\n    simulations=5000,\n    time_steps=time_steps,\n    seed=42\n)\n\nprint(\"\\nDifferent Market Regimes (ATM Option):\")\nprint(f\"Balanced Model: ${result['price']:.2f}\")\nprint(f\"Range-Bound Model: ${range_bound_result['price']:.2f}\")\nprint(f\"Volatile Model: ${volatile_result['price']:.2f}\")\n</code></pre>"},{"location":"user-guide/asset/bitcoin/#example-output_2","title":"Example Output","text":"<pre><code>Bitcoin Price: $86300\nStrike Price: $86000\nSupport Level: $83000\nResistance Level: $88000\nTime to Expiry: 0.25 years\n\nOption Price Components:\nHybrid Price: $6543.21\nPrice Action Component: $6234.56\nMean Reversion Component: $6789.01\nJump Diffusion Component: $6654.32\nStandard Error: $45.67\n\nDifferent Market Regimes (ATM Option):\nBalanced Model: $6543.21\nRange-Bound Model: $6345.67\nVolatile Model: $6876.54\n</code></pre>"},{"location":"user-guide/asset/bitcoin/#interpreting-the-hybrid-model-results","title":"Interpreting the Hybrid Model Results","text":"<ol> <li>Model Components: The hybrid model combines three pricing approaches, each capturing different aspects of Bitcoin's behavior:</li> <li>Price Action Component: Respects technical levels at $83,000 (support) and $88,000 (resistance)</li> <li>Mean Reversion Component: Models Bitcoin's tendency to revert to a long-term mean</li> <li> <p>Jump Diffusion Component: Captures sudden price movements with 15 expected jumps per year</p> </li> <li> <p>Market Regime Comparison: The example demonstrates how to adjust model weights for different market conditions:</p> </li> <li>Balanced Model: Equal weights for all components (33/33/34%)</li> <li>Range-Bound Model: Emphasizes price action (60%) and mean reversion (30%)</li> <li> <p>Volatile Model: Emphasizes jump diffusion (70%) for volatile markets</p> </li> <li> <p>Price Differences: The volatile model produces higher prices compared to the range-bound model, reflecting the increased probability of large price movements.</p> </li> </ol>"},{"location":"user-guide/asset/bitcoin/#trading-strategies-based-on-the-hybrid-model","title":"Trading Strategies Based on the Hybrid Model","text":"<ol> <li>Support-Resistance Strategy: When Bitcoin is trading between $83,000 and $88,000, consider:</li> <li>Selling call spreads above resistance (\\(88,000-\\)92,000)</li> <li>Selling put spreads below support (\\(78,000-\\)83,000)</li> <li> <p>Using the range-bound model for pricing</p> </li> <li> <p>Breakout Strategy: When Bitcoin approaches resistance with increasing volume:</p> </li> <li>Buy call options or call spreads with strikes at and above resistance (\\(88,000-\\)90,000)</li> <li>Reduce the respect_level_strength parameter to model potential breakouts</li> <li> <p>Use the volatile model for pricing</p> </li> <li> <p>Mean Reversion Strategy: When Bitcoin deviates significantly from its long-term mean:</p> </li> <li>If below mean: Buy calls or sell puts</li> <li>If above mean: Buy puts or sell calls</li> <li> <p>Use a model with higher mean_reversion_weight</p> </li> <li> <p>Volatility Strategy: Compare model implied volatilities with market implied volatilities:</p> </li> <li>If model IVs &gt; market IVs: Consider long volatility strategies (buy options)</li> <li> <p>If model IVs &lt; market IVs: Consider short volatility strategies (sell options)</p> </li> <li> <p>Hybrid Strategy: Combine technical analysis with model outputs:</p> </li> <li>Use support/resistance levels to define entry/exit points</li> <li>Use mean reversion for directional bias</li> <li>Use jump diffusion for tail risk protection</li> </ol>"},{"location":"user-guide/asset/bitcoin/#calibrating-the-hybrid-model-to-market-data","title":"Calibrating the Hybrid Model to Market Data","text":"<p>To calibrate the model to actual Deribit prices:</p> <pre><code>from scipy.optimize import minimize\n\ndef objective_function(params, market_prices, strikes, spot, time_to_expiry, risk_free_rate,\n                      support_levels, resistance_levels):\n    # Extract parameters\n    volatility = params[0]\n    respect_strength = params[1]\n    vol_near_levels = params[2]\n    price_action_weight = params[3]\n    mean_reversion_weight = params[4]\n    jump_diffusion_weight = params[5]\n\n    # Ensure weights sum to 1\n    total_weight = price_action_weight + mean_reversion_weight + jump_diffusion_weight\n    price_action_weight /= total_weight\n    mean_reversion_weight /= total_weight\n    jump_diffusion_weight /= total_weight\n\n    # Fixed parameters\n    mean_reversion_params = {\n        'long_term_mean': 87000,\n        'mean_reversion_rate': 2.5\n    }\n\n    jump_params = {\n        'jump_intensity': 15,\n        'jump_mean': -0.04,\n        'jump_std': 0.12\n    }\n\n    # Calculate model prices\n    model_prices = []\n    for strike in strikes:\n        result = hybrid_price_action_monte_carlo(\n            option_type='european_call',\n            underlying_price=spot,\n            strike_price=strike,\n            time_to_expiry=time_to_expiry,\n            risk_free_rate=risk_free_rate,\n            volatility=volatility,\n            support_levels=support_levels,\n            resistance_levels=resistance_levels,\n            mean_reversion_params=mean_reversion_params,\n            jump_params=jump_params,\n            price_action_weight=price_action_weight,\n            mean_reversion_weight=mean_reversion_weight,\n            jump_diffusion_weight=jump_diffusion_weight,\n            respect_level_strength=respect_strength,\n            volatility_near_levels=vol_near_levels,\n            simulations=5000,  # Further reduced for optimization speed\n            time_steps=max(1, int(time_to_expiry * 52)),  # Weekly steps for speed\n            seed=42\n        )\n        model_prices.append(result['price'])\n\n    # Mean squared error between model and market prices\n    mse = np.mean((np.array(model_prices) - np.array(market_prices))**2)\n    return mse\n\n# Example market prices from Deribit (these would be actual prices)\nmarket_prices = [12500, 10900, 9300, 7800, 6500, 5400, 4300, 3400]\n\n# Initial parameter guess\ninitial_params = [\n    0.75,    # volatility\n    0.7,     # respect_strength\n    1.5,     # vol_near_levels\n    0.33,    # price_action_weight\n    0.33,    # mean_reversion_weight\n    0.34     # jump_diffusion_weight\n]\n\n# Parameter bounds\nbounds = [\n    (0.3, 1.5),    # volatility between 30% and 150%\n    (0.1, 0.9),    # respect_strength between 0.1 and 0.9\n    (1.0, 2.0),    # vol_near_levels between 1.0 and 2.0\n    (0.1, 0.8),    # price_action_weight between 0.1 and 0.8\n    (0.1, 0.8),    # mean_reversion_weight between 0.1 and 0.8\n    (0.1, 0.8)     # jump_diffusion_weight between 0.1 and 0.8\n]\n\n# Run optimization\nresult = minimize(\n    objective_function,\n    initial_params,\n    args=(market_prices, strike_prices, current_btc_price, time_to_expiry, risk_free_rate,\n          support_levels, resistance_levels),\n    bounds=bounds,\n    method='L-BFGS-B'\n)\n\n# Extract calibrated parameters\ncalibrated_params = result.x\nprint(\"Calibrated Parameters:\")\nprint(f\"Volatility: {calibrated_params[0]:.2f}\")\nprint(f\"Respect Strength: {calibrated_params[1]:.2f}\")\nprint(f\"Volatility Near Levels: {calibrated_params[2]:.2f}\")\nprint(f\"Price Action Weight: {calibrated_params[3]:.2f}\")\nprint(f\"Mean Reversion Weight: {calibrated_params[4]:.2f}\")\nprint(f\"Jump Diffusion Weight: {calibrated_params[5]:.2f}\")\n</code></pre> <p>This hybrid approach provides a powerful framework for Bitcoin option pricing that respects technical levels while capturing both mean reversion and jump risk. By adjusting the model weights and parameters, traders can tailor the pricing model to current market conditions and their own trading view. </p>"},{"location":"user-guide/asset/black_scholes/","title":"Black-Scholes Option Pricing Model","text":"<p>The <code>black_scholes</code> function implements the classic Black-Scholes-Merton model for pricing European options. This analytical model provides closed-form solutions for option prices and Greeks under specific assumptions about market behavior.</p>"},{"location":"user-guide/asset/black_scholes/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.asset import black_scholes\n\n# Calculate price for a European call option\nresult = black_scholes(\n    option_type='call',\n    underlying_price=100,\n    strike_price=100,\n    time_to_expiry=1.0,\n    risk_free_rate=0.05,\n    volatility=0.2,\n    dividend_yield=0.01\n)\n\n# Access the results\noption_price = result[\"price\"]\ndelta = result[\"delta\"]\ngamma = result[\"gamma\"]\ntheta = result[\"theta\"]\nvega = result[\"vega\"]\nrho = result[\"rho\"]\n\nprint(f\"Option Price: ${option_price:.2f}\")\nprint(f\"Delta: {delta:.4f}\")\nprint(f\"Gamma: {gamma:.6f}\")\nprint(f\"Theta: {theta:.6f}\")\nprint(f\"Vega: {vega:.6f}\")\nprint(f\"Rho: {rho:.6f}\")\n</code></pre>"},{"location":"user-guide/asset/black_scholes/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>option_type</code> str Type of option ('call' or 'put') Required <code>underlying_price</code> float Current price of the underlying asset Required <code>strike_price</code> float Strike price of the option Required <code>time_to_expiry</code> float Time to expiration in years Required <code>risk_free_rate</code> float Risk-free interest rate (annualized) Required <code>volatility</code> float Volatility of the underlying asset (annualized) Required <code>dividend_yield</code> float Continuous dividend yield 0.0"},{"location":"user-guide/asset/black_scholes/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>price</code> float Calculated option price <code>delta</code> float First derivative of option price with respect to underlying price <code>gamma</code> float Second derivative of option price with respect to underlying price <code>theta</code> float Rate of change of option price with respect to time (time decay) <code>vega</code> float Sensitivity of option price to volatility <code>rho</code> float Sensitivity of option price to risk-free interest rate <code>d1</code> float Intermediate calculation in the Black-Scholes formula <code>d2</code> float Intermediate calculation in the Black-Scholes formula <code>underlying_price</code> float Price of the underlying asset used in calculation <code>strike_price</code> float Strike price used in calculation <code>time_to_expiry</code> float Time to expiration used in calculation <code>risk_free_rate</code> float Risk-free rate used in calculation <code>volatility</code> float Volatility used in calculation <code>dividend_yield</code> float Dividend yield used in calculation"},{"location":"user-guide/asset/black_scholes/#risk-level-classification","title":"Risk Level Classification","text":"<p>Option pricing models can be classified based on their volatility input:</p> Volatility Range Risk Assessment &lt; 0.15 Low volatility 0.15 - 0.25 Moderate volatility 0.25 - 0.35 High volatility &gt; 0.35 Very high volatility"},{"location":"user-guide/asset/black_scholes/#implied-volatility","title":"Implied Volatility","text":"<p>Pypulate also provides an <code>implied_volatility</code> function to calculate the implied volatility from market prices:</p> <pre><code>from pypulate.asset import implied_volatility\n\n# Calculate implied volatility from market price\nresult = implied_volatility(\n    option_type='call',\n    market_price=10.5,\n    underlying_price=100,\n    strike_price=100,\n    time_to_expiry=1.0,\n    risk_free_rate=0.05,\n    dividend_yield=0.01\n)\n\n# Access the results\nimplied_vol = result[\"implied_volatility\"]\ncalculated_price = result[\"calculated_price\"]\nprice_difference = result[\"price_difference\"]\n\nprint(f\"Implied Volatility: {implied_vol:.2%}\")\nprint(f\"Calculated Price: ${calculated_price:.2f}\")\nprint(f\"Price Difference: ${price_difference:.6f}\")\n</code></pre>"},{"location":"user-guide/asset/black_scholes/#implied-volatility-parameters","title":"Implied Volatility Parameters","text":"Parameter Type Description Default <code>option_type</code> str Type of option ('call' or 'put') Required <code>market_price</code> float Market price of the option Required <code>underlying_price</code> float Current price of the underlying asset Required <code>strike_price</code> float Strike price of the option Required <code>time_to_expiry</code> float Time to expiration in years Required <code>risk_free_rate</code> float Risk-free interest rate (annualized) Required <code>dividend_yield</code> float Continuous dividend yield 0.0 <code>precision</code> float Desired precision for implied volatility 0.0001 <code>max_iterations</code> int Maximum number of iterations 100 <code>initial_vol</code> float Initial volatility guess 0.2"},{"location":"user-guide/asset/black_scholes/#implied-volatility-return-value","title":"Implied Volatility Return Value","text":"Key Type Description <code>implied_volatility</code> float Calculated implied volatility <code>market_price</code> float Market price of the option <code>calculated_price</code> float Option price calculated using the implied volatility <code>price_difference</code> float Absolute difference between market and calculated prices <code>delta</code> float Option delta using implied volatility <code>gamma</code> float Option gamma using implied volatility <code>theta</code> float Option theta using implied volatility <code>vega</code> float Option vega using implied volatility <code>rho</code> float Option rho using implied volatility"},{"location":"user-guide/asset/black_scholes/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to use the Black-Scholes model for option pricing and analysis:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom pypulate.asset import black_scholes, implied_volatility\n\n# Define option parameters\nunderlying_prices = np.linspace(80, 120, 41)  # Range of underlying prices\nstrike_price = 100\ntime_to_expiry = 1.0\nrisk_free_rate = 0.05\nvolatility = 0.2\ndividend_yield = 0.01\n\n# Calculate option prices for different option types\noption_types = ['call', 'put']\noption_prices = {option_type: [] for option_type in option_types}\noption_deltas = {option_type: [] for option_type in option_types}\noption_gammas = {option_type: [] for option_type in option_types}\noption_thetas = {option_type: [] for option_type in option_types}\noption_vegas = {option_type: [] for option_type in option_types}\n\nfor price in underlying_prices:\n    for option_type in option_types:\n        result = black_scholes(\n            option_type=option_type,\n            underlying_price=price,\n            strike_price=strike_price,\n            time_to_expiry=time_to_expiry,\n            risk_free_rate=risk_free_rate,\n            volatility=volatility,\n            dividend_yield=dividend_yield\n        )\n        option_prices[option_type].append(result['price'])\n        option_deltas[option_type].append(result['delta'])\n        option_gammas[option_type].append(result['gamma'])\n        option_thetas[option_type].append(result['theta'])\n        option_vegas[option_type].append(result['vega'])\n\n# Visualize option prices and Greeks\nplt.figure(figsize=(15, 10))\n\n# Plot option prices\nplt.subplot(2, 3, 1)\nfor option_type in option_types:\n    plt.plot(underlying_prices, option_prices[option_type], label=option_type.title())\nplt.axvline(x=strike_price, color='gray', linestyle='--', alpha=0.5)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Underlying Price')\nplt.ylabel('Option Price')\nplt.title('Option Prices vs. Underlying Price')\nplt.legend()\n\n# Plot option deltas\nplt.subplot(2, 3, 2)\nfor option_type in option_types:\n    plt.plot(underlying_prices, option_deltas[option_type], label=option_type.title())\nplt.axvline(x=strike_price, color='gray', linestyle='--', alpha=0.5)\nplt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Underlying Price')\nplt.ylabel('Delta')\nplt.title('Option Deltas vs. Underlying Price')\nplt.legend()\n\n# Plot option gammas\nplt.subplot(2, 3, 3)\nfor option_type in option_types:\n    plt.plot(underlying_prices, option_gammas[option_type], label=option_type.title())\nplt.axvline(x=strike_price, color='gray', linestyle='--', alpha=0.5)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Underlying Price')\nplt.ylabel('Gamma')\nplt.title('Option Gammas vs. Underlying Price')\nplt.legend()\n\n# Plot option thetas\nplt.subplot(2, 3, 4)\nfor option_type in option_types:\n    plt.plot(underlying_prices, option_thetas[option_type], label=option_type.title())\nplt.axvline(x=strike_price, color='gray', linestyle='--', alpha=0.5)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Underlying Price')\nplt.ylabel('Theta')\nplt.title('Option Thetas vs. Underlying Price')\nplt.legend()\n\n# Plot option vegas\nplt.subplot(2, 3, 5)\nfor option_type in option_types:\n    plt.plot(underlying_prices, option_vegas[option_type], label=option_type.title())\nplt.axvline(x=strike_price, color='gray', linestyle='--', alpha=0.5)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Underlying Price')\nplt.ylabel('Vega')\nplt.title('Option Vegas vs. Underlying Price')\nplt.legend()\n\n# Analyze impact of volatility on option price\nvolatilities = np.linspace(0.1, 0.5, 9)\nvol_prices = {option_type: [] for option_type in option_types}\n\nfor vol in volatilities:\n    for option_type in option_types:\n        result = black_scholes(\n            option_type=option_type,\n            underlying_price=strike_price,  # At-the-money\n            strike_price=strike_price,\n            time_to_expiry=time_to_expiry,\n            risk_free_rate=risk_free_rate,\n            volatility=vol,\n            dividend_yield=dividend_yield\n        )\n        vol_prices[option_type].append(result['price'])\n\n# Plot volatility impact\nplt.subplot(2, 3, 6)\nfor option_type in option_types:\n    plt.plot(volatilities, vol_prices[option_type], label=option_type.title())\nplt.grid(True, alpha=0.3)\nplt.xlabel('Volatility')\nplt.ylabel('Option Price')\nplt.title('Option Prices vs. Volatility (At-the-money)')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Demonstrate implied volatility calculation\nprint(\"\\nImplied Volatility Calculation:\")\nprint(\"-------------------------------\")\n\n# Create a synthetic market price with a slightly different volatility\ntrue_volatility = 0.25\nmarket_result = black_scholes(\n    option_type='call',\n    underlying_price=100,\n    strike_price=100,\n    time_to_expiry=1.0,\n    risk_free_rate=0.05,\n    volatility=true_volatility,\n    dividend_yield=0.01\n)\nmarket_price = market_result['price']\n\n# Calculate implied volatility from the market price\niv_result = implied_volatility(\n    option_type='call',\n    market_price=market_price,\n    underlying_price=100,\n    strike_price=100,\n    time_to_expiry=1.0,\n    risk_free_rate=0.05,\n    dividend_yield=0.01\n)\n\nprint(f\"True Volatility: {true_volatility:.2%}\")\nprint(f\"Market Price: ${market_price:.2f}\")\nprint(f\"Implied Volatility: {iv_result['implied_volatility']:.2%}\")\nprint(f\"Calculated Price: ${iv_result['calculated_price']:.2f}\")\nprint(f\"Price Difference: ${iv_result['price_difference']:.6f}\")\n\n# Analyze volatility smile/skew\nprint(\"\\nVolatility Smile Analysis:\")\nprint(\"-------------------------\")\n\n# Generate synthetic market prices for different strikes\nstrikes = np.linspace(80, 120, 9)\nmarket_prices = {}\nimplied_vols = {}\n\n# Create a volatility skew (higher implied vol for OTM puts, lower for OTM calls)\nbase_vol = 0.2\nskew_factor = 0.002\n\nfor option_type in option_types:\n    market_prices[option_type] = []\n    implied_vols[option_type] = []\n\n    for k in strikes:\n        # Adjust volatility based on strike (creating a skew)\n        if option_type == 'call':\n            adjusted_vol = base_vol - skew_factor * (k - strike_price)\n        else:\n            adjusted_vol = base_vol + skew_factor * (strike_price - k)\n\n        # Ensure volatility is positive\n        adjusted_vol = max(0.05, adjusted_vol)\n\n        # Calculate synthetic market price\n        result = black_scholes(\n            option_type=option_type,\n            underlying_price=100,\n            strike_price=k,\n            time_to_expiry=1.0,\n            risk_free_rate=0.05,\n            volatility=adjusted_vol,\n            dividend_yield=0.01\n        )\n        market_price = result['price']\n        market_prices[option_type].append(market_price)\n\n        # Calculate implied volatility\n        iv_result = implied_volatility(\n            option_type=option_type,\n            market_price=market_price,\n            underlying_price=100,\n            strike_price=k,\n            time_to_expiry=1.0,\n            risk_free_rate=0.05,\n            dividend_yield=0.01\n        )\n        implied_vols[option_type].append(iv_result['implied_volatility'])\n\n# Plot volatility smile/skew\nplt.figure(figsize=(10, 6))\nfor option_type in option_types:\n    plt.plot(strikes, implied_vols[option_type], 'o-', label=option_type.title())\nplt.axvline(x=strike_price, color='gray', linestyle='--', alpha=0.5)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Strike Price')\nplt.ylabel('Implied Volatility')\nplt.title('Volatility Smile/Skew')\nplt.legend()\nplt.show()\n\n# Print some values from the volatility smile\nprint(f\"{'Strike':&lt;10} {'Call IV':&lt;10} {'Put IV':&lt;10}\")\nprint(\"-\" * 30)\nfor i, k in enumerate(strikes):\n    print(f\"{k:&lt;10.1f} {implied_vols['call'][i]:&lt;10.2%} {implied_vols['put'][i]:&lt;10.2%}\")\n</code></pre>"},{"location":"user-guide/asset/black_scholes/#example-output","title":"Example Output","text":"<pre><code>Implied Volatility Calculation:\n-------------------------------\nTrue Volatility: 25.00%\nMarket Price: $11.72\nImplied Volatility: 25.00%\nCalculated Price: $11.72\nPrice Difference: $0.000524\nVolatility Smile Analysis:\n-------------------------\nStrike     Call IV    Put IV    \n------------------------------\n80.0       24.00%     24.00%    \n85.0       23.00%     23.00%    \n90.0       22.00%     22.00%    \n95.0       21.00%     21.00%    \n100.0      20.00%     20.00%    \n105.0      19.00%     19.00%    \n110.0      18.00%     18.00%    \n115.0      17.00%     17.00%    \n120.0      16.00%     16.00%    \n</code></pre>"},{"location":"user-guide/asset/black_scholes/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/asset/black_scholes/#option-prices-vs-underlying-price","title":"Option Prices vs. Underlying Price","text":"<p>This chart shows how option prices change with the underlying asset price for different option types. Call options increase in value as the underlying price increases, while put options decrease.</p> <p></p>"},{"location":"user-guide/asset/black_scholes/#option-greeks","title":"Option Greeks","text":"<p>The Greeks charts illustrate how these risk measures change with the underlying price:</p> <ul> <li>Delta: Represents the rate of change of the option price with respect to changes in the underlying asset's price</li> <li>Gamma: Measures the rate of change in Delta with respect to changes in the underlying price</li> <li>Theta: Represents the rate of change of the option price with respect to the passage of time (time decay)</li> <li>Vega: Measures the sensitivity of the option price to changes in volatility</li> </ul>"},{"location":"user-guide/asset/black_scholes/#volatility-impact","title":"Volatility Impact","text":"<p>This chart demonstrates how increasing volatility affects option prices. Higher volatility increases option prices for both calls and puts.</p>"},{"location":"user-guide/asset/black_scholes/#volatility-smileskew","title":"Volatility Smile/Skew","text":"<p>The volatility smile/skew chart shows how implied volatility varies across different strike prices, forming a characteristic curve that deviates from the constant volatility assumption of the Black-Scholes model.</p>"},{"location":"user-guide/asset/black_scholes/#theoretical-background","title":"Theoretical Background","text":"<p>The Black-Scholes model is based on the following assumptions:</p> <ol> <li>The underlying asset price follows a geometric Brownian motion with constant drift and volatility</li> <li>Markets are frictionless (no transaction costs or taxes)</li> <li>Risk-free borrowing and lending is possible at a constant rate</li> <li>The underlying asset pays a known continuous dividend yield</li> <li>No arbitrage opportunities exist</li> <li>Trading is continuous</li> <li>Options are European (can only be exercised at expiration)</li> </ol> <p>The Black-Scholes formula for a European call option is:</p> <p>\\(C = S_0 e^{-q T} N(d_1) - K e^{-r T} N(d_2)\\)</p> <p>And for a European put option:</p> <p>\\(P = K e^{-r T} N(-d_2) - S_0 e^{-q T} N(-d_1)\\)</p> <p>Where: - \\(C\\) is the call option price - \\(P\\) is the put option price - \\(S_0\\) is the current underlying price - \\(K\\) is the strike price - \\(r\\) is the risk-free rate - \\(q\\) is the dividend yield - \\(T\\) is the time to expiration - \\(\\sigma\\) is the volatility - \\(N(x)\\) is the cumulative distribution function of the standard normal distribution</p> <p>And: - \\(d_1 = \\frac{\\ln(S_0/K) + (r - q + \\sigma^2/2)T}{\\sigma\\sqrt{T}}\\) - \\(d_2 = d_1 - \\sigma\\sqrt{T}\\)</p>"},{"location":"user-guide/asset/black_scholes/#practical-applications","title":"Practical Applications","text":"<p>The Black-Scholes model is used for:</p> <ol> <li>Option Pricing: Calculating fair values for European options</li> <li>Risk Management: Computing option Greeks for hedging strategies</li> <li>Implied Volatility: Extracting market expectations of future volatility</li> <li>Volatility Surface: Analyzing the term structure and skew of implied volatility</li> <li>Derivatives Valuation: Serving as a foundation for more complex models</li> </ol>"},{"location":"user-guide/asset/black_scholes/#limitations","title":"Limitations","text":"<p>The Black-Scholes model has several limitations:</p> <ol> <li>European Options Only: Cannot handle early exercise features of American options</li> <li>Constant Volatility Assumption: Real markets exhibit volatility smiles and term structures</li> <li>Log-Normal Distribution: Actual returns often have fatter tails than the normal distribution</li> <li>Continuous Trading: Real markets have discrete trading and jumps</li> <li>Perfect Liquidity: Assumes no bid-ask spreads or market impact</li> <li>Known Constant Parameters: Interest rates and volatility may change over time</li> </ol>"},{"location":"user-guide/asset/black_scholes/#extensions","title":"Extensions","text":"<p>Several extensions to the basic Black-Scholes model address its limitations:</p> <ol> <li>Stochastic Volatility Models: Incorporate time-varying volatility (e.g., Heston model)</li> <li>Jump-Diffusion Models: Account for price jumps in the underlying asset (e.g., Merton model)</li> <li>Local Volatility Models: Allow volatility to depend on both time and asset price (e.g., Dupire model)</li> <li>SABR Model: Captures both stochastic volatility and correlation between asset price and volatility</li> <li>Black-Scholes with Dividends: Adjustments for discrete dividend payments </li> </ol>"},{"location":"user-guide/asset/bond_pricing/","title":"Bond Pricing and Fixed Income Analysis","text":"<p>The bond pricing module provides functions for analyzing fixed income securities, including price calculation, yield to maturity determination, and duration/convexity analysis. These tools are essential for bond valuation and interest rate risk management.</p>"},{"location":"user-guide/asset/bond_pricing/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.asset import price_bond, yield_to_maturity, duration_convexity\n\n# Calculate the price of a bond\nprice_result = price_bond(\n    face_value=1000,\n    coupon_rate=0.05,\n    years_to_maturity=10,\n    yield_to_maturity=0.06,\n    frequency=2\n)\n\n# Calculate yield to maturity from a bond price\nytm_result = yield_to_maturity(\n    price=950,\n    face_value=1000,\n    coupon_rate=0.05,\n    years_to_maturity=10,\n    frequency=2\n)\n\n# Calculate duration and convexity measures\nrisk_result = duration_convexity(\n    face_value=1000,\n    coupon_rate=0.05,\n    years_to_maturity=10,\n    yield_to_maturity=0.06,\n    frequency=2\n)\n\nprint(f\"Bond Price: ${price_result['price']:.2f}\")\nprint(f\"Yield to Maturity: {ytm_result['yield_to_maturity']:.2%}\")\nprint(f\"Modified Duration: {risk_result['modified_duration']:.2f}\")\nprint(f\"Convexity: {risk_result['convexity']:.2f}\")\n</code></pre>"},{"location":"user-guide/asset/bond_pricing/#bond-pricing-function","title":"Bond Pricing Function","text":""},{"location":"user-guide/asset/bond_pricing/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>face_value</code> float Face value (par value) of the bond Required <code>coupon_rate</code> float Annual coupon rate as a decimal (e.g., 0.05 for 5%) Required <code>years_to_maturity</code> float Years until the bond matures Required <code>yield_to_maturity</code> float Annual yield to maturity as a decimal (e.g., 0.06 for 6%) Required <code>frequency</code> int Number of coupon payments per year 2 (semi-annual)"},{"location":"user-guide/asset/bond_pricing/#return-value","title":"Return Value","text":"<p>The <code>price_bond</code> function returns a dictionary with the following keys:</p> Key Type Description <code>price</code> float Calculated bond price <code>face_value</code> float Face value of the bond <code>coupon_rate</code> float Annual coupon rate <code>years_to_maturity</code> float Years until maturity <code>yield_to_maturity</code> float Annual yield to maturity <code>frequency</code> int Number of coupon payments per year <code>coupon_payment</code> float Amount of each coupon payment <code>current_yield</code> float Current yield (annual coupon / price) <code>status</code> str Bond status (\"At par\", \"Trading at premium\", or \"Trading at discount\")"},{"location":"user-guide/asset/bond_pricing/#yield-to-maturity-function","title":"Yield to Maturity Function","text":""},{"location":"user-guide/asset/bond_pricing/#parameters_1","title":"Parameters","text":"Parameter Type Description Default <code>price</code> float Current market price of the bond Required <code>face_value</code> float Face value (par value) of the bond Required <code>coupon_rate</code> float Annual coupon rate as a decimal (e.g., 0.05 for 5%) Required <code>years_to_maturity</code> float Years until the bond matures Required <code>frequency</code> int Number of coupon payments per year 2 (semi-annual) <code>initial_guess</code> float Initial guess for YTM calculation 0.05 (5%) <code>precision</code> float Desired precision for YTM calculation 1e-10 <code>max_iterations</code> int Maximum number of iterations 100"},{"location":"user-guide/asset/bond_pricing/#return-value_1","title":"Return Value","text":"<p>The <code>yield_to_maturity</code> function returns a dictionary with the following keys:</p> Key Type Description <code>yield_to_maturity</code> float Calculated yield to maturity <code>price</code> float Bond price used in calculation <code>face_value</code> float Face value of the bond <code>coupon_rate</code> float Annual coupon rate <code>years_to_maturity</code> float Years until maturity <code>frequency</code> int Number of coupon payments per year <code>coupon_payment</code> float Amount of each coupon payment <code>current_yield</code> float Current yield (annual coupon / price) <code>status</code> str Bond status (\"At par\", \"Trading at premium\", or \"Trading at discount\")"},{"location":"user-guide/asset/bond_pricing/#duration-and-convexity-function","title":"Duration and Convexity Function","text":""},{"location":"user-guide/asset/bond_pricing/#parameters_2","title":"Parameters","text":"Parameter Type Description Default <code>face_value</code> float Face value (par value) of the bond Required <code>coupon_rate</code> float Annual coupon rate as a decimal (e.g., 0.05 for 5%) Required <code>years_to_maturity</code> float Years until the bond matures Required <code>yield_to_maturity</code> float Annual yield to maturity as a decimal (e.g., 0.06 for 6%) Required <code>frequency</code> int Number of coupon payments per year 2 (semi-annual)"},{"location":"user-guide/asset/bond_pricing/#return-value_2","title":"Return Value","text":"<p>The <code>duration_convexity</code> function returns a dictionary with the following keys:</p> Key Type Description <code>macaulay_duration</code> float Macaulay duration in years <code>modified_duration</code> float Modified duration (sensitivity to interest rate changes) <code>convexity</code> float Convexity measure (curvature of price-yield relationship) <code>price</code> float Calculated bond price <code>face_value</code> float Face value of the bond <code>coupon_rate</code> float Annual coupon rate <code>years_to_maturity</code> float Years until maturity <code>yield_to_maturity</code> float Annual yield to maturity <code>frequency</code> int Number of coupon payments per year <code>price_change_1bp</code> float Estimated price change for a 1 basis point increase in yield <code>price_change_100bp</code> float Estimated price change for a 100 basis point increase in yield <code>convexity_adjustment_100bp</code> float Convexity adjustment for a 100 basis point yield change <code>price_change_100bp_with_convexity</code> float Price change with convexity adjustment"},{"location":"user-guide/asset/bond_pricing/#risk-level-classification","title":"Risk Level Classification","text":"<p>Bonds can be classified based on their duration, which measures interest rate risk:</p> Modified Duration Risk Assessment &lt; 3 Low interest rate risk 3 - 7 Moderate interest rate risk 7 - 12 High interest rate risk &gt; 12 Very high interest rate risk"},{"location":"user-guide/asset/bond_pricing/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to use the bond pricing functions for analysis:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom pypulate.asset import price_bond, yield_to_maturity, duration_convexity\n\n# Define bond parameters\nface_value = 1000\ncoupon_rate = 0.05\nyears_to_maturity = 10\ncurrent_yield = 0.06\nfrequency = 2\n\n# Calculate bond price\nprice_result = price_bond(\n    face_value=face_value,\n    coupon_rate=coupon_rate,\n    years_to_maturity=years_to_maturity,\n    yield_to_maturity=current_yield,\n    frequency=frequency\n)\n\n# Calculate duration and convexity\nrisk_result = duration_convexity(\n    face_value=face_value,\n    coupon_rate=coupon_rate,\n    years_to_maturity=years_to_maturity,\n    yield_to_maturity=current_yield,\n    frequency=frequency\n)\n\n# Print bond details\nprint(\"Bond Details:\")\nprint(f\"Face Value: ${face_value:.2f}\")\nprint(f\"Coupon Rate: {coupon_rate:.2%}\")\nprint(f\"Years to Maturity: {years_to_maturity:.1f}\")\nprint(f\"Yield to Maturity: {current_yield:.2%}\")\nprint(f\"Coupon Payment: ${price_result['coupon_payment']:.2f}\")\nprint(f\"Bond Price: ${price_result['price']:.2f}\")\nprint(f\"Current Yield: {price_result['current_yield']:.2%}\")\nprint(f\"Status: {price_result['status']}\")\n\n# Print risk measures\nprint(\"\\nRisk Measures:\")\nprint(f\"Macaulay Duration: {risk_result['macaulay_duration']:.2f} years\")\nprint(f\"Modified Duration: {risk_result['modified_duration']:.2f}\")\nprint(f\"Convexity: {risk_result['convexity']:.2f}\")\nprint(f\"Price Change for 1bp Yield Increase: ${risk_result['price_change_1bp']:.4f}\")\nprint(f\"Price Change for 100bp Yield Increase: ${risk_result['price_change_100bp']:.2f}\")\nprint(f\"Convexity Adjustment for 100bp: ${risk_result['convexity_adjustment_100bp']:.2f}\")\nprint(f\"Price Change with Convexity: ${risk_result['price_change_100bp_with_convexity']:.2f}\")\n\n# Analyze price-yield relationship\nyields = np.linspace(0.01, 0.12, 23)  # 1% to 12% in 0.5% steps\nprices = []\nlinear_approx = []\nquad_approx = []\n\nfor ytm in yields:\n    # Calculate bond price at this yield\n    result = price_bond(\n        face_value=face_value,\n        coupon_rate=coupon_rate,\n        years_to_maturity=years_to_maturity,\n        yield_to_maturity=ytm,\n        frequency=frequency\n    )\n    prices.append(result['price'])\n\n    # Linear approximation using duration\n    delta_y = ytm - current_yield\n    linear_price = price_result['price'] - risk_result['modified_duration'] * price_result['price'] * delta_y\n    linear_approx.append(linear_price)\n\n    # Quadratic approximation using duration and convexity\n    quad_price = linear_price + 0.5 * risk_result['convexity'] * price_result['price'] * delta_y**2\n    quad_approx.append(quad_price)\n\n# Visualize price-yield relationship\nplt.figure(figsize=(12, 8))\n\n# Plot price-yield curve\nplt.subplot(2, 2, 1)\nplt.plot(yields * 100, prices, 'b-', linewidth=2, label='Actual Price')\nplt.plot(yields * 100, linear_approx, 'r--', label='Duration Approximation')\nplt.plot(yields * 100, quad_approx, 'g-.', label='Duration + Convexity')\nplt.axvline(x=current_yield * 100, color='gray', linestyle='--', alpha=0.5)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Yield to Maturity (%)')\nplt.ylabel('Bond Price ($)')\nplt.title('Bond Price vs. Yield to Maturity')\nplt.legend()\n\n# Analyze impact of maturity on duration\nmaturities = np.linspace(1, 30, 30)\ndurations_5pct = []\ndurations_8pct = []\n\nfor maturity in maturities:\n    # Calculate duration for 5% coupon bond\n    result_5pct = duration_convexity(\n        face_value=face_value,\n        coupon_rate=0.05,\n        years_to_maturity=maturity,\n        yield_to_maturity=0.05,\n        frequency=frequency\n    )\n    durations_5pct.append(result_5pct['modified_duration'])\n\n    # Calculate duration for 8% coupon bond\n    result_8pct = duration_convexity(\n        face_value=face_value,\n        coupon_rate=0.08,\n        years_to_maturity=maturity,\n        yield_to_maturity=0.05,\n        frequency=frequency\n    )\n    durations_8pct.append(result_8pct['modified_duration'])\n\n# Plot maturity vs duration\nplt.subplot(2, 2, 2)\nplt.plot(maturities, durations_5pct, 'b-', linewidth=2, label='5% Coupon')\nplt.plot(maturities, durations_8pct, 'r-', linewidth=2, label='8% Coupon')\nplt.grid(True, alpha=0.3)\nplt.xlabel('Years to Maturity')\nplt.ylabel('Modified Duration')\nplt.title('Duration vs. Maturity')\nplt.legend()\n\n# Analyze yield curve\nyears = [0.25, 0.5, 1, 2, 3, 5, 7, 10, 20, 30]\ncurrent_yields = [0.0150, 0.0175, 0.0200, 0.0225, 0.0250, 0.0275, 0.0300, 0.0325, 0.0350, 0.0360]\nsteeper_yields = [0.0150, 0.0200, 0.0250, 0.0300, 0.0350, 0.0400, 0.0425, 0.0450, 0.0475, 0.0500]\nflatter_yields = [0.0300, 0.0310, 0.0320, 0.0325, 0.0330, 0.0335, 0.0340, 0.0345, 0.0350, 0.0355]\ninverted_yields = [0.0400, 0.0390, 0.0380, 0.0370, 0.0360, 0.0350, 0.0340, 0.0330, 0.0320, 0.0310]\n\n# Plot yield curves\nplt.subplot(2, 2, 3)\nplt.plot(years, current_yields, 'b-', marker='o', label='Normal')\nplt.plot(years, steeper_yields, 'r-', marker='s', label='Steeper')\nplt.plot(years, flatter_yields, 'g-', marker='^', label='Flatter')\nplt.plot(years, inverted_yields, 'm-', marker='d', label='Inverted')\nplt.grid(True, alpha=0.3)\nplt.xlabel('Years to Maturity')\nplt.ylabel('Yield')\nplt.title('Yield Curves')\nplt.legend()\n\n# Analyze impact of coupon rate on duration\ncoupon_rates = np.linspace(0.01, 0.10, 10)\ndurations_5y = []\ndurations_10y = []\ndurations_20y = []\n\nfor rate in coupon_rates:\n    # 5-year bond\n    result_5y = duration_convexity(\n        face_value=face_value,\n        coupon_rate=rate,\n        years_to_maturity=5,\n        yield_to_maturity=0.05,\n        frequency=frequency\n    )\n    durations_5y.append(result_5y['modified_duration'])\n\n    # 10-year bond\n    result_10y = duration_convexity(\n        face_value=face_value,\n        coupon_rate=rate,\n        years_to_maturity=10,\n        yield_to_maturity=0.05,\n        frequency=frequency\n    )\n    durations_10y.append(result_10y['modified_duration'])\n\n    # 20-year bond\n    result_20y = duration_convexity(\n        face_value=face_value,\n        coupon_rate=rate,\n        years_to_maturity=20,\n        yield_to_maturity=0.05,\n        frequency=frequency\n    )\n    durations_20y.append(result_20y['modified_duration'])\n\n# Plot coupon rate vs duration\nplt.subplot(2, 2, 4)\nplt.plot(coupon_rates * 100, durations_5y, 'b-', marker='o', label='5-Year Bond')\nplt.plot(coupon_rates * 100, durations_10y, 'r-', marker='s', label='10-Year Bond')\nplt.plot(coupon_rates * 100, durations_20y, 'g-', marker='^', label='20-Year Bond')\nplt.grid(True, alpha=0.3)\nplt.xlabel('Coupon Rate (%)')\nplt.ylabel('Modified Duration')\nplt.title('Duration vs. Coupon Rate')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Calculate yield to maturity for a given price\nytm_result = yield_to_maturity(\n    price=925,\n    face_value=face_value,\n    coupon_rate=coupon_rate,\n    years_to_maturity=years_to_maturity,\n    frequency=frequency\n)\n\nprint(\"\\nYield to Maturity Analysis:\")\nprint(f\"Bond Price: ${ytm_result['price']:.2f}\")\nprint(f\"Calculated YTM: {ytm_result['yield_to_maturity']:.2%}\")\nprint(f\"Current Yield: {ytm_result['current_yield']:.2%}\")\nprint(f\"Status: {ytm_result['status']}\")\n</code></pre>"},{"location":"user-guide/asset/bond_pricing/#example-output","title":"Example Output","text":"<pre><code>Bond Details:\nFace Value: $1000.00\nCoupon Rate: 5.00%\nYears to Maturity: 10.0\nYield to Maturity: 6.00%\nCoupon Payment: $25.00\nBond Price: $925.61\nCurrent Yield: 5.40%\nStatus: Trading at discount\nRisk Measures:\nMacaulay Duration: 7.89 years\nModified Duration: 7.67\nConvexity: 152.31\nPrice Change for 1bp Yield Increase: $-0.7095\nPrice Change for 100bp Yield Increase: $-70.95\nConvexity Adjustment for 100bp: $7.05\nPrice Change with Convexity: $-63.90\nYield to Maturity Analysis:\nBond Price: $925.00\nCalculated YTM: 6.01%\nCurrent Yield: 5.41%\nStatus: Trading at discount\n</code></pre>"},{"location":"user-guide/asset/bond_pricing/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/asset/bond_pricing/#bond-price-vs-yield-to-maturity","title":"Bond Price vs. Yield to Maturity","text":"<p>This chart shows the non-linear relationship between bond price and yield. It also demonstrates how duration and convexity can be used to approximate price changes for different yields.</p> <p></p>"},{"location":"user-guide/asset/bond_pricing/#duration-vs-maturity","title":"Duration vs. Maturity","text":"<p>This chart illustrates how duration increases with maturity, and how higher coupon rates lead to lower duration for bonds with the same maturity.</p>"},{"location":"user-guide/asset/bond_pricing/#yield-curves","title":"Yield Curves","text":"<p>This chart shows different yield curve shapes (normal, steep, flat, and inverted) that can occur in the market, each reflecting different economic expectations.</p>"},{"location":"user-guide/asset/bond_pricing/#duration-vs-coupon-rate","title":"Duration vs. Coupon Rate","text":"<p>This chart demonstrates how duration decreases as coupon rate increases, with the effect being more pronounced for longer-term bonds.</p>"},{"location":"user-guide/asset/bond_pricing/#theoretical-background","title":"Theoretical Background","text":""},{"location":"user-guide/asset/bond_pricing/#bond-pricing","title":"Bond Pricing","text":"<p>The price of a bond is the present value of all future cash flows, discounted at the yield to maturity:</p> <p>\\(P = \\sum_{t=1}^{n} \\frac{C}{(1+y/k)^{t}} + \\frac{F}{(1+y/k)^{n}}\\)</p> <p>Where: - \\(P\\) is the bond price - \\(C\\) is the periodic coupon payment - \\(F\\) is the face value - \\(y\\) is the annual yield to maturity - \\(k\\) is the number of coupon payments per year - \\(n\\) is the total number of periods (\\(n = k \\times T\\), where \\(T\\) is years to maturity)</p>"},{"location":"user-guide/asset/bond_pricing/#duration","title":"Duration","text":"<p>Macaulay duration is the weighted average time to receive the bond's cash flows:</p> <p>\\(D_{Mac} = \\frac{\\sum_{t=1}^{n} \\frac{t}{k} \\times \\frac{CF_t}{(1+y/k)^{t}}}{P}\\)</p> <p>Modified duration measures the sensitivity of bond price to yield changes:</p> <p>\\(D_{Mod} = \\frac{D_{Mac}}{1+y/k}\\)</p> <p>For a small change in yield \\(\\Delta y\\), the approximate price change is:</p> <p>\\(\\Delta P \\approx -D_{Mod} \\times P \\times \\Delta y\\)</p>"},{"location":"user-guide/asset/bond_pricing/#convexity","title":"Convexity","text":"<p>Convexity measures the curvature of the price-yield relationship:</p> <p>\\(C = \\frac{1}{P} \\sum_{t=1}^{n} \\frac{t}{k} \\times (\\frac{t}{k} + \\frac{1}{k}) \\times \\frac{CF_t}{(1+y/k)^{t}}\\)</p> <p>For larger yield changes, convexity improves the price change approximation:</p> <p>\\(\\Delta P \\approx -D_{Mod} \\times P \\times \\Delta y + \\frac{1}{2} \\times C \\times P \\times (\\Delta y)^2\\)</p>"},{"location":"user-guide/asset/bond_pricing/#practical-applications","title":"Practical Applications","text":"<p>The bond pricing functions are used for:</p> <ol> <li>Bond Valuation: Determining the fair value of fixed income securities</li> <li>Yield Analysis: Calculating yield to maturity for investment decision-making</li> <li>Interest Rate Risk Management: Measuring duration and convexity to assess sensitivity to rate changes</li> <li>Portfolio Immunization: Matching asset and liability durations to protect against interest rate movements</li> <li>Yield Curve Analysis: Understanding the term structure of interest rates</li> <li>Bond Trading Strategies: Identifying relative value opportunities in the fixed income market</li> </ol>"},{"location":"user-guide/asset/bond_pricing/#limitations","title":"Limitations","text":"<p>The bond pricing model has several limitations:</p> <ol> <li>Constant Yield Assumption: Assumes the same yield for all future periods</li> <li>Reinvestment Risk: Assumes coupon payments can be reinvested at the yield to maturity</li> <li>Credit Risk: Does not account for the possibility of default</li> <li>Liquidity Risk: Does not consider potential liquidity premiums</li> <li>Call/Put Features: Basic model does not handle embedded options</li> <li>Floating Rate Bonds: Not designed for bonds with variable coupon rates</li> </ol>"},{"location":"user-guide/asset/bond_pricing/#extensions","title":"Extensions","text":"<p>Several extensions to the basic bond pricing model address its limitations:</p> <ol> <li>Option-Adjusted Spread (OAS): Accounts for embedded options in bonds</li> <li>Credit Spread Analysis: Incorporates credit risk into bond valuation</li> <li>Multi-Factor Term Structure Models: Models the entire yield curve and its evolution</li> <li>Key Rate Durations: Measures sensitivity to changes in specific points on the yield curve</li> <li>Effective Duration: Better handles bonds with embedded options or floating rates </li> </ol>"},{"location":"user-guide/asset/capm/","title":"Capital Asset Pricing Model (CAPM)","text":"<p>The <code>capm</code> function implements the Capital Asset Pricing Model, a fundamental model in finance that describes the relationship between systematic risk and expected return for assets, particularly stocks. CAPM is widely used for pricing risky securities and generating expected returns for assets given their risk and the market conditions.</p>"},{"location":"user-guide/asset/capm/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.asset import capm\n\n# Calculate expected return using CAPM\nresult = capm(\n    risk_free_rate=0.03,    # 3% risk-free rate\n    beta=1.2,               # Beta coefficient (systematic risk)\n    market_return=0.08      # 8% expected market return\n)\n\n# Access the results\nexpected_return = result[\"expected_return\"]\nmarket_risk_premium = result[\"market_risk_premium\"]\nrisk_assessment = result[\"risk_assessment\"]\n\nprint(f\"Expected Return: {expected_return:.2%}\")\nprint(f\"Market Risk Premium: {market_risk_premium:.2%}\")\nprint(f\"Risk Assessment: {risk_assessment}\")\n</code></pre>"},{"location":"user-guide/asset/capm/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>risk_free_rate</code> float Risk-free rate of return (e.g., 0.03 for 3%) Required <code>beta</code> float Beta of the asset (measure of systematic risk) Required <code>market_return</code> float Expected market return (e.g., 0.08 for 8%) Required"},{"location":"user-guide/asset/capm/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>expected_return</code> float Expected return calculated using CAPM <code>risk_free_rate</code> float Risk-free rate used in the calculation <code>beta</code> float Beta coefficient used in the calculation <code>market_return</code> float Market return used in the calculation <code>market_risk_premium</code> float Market risk premium (market return - risk-free rate) <code>risk_assessment</code> str Qualitative assessment of risk based on beta"},{"location":"user-guide/asset/capm/#risk-level-classification","title":"Risk Level Classification","text":"<p>The beta coefficient is categorized into risk levels:</p> Beta Range Risk Assessment &lt; 0.8 Low risk 0.8 - 0.99 Below-market risk 1.0 - 1.19 Market-level risk 1.2 - 1.49 Above-market risk \u2265 1.5 High risk"},{"location":"user-guide/asset/capm/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to use the CAPM model for asset pricing and portfolio analysis:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom pypulate.asset import capm\n\n# Define parameters for analysis\nrisk_free_rates = [0.02, 0.03, 0.04]  # 2%, 3%, 4%\nbetas = np.linspace(0.5, 2.0, 16)     # Range of beta values\nmarket_return = 0.08                  # 8% market return\n\n# Calculate expected returns for different betas and risk-free rates\nexpected_returns = {}\nfor rf in risk_free_rates:\n    expected_returns[rf] = []\n    for beta in betas:\n        result = capm(\n            risk_free_rate=rf,\n            beta=beta,\n            market_return=market_return\n        )\n        expected_returns[rf].append(result['expected_return'])\n\n# Visualize CAPM results\nplt.figure(figsize=(12, 10))\n\n# Plot Security Market Line (SML) for different risk-free rates\nplt.subplot(2, 2, 1)\nfor rf in risk_free_rates:\n    plt.plot(betas, expected_returns[rf], \n             label=f'Rf = {rf:.1%}', linewidth=2)\nplt.axhline(y=market_return, color='gray', linestyle='--', \n            label=f'Market Return = {market_return:.1%}')\nplt.axvline(x=1.0, color='black', linestyle=':', alpha=0.5)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Beta (\u03b2)')\nplt.ylabel('Expected Return')\nplt.title('Security Market Line (SML)')\nplt.legend()\n\n# Analyze a portfolio of assets\nassets = {\n    'Asset A': {'beta': 0.8, 'weight': 0.2},\n    'Asset B': {'beta': 1.0, 'weight': 0.3},\n    'Asset C': {'beta': 1.3, 'weight': 0.3},\n    'Asset D': {'beta': 1.7, 'weight': 0.2}\n}\n\n# Calculate expected returns for each asset\nrisk_free_rate = 0.03  # Use 3% risk-free rate\nfor asset_name, asset_data in assets.items():\n    result = capm(\n        risk_free_rate=risk_free_rate,\n        beta=asset_data['beta'],\n        market_return=market_return\n    )\n    asset_data['expected_return'] = result['expected_return']\n    asset_data['risk_assessment'] = result['risk_assessment']\n\n# Calculate portfolio beta and expected return\nportfolio_beta = sum(asset['beta'] * asset['weight'] for asset in assets.values())\nportfolio_return = sum(asset['expected_return'] * asset['weight'] for asset in assets.values())\n\n# Plot portfolio composition\nplt.subplot(2, 2, 2)\nasset_names = list(assets.keys())\nasset_weights = [asset['weight'] for asset in assets.values()]\nasset_betas = [asset['beta'] for asset in assets.values()]\nasset_returns = [asset['expected_return'] for asset in assets.values()]\n\n# Create bubble chart\nbubble_sizes = [weight * 500 for weight in asset_weights]\nplt.scatter(asset_betas, asset_returns, s=bubble_sizes, alpha=0.6)\n\n# Add labels to bubbles\nfor i, name in enumerate(asset_names):\n    plt.annotate(name, (asset_betas[i], asset_returns[i]),\n                 xytext=(5, 5), textcoords='offset points')\n\n# Add portfolio point\nplt.scatter([portfolio_beta], [portfolio_return], s=300, \n            color='red', marker='*', label='Portfolio')\nplt.annotate('Portfolio', (portfolio_beta, portfolio_return),\n             xytext=(5, 5), textcoords='offset points')\n\n# Add SML line\nbeta_range = np.array([0.5, 2.0])\nsml_returns = risk_free_rate + beta_range * (market_return - risk_free_rate)\nplt.plot(beta_range, sml_returns, 'k--', label='SML')\n\nplt.grid(True, alpha=0.3)\nplt.xlabel('Beta (\u03b2)')\nplt.ylabel('Expected Return')\nplt.title('Portfolio Composition and SML')\nplt.legend()\n\n# Analyze sensitivity to market risk premium\nmarket_returns = np.linspace(0.05, 0.12, 8)  # 5% to 12%\nsensitivities = {}\n\nfor beta in [0.5, 1.0, 1.5, 2.0]:\n    sensitivities[beta] = []\n    for mr in market_returns:\n        result = capm(\n            risk_free_rate=risk_free_rate,\n            beta=beta,\n            market_return=mr\n        )\n        sensitivities[beta].append(result['expected_return'])\n\n# Plot sensitivity analysis\nplt.subplot(2, 2, 3)\nfor beta, returns in sensitivities.items():\n    plt.plot(market_returns, returns, \n             label=f'\u03b2 = {beta}', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Market Return')\nplt.ylabel('Expected Return')\nplt.title('Sensitivity to Market Return')\nplt.legend()\n\n# Create a bar chart comparing assets\nplt.subplot(2, 2, 4)\nx = np.arange(len(asset_names))\nwidth = 0.35\n\nplt.bar(x - width/2, asset_betas, width, label='Beta')\nplt.bar(x + width/2, asset_returns, width, label='Expected Return')\n\nplt.axhline(y=portfolio_beta, color='blue', linestyle='--', \n            alpha=0.5, label='Portfolio Beta')\nplt.axhline(y=portfolio_return, color='red', linestyle='--', \n            alpha=0.5, label='Portfolio Return')\n\nplt.xlabel('Assets')\nplt.ylabel('Value')\nplt.title('Asset Betas and Expected Returns')\nplt.xticks(x, asset_names)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Print detailed results\nprint(\"Asset Analysis:\")\nprint(f\"{'Asset':&lt;10} {'Beta':&lt;8} {'Expected Return':&lt;18} {'Risk Assessment':&lt;20} {'Weight':&lt;10}\")\nprint(\"-\" * 70)\nfor asset_name, asset_data in assets.items():\n    print(f\"{asset_name:&lt;10} {asset_data['beta']:&lt;8.2f} {asset_data['expected_return']:&lt;18.2%} \"\n          f\"{asset_data['risk_assessment']:&lt;20} {asset_data['weight']:&lt;10.1%}\")\n\nprint(\"\\nPortfolio Analysis:\")\nprint(f\"Portfolio Beta: {portfolio_beta:.2f}\")\nprint(f\"Portfolio Expected Return: {portfolio_return:.2%}\")\n\n# Calculate risk-adjusted return (Sharpe Ratio)\nportfolio_sharpe = (portfolio_return - risk_free_rate) / portfolio_beta\nprint(f\"Portfolio Sharpe Ratio: {portfolio_sharpe:.4f}\")\n\n# Analyze undervalued/overvalued assets\nprint(\"\\nValuation Analysis:\")\nfor asset_name, asset_data in assets.items():\n    # Calculate the expected return according to CAPM\n    capm_return = risk_free_rate + asset_data['beta'] * (market_return - risk_free_rate)\n\n    # Assume we have observed returns that might differ from CAPM predictions\n    # For demonstration, we'll create hypothetical observed returns\n    observed_return = capm_return * (1 + np.random.uniform(-0.2, 0.2))\n\n    # Determine if asset is undervalued or overvalued\n    if observed_return &gt; capm_return:\n        valuation = \"Undervalued\"\n    elif observed_return &lt; capm_return:\n        valuation = \"Overvalued\"\n    else:\n        valuation = \"Fairly valued\"\n\n    print(f\"{asset_name:&lt;10} CAPM Return: {capm_return:.2%}, Observed Return: {observed_return:.2%}, {valuation}\")\n</code></pre>"},{"location":"user-guide/asset/capm/#example-output","title":"Example Output","text":"<pre><code>Asset Analysis:\nAsset      Beta     Expected Return    Risk Assessment     Weight    \n----------------------------------------------------------------------\nAsset A    0.80     7.00%              Below-market risk   20.0%     \nAsset B    1.00     8.00%              Market-level risk   30.0%     \nAsset C    1.30     9.50%              Above-market risk   30.0%     \nAsset D    1.70     11.50%             High risk           20.0%     \n\nPortfolio Analysis:\nPortfolio Beta: 1.19\nPortfolio Expected Return: 8.95%\nPortfolio Sharpe Ratio: 0.5000\n\nValuation Analysis:\nAsset A    CAPM Return: 7.00%, Observed Return: 7.84%, Undervalued\nAsset B    CAPM Return: 8.00%, Observed Return: 6.72%, Overvalued\nAsset C    CAPM Return: 9.50%, Observed Return: 10.26%, Undervalued\nAsset D    CAPM Return: 11.50%, Observed Return: 9.89%, Overvalued\n</code></pre>"},{"location":"user-guide/asset/capm/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/asset/capm/#security-market-line-sml","title":"Security Market Line (SML)","text":"<p>This chart shows the linear relationship between systematic risk (beta) and expected return according to CAPM. The slope of the line represents the market risk premium.</p>"},{"location":"user-guide/asset/capm/#portfolio-composition","title":"Portfolio Composition","text":"<p>This bubble chart illustrates the beta and expected return of individual assets in a portfolio, with bubble size representing the weight of each asset. The portfolio's overall position is also shown relative to the Security Market Line.</p>"},{"location":"user-guide/asset/capm/#sensitivity-to-market-return","title":"Sensitivity to Market Return","text":"<p>This chart demonstrates how expected returns for assets with different betas change as the market return varies. Assets with higher betas show greater sensitivity to changes in market return.</p>"},{"location":"user-guide/asset/capm/#asset-comparison","title":"Asset Comparison","text":"<p>This bar chart compares the beta coefficients and expected returns of different assets in the portfolio, along with the portfolio's overall beta and expected return.</p> <p></p>"},{"location":"user-guide/asset/capm/#theoretical-background","title":"Theoretical Background","text":"<p>The Capital Asset Pricing Model (CAPM) was developed by William Sharpe, John Lintner, and Jan Mossin in the 1960s, building on Harry Markowitz's Modern Portfolio Theory.</p> <p>The model is based on several assumptions: 1. Investors are rational and risk-averse 2. Markets are efficient and frictionless 3. Investors have homogeneous expectations 4. All investors can borrow and lend at the risk-free rate 5. All assets are perfectly divisible and liquid</p>"},{"location":"user-guide/asset/capm/#practical-applications","title":"Practical Applications","text":"<p>The CAPM model is used for:</p> <ol> <li>Asset Pricing: Determining the required rate of return for risky assets</li> <li>Portfolio Management: Constructing portfolios with desired risk-return characteristics</li> <li>Capital Budgeting: Evaluating investment projects by calculating the cost of equity</li> <li>Performance Evaluation: Assessing investment performance relative to systematic risk</li> <li>Valuation: Identifying undervalued or overvalued securities by comparing expected returns</li> <li>Risk Management: Understanding and managing systematic risk exposure</li> </ol>"},{"location":"user-guide/asset/capm/#limitations","title":"Limitations","text":"<p>The CAPM model has several limitations:</p> <ol> <li>Single-Factor Model: Only considers systematic risk (beta) and ignores other factors</li> <li>Simplified Assumptions: Real markets violate many of the model's assumptions</li> <li>Beta Instability: Beta coefficients may not be stable over time</li> <li>Market Proxy Issues: Defining and measuring the \"market portfolio\" is challenging</li> <li>Empirical Challenges: Historical tests have shown mixed support for the model</li> <li>Time Horizon: Does not account for different investment horizons</li> </ol>"},{"location":"user-guide/asset/capm/#extensions","title":"Extensions","text":"<p>Several extensions to the basic CAPM model address its limitations:</p> <ol> <li>Multi-Factor Models: Incorporate additional risk factors (e.g., Fama-French Three-Factor Model)</li> <li>Conditional CAPM: Allow for time-varying betas and risk premiums</li> <li>International CAPM: Account for international investments and currency risk</li> <li>Consumption CAPM: Link asset returns to consumption growth</li> <li>Downside CAPM: Focus on downside risk rather than total volatility </li> </ol>"},{"location":"user-guide/asset/fama_french/","title":"Fama-French Factor Models","text":"<p>The Fama-French factor models extend the Capital Asset Pricing Model (CAPM) by adding additional risk factors beyond market risk. Pypulate implements both the Three-Factor and Five-Factor models, which are widely used in academic research and professional asset management to explain stock returns and assess investment performance.</p>"},{"location":"user-guide/asset/fama_french/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.asset import fama_french_three_factor, fama_french_five_factor\n\n# Calculate expected return using the Three-Factor model\nthree_factor_result = fama_french_three_factor(\n    risk_free_rate=0.03,       # 3% risk-free rate\n    market_beta=1.2,           # Market factor beta\n    size_beta=0.5,             # Size factor beta (SMB)\n    value_beta=0.3,            # Value factor beta (HML)\n    market_premium=0.05,       # Market risk premium\n    size_premium=0.02,         # Size premium\n    value_premium=0.03         # Value premium\n)\n\n# Calculate expected return using the Five-Factor model\nfive_factor_result = fama_french_five_factor(\n    risk_free_rate=0.03,       # 3% risk-free rate\n    market_beta=1.2,           # Market factor beta\n    size_beta=0.5,             # Size factor beta (SMB)\n    value_beta=0.3,            # Value factor beta (HML)\n    profitability_beta=0.2,    # Profitability factor beta (RMW)\n    investment_beta=0.1,       # Investment factor beta (CMA)\n    market_premium=0.05,       # Market risk premium\n    size_premium=0.02,         # Size premium\n    value_premium=0.03,        # Value premium\n    profitability_premium=0.01,# Profitability premium\n    investment_premium=0.01    # Investment premium\n)\n\n# Access the results\nthree_factor_return = three_factor_result[\"expected_return\"]\nfive_factor_return = five_factor_result[\"expected_return\"]\n\nprint(f\"Three-Factor Expected Return: {three_factor_return:.2%}\")\nprint(f\"Five-Factor Expected Return: {five_factor_return:.2%}\")\n</code></pre>"},{"location":"user-guide/asset/fama_french/#three-factor-model","title":"Three-Factor Model","text":"<p>The Fama-French Three-Factor Model extends CAPM by adding size and value factors to the market risk factor.</p>"},{"location":"user-guide/asset/fama_french/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>risk_free_rate</code> float Risk-free rate of return (e.g., 0.03 for 3%) Required <code>market_beta</code> float Beta coefficient for market risk factor Required <code>size_beta</code> float Beta coefficient for size factor (SMB - Small Minus Big) Required <code>value_beta</code> float Beta coefficient for value factor (HML - High Minus Low) Required <code>market_premium</code> float Market risk premium (Rm - Rf) Required <code>size_premium</code> float Size premium (SMB) Required <code>value_premium</code> float Value premium (HML) Required"},{"location":"user-guide/asset/fama_french/#return-value","title":"Return Value","text":"<p>The <code>fama_french_three_factor</code> function returns a dictionary with the following keys:</p> Key Type Description <code>expected_return</code> float Expected return calculated using the Three-Factor model <code>risk_free_rate</code> float Risk-free rate used in the calculation <code>total_systematic_risk</code> float Sum of absolute factor contributions <code>risk_assessment</code> str Qualitative assessment of risk based on total systematic risk <code>factor_contributions</code> dict Detailed information about each factor's contribution <p>The <code>factor_contributions</code> dictionary contains detailed information for each factor (market, size, value):</p> Key Type Description <code>beta</code> float Beta coefficient for the factor <code>premium</code> float Risk premium for the factor <code>contribution</code> float Contribution to expected return (beta \u00d7 premium) <code>contribution_pct</code> float Percentage contribution to total systematic risk"},{"location":"user-guide/asset/fama_french/#five-factor-model","title":"Five-Factor Model","text":"<p>The Fama-French Five-Factor Model extends the Three-Factor Model by adding profitability and investment factors.</p>"},{"location":"user-guide/asset/fama_french/#parameters_1","title":"Parameters","text":"Parameter Type Description Default <code>risk_free_rate</code> float Risk-free rate of return (e.g., 0.03 for 3%) Required <code>market_beta</code> float Beta coefficient for market risk factor Required <code>size_beta</code> float Beta coefficient for size factor (SMB - Small Minus Big) Required <code>value_beta</code> float Beta coefficient for value factor (HML - High Minus Low) Required <code>profitability_beta</code> float Beta coefficient for profitability factor (RMW - Robust Minus Weak) Required <code>investment_beta</code> float Beta coefficient for investment factor (CMA - Conservative Minus Aggressive) Required <code>market_premium</code> float Market risk premium (Rm - Rf) Required <code>size_premium</code> float Size premium (SMB) Required <code>value_premium</code> float Value premium (HML) Required <code>profitability_premium</code> float Profitability premium (RMW) Required <code>investment_premium</code> float Investment premium (CMA) Required"},{"location":"user-guide/asset/fama_french/#return-value_1","title":"Return Value","text":"<p>The <code>fama_french_five_factor</code> function returns a dictionary with the same structure as the Three-Factor model, but with additional entries for the profitability and investment factors in the <code>factor_contributions</code> dictionary.</p>"},{"location":"user-guide/asset/fama_french/#risk-level-classification","title":"Risk Level Classification","text":"<p>Both models classify total systematic risk into risk levels:</p> Total Systematic Risk Risk Assessment &lt; 0.03 Low risk 0.03 - 0.059 Moderate risk 0.06 - 0.089 Above-average risk \u2265 0.09 High risk"},{"location":"user-guide/asset/fama_french/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to use the Fama-French models for asset pricing and factor analysis:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom pypulate.asset import fama_french_three_factor, fama_french_five_factor\n\n# Define parameters for analysis\nrisk_free_rate = 0.03\nmarket_premium = 0.05\n\n# Define factor premiums\nfactor_premiums = {\n    'size': 0.02,          # SMB premium\n    'value': 0.03,         # HML premium\n    'profitability': 0.01, # RMW premium\n    'investment': 0.01     # CMA premium\n}\n\n# Define assets with different factor exposures\nassets = {\n    'Large Growth': {\n        'market_beta': 1.1,\n        'size_beta': -0.3,\n        'value_beta': -0.5,\n        'profitability_beta': 0.4,\n        'investment_beta': -0.2\n    },\n    'Large Value': {\n        'market_beta': 0.9,\n        'size_beta': -0.2,\n        'value_beta': 0.6,\n        'profitability_beta': 0.2,\n        'investment_beta': 0.3\n    },\n    'Small Growth': {\n        'market_beta': 1.3,\n        'size_beta': 0.7,\n        'value_beta': -0.4,\n        'profitability_beta': 0.1,\n        'investment_beta': -0.3\n    },\n    'Small Value': {\n        'market_beta': 1.0,\n        'size_beta': 0.8,\n        'value_beta': 0.7,\n        'profitability_beta': -0.1,\n        'investment_beta': 0.4\n    }\n}\n\n# Calculate expected returns using both models\nfor asset_name, asset_data in assets.items():\n    # Three-Factor model\n    three_factor_result = fama_french_three_factor(\n        risk_free_rate=risk_free_rate,\n        market_beta=asset_data['market_beta'],\n        size_beta=asset_data['size_beta'],\n        value_beta=asset_data['value_beta'],\n        market_premium=market_premium,\n        size_premium=factor_premiums['size'],\n        value_premium=factor_premiums['value']\n    )\n\n    # Five-Factor model\n    five_factor_result = fama_french_five_factor(\n        risk_free_rate=risk_free_rate,\n        market_beta=asset_data['market_beta'],\n        size_beta=asset_data['size_beta'],\n        value_beta=asset_data['value_beta'],\n        profitability_beta=asset_data['profitability_beta'],\n        investment_beta=asset_data['investment_beta'],\n        market_premium=market_premium,\n        size_premium=factor_premiums['size'],\n        value_premium=factor_premiums['value'],\n        profitability_premium=factor_premiums['profitability'],\n        investment_premium=factor_premiums['investment']\n    )\n\n    # Store results in asset data\n    asset_data['three_factor'] = three_factor_result\n    asset_data['five_factor'] = five_factor_result\n\n# Visualize results\nplt.figure(figsize=(15, 10))\n\n# Plot expected returns comparison\nplt.subplot(2, 2, 1)\nasset_names = list(assets.keys())\nthree_factor_returns = [assets[name]['three_factor']['expected_return'] for name in asset_names]\nfive_factor_returns = [assets[name]['five_factor']['expected_return'] for name in asset_names]\n\nx = np.arange(len(asset_names))\nwidth = 0.35\n\nplt.bar(x - width/2, [r * 100 for r in three_factor_returns], width, \n        label='Three-Factor Model', color='skyblue')\nplt.bar(x + width/2, [r * 100 for r in five_factor_returns], width, \n        label='Five-Factor Model', color='salmon')\n\nplt.axhline(y=risk_free_rate * 100, color='black', linestyle='--', \n            label=f'Risk-Free Rate ({risk_free_rate:.1%})')\nplt.axhline(y=(risk_free_rate + market_premium) * 100, color='gray', linestyle='--', \n            label=f'Market Return ({risk_free_rate + market_premium:.1%})')\n\nplt.xlabel('Assets')\nplt.ylabel('Expected Return (%)')\nplt.title('Expected Returns: Three-Factor vs. Five-Factor Model')\nplt.xticks(x, asset_names)\nplt.legend()\nplt.grid(axis='y', alpha=0.3)\n\n# Plot factor contributions for a selected asset\nselected_asset = 'Small Value'\nasset_data = assets[selected_asset]\n\n# Three-Factor contributions\nplt.subplot(2, 2, 2)\nfactor_names_3f = ['Market', 'Size', 'Value']\ncontributions_3f = [\n    asset_data['three_factor']['factor_contributions']['market']['contribution'],\n    asset_data['three_factor']['factor_contributions']['size']['contribution'],\n    asset_data['three_factor']['factor_contributions']['value']['contribution']\n]\n\ncolors_3f = ['#ff9999', '#66b3ff', '#99ff99']\nplt.pie([abs(c) for c in contributions_3f], labels=factor_names_3f, colors=colors_3f,\n        autopct='%1.1f%%', startangle=90, wedgeprops={'edgecolor': 'w'})\nplt.axis('equal')\nplt.title(f'Three-Factor Model: {selected_asset}\\nFactor Contributions')\n\n# Five-Factor contributions\nplt.subplot(2, 2, 3)\nfactor_names_5f = ['Market', 'Size', 'Value', 'Profitability', 'Investment']\ncontributions_5f = [\n    asset_data['five_factor']['factor_contributions']['market']['contribution'],\n    asset_data['five_factor']['factor_contributions']['size']['contribution'],\n    asset_data['five_factor']['factor_contributions']['value']['contribution'],\n    asset_data['five_factor']['factor_contributions']['profitability']['contribution'],\n    asset_data['five_factor']['factor_contributions']['investment']['contribution']\n]\n\ncolors_5f = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#c2c2f0']\nplt.pie([abs(c) for c in contributions_5f], labels=factor_names_5f, colors=colors_5f,\n        autopct='%1.1f%%', startangle=90, wedgeprops={'edgecolor': 'w'})\nplt.axis('equal')\nplt.title(f'Five-Factor Model: {selected_asset}\\nFactor Contributions')\n\n# Plot factor betas for all assets\nplt.subplot(2, 2, 4)\nfactor_names = ['Market', 'Size', 'Value', 'Profitability', 'Investment']\nfactor_keys = ['market_beta', 'size_beta', 'value_beta', 'profitability_beta', 'investment_beta']\n\n# Create a grouped bar chart for factor betas\nbar_width = 0.15\nindex = np.arange(len(factor_names))\n\nfor i, (asset_name, asset_data) in enumerate(assets.items()):\n    betas = [asset_data[key] for key in factor_keys]\n    plt.bar(index + i * bar_width, betas, bar_width, label=asset_name)\n\nplt.xlabel('Factors')\nplt.ylabel('Beta')\nplt.title('Factor Betas by Asset')\nplt.xticks(index + bar_width * (len(assets) - 1) / 2, factor_names)\nplt.axhline(y=0, color='black', linestyle='-', alpha=0.2)\nplt.legend()\nplt.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print detailed results\nprint(\"Asset Analysis:\")\nprint(f\"{'Asset':&lt;15} {'3F Return':&lt;12} {'5F Return':&lt;12} {'3F Risk':&lt;20} {'5F Risk':&lt;20}\")\nprint(\"-\" * 80)\nfor asset_name, asset_data in assets.items():\n    three_factor_return = asset_data['three_factor']['expected_return']\n    five_factor_return = asset_data['five_factor']['expected_return']\n    three_factor_risk = asset_data['three_factor']['risk_assessment']\n    five_factor_risk = asset_data['five_factor']['risk_assessment']\n\n    print(f\"{asset_name:&lt;15} {three_factor_return:&lt;12.2%} {five_factor_return:&lt;12.2%} \"\n          f\"{three_factor_risk:&lt;20} {five_factor_risk:&lt;20}\")\n\n# Analyze factor contributions for each asset\nprint(\"\\nFactor Contribution Analysis (Five-Factor Model):\")\nfor asset_name, asset_data in assets.items():\n    print(f\"\\n{asset_name}:\")\n    contributions = asset_data['five_factor']['factor_contributions']\n\n    print(f\"{'Factor':&lt;15} {'Beta':&lt;8} {'Premium':&lt;10} {'Contribution':&lt;15} {'% of Total':&lt;12}\")\n    print(\"-\" * 60)\n\n    for factor, data in contributions.items():\n        print(f\"{factor.capitalize():&lt;15} {data['beta']:&gt;+.2f}    {data['premium']:&gt;6.2%}     \"\n              f\"{data['contribution']:&gt;+.4f}         {data['contribution_pct']:&gt;6.1%}\")\n\n    print(f\"Total Expected Return: {asset_data['five_factor']['expected_return']:.2%}\")\n    print(f\"Total Systematic Risk: {asset_data['five_factor']['total_systematic_risk']:.2%}\")\n    print(f\"Risk Assessment: {asset_data['five_factor']['risk_assessment']}\")\n</code></pre>"},{"location":"user-guide/asset/fama_french/#example-output","title":"Example Output","text":"<pre><code>Asset Analysis:\nAsset           3F Return     5F Return     3F Risk              5F Risk              \n--------------------------------------------------------------------------------\nLarge Growth    7.35%         7.55%         Moderate risk        Moderate risk        \nLarge Value     9.30%         9.50%         Moderate risk        Moderate risk        \nSmall Growth    8.95%         8.85%         Moderate risk        Moderate risk        \nSmall Value     11.10%        11.10%        Above-average risk   Above-average risk   \n\nFactor Contribution Analysis (Five-Factor Model):\n\nLarge Growth:\nFactor          Beta     Premium    Contribution     % of Total  \n------------------------------------------------------------\nMarket          +1.10    5.00%      +0.0550          68.8%\nSize            -0.30    2.00%      -0.0060          7.5%\nValue           -0.50    3.00%      -0.0150          18.8%\nProfitability   +0.40    1.00%      +0.0040          5.0%\nInvestment      -0.20    1.00%      -0.0020          2.5%\nTotal Expected Return: 7.55%\nTotal Systematic Risk: 8.20%\nRisk Assessment: Moderate risk\n\nLarge Value:\nFactor          Beta     Premium    Contribution     % of Total  \n------------------------------------------------------------\nMarket          +0.90    5.00%      +0.0450          56.3%\nSize            -0.20    2.00%      -0.0040          5.0%\nValue           +0.60    3.00%      +0.0180          22.5%\nProfitability   +0.20    1.00%      +0.0020          2.5%\nInvestment      +0.30    1.00%      +0.0030          3.8%\nTotal Expected Return: 9.50%\nTotal Systematic Risk: 7.20%\nRisk Assessment: Moderate risk\n\nSmall Growth:\nFactor          Beta     Premium    Contribution     % of Total  \n------------------------------------------------------------\nMarket          +1.30    5.00%      +0.0650          65.0%\nSize            +0.70    2.00%      +0.0140          14.0%\nValue           -0.40    3.00%      -0.0120          12.0%\nProfitability   +0.10    1.00%      +0.0010          1.0%\nInvestment      -0.30    1.00%      -0.0030          3.0%\nTotal Expected Return: 8.85%\nTotal Systematic Risk: 9.50%\nRisk Assessment: Above-average risk\n\nSmall Value:\nFactor          Beta     Premium    Contribution     % of Total  \n------------------------------------------------------------\nMarket          +1.00    5.00%      +0.0500          50.0%\nSize            +0.80    2.00%      +0.0160          16.0%\nValue           +0.70    3.00%      +0.0210          21.0%\nProfitability   -0.10    1.00%      -0.0010          1.0%\nInvestment      +0.40    1.00%      +0.0040          4.0%\nTotal Expected Return: 11.10%\nTotal Systematic Risk: 9.20%\nRisk Assessment: Above-average risk\n</code></pre>"},{"location":"user-guide/asset/fama_french/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/asset/fama_french/#expected-returns-comparison","title":"Expected Returns Comparison","text":"<p>This chart compares the expected returns calculated using the Three-Factor and Five-Factor models for different asset types. It also shows the risk-free rate and market return as reference points.</p> <p></p>"},{"location":"user-guide/asset/fama_french/#factor-contributions","title":"Factor Contributions","text":"<p>These pie charts show the relative contribution of each factor to the total systematic risk for a selected asset, comparing the Three-Factor and Five-Factor models.</p>"},{"location":"user-guide/asset/fama_french/#factor-betas","title":"Factor Betas","text":"<p>This grouped bar chart displays the factor betas for different assets, allowing for comparison of factor exposures across the investment universe.</p>"},{"location":"user-guide/asset/fama_french/#theoretical-background","title":"Theoretical Background","text":"<p>The Fama-French factor models were developed by Eugene Fama and Kenneth French to explain stock returns better than the single-factor CAPM.</p>"},{"location":"user-guide/asset/fama_french/#practical-applications","title":"Practical Applications","text":"<p>The Fama-French factor models are used for:</p> <ol> <li>Asset Pricing: Determining the required rate of return for risky assets</li> <li>Performance Evaluation: Assessing investment performance relative to multiple risk factors</li> <li>Portfolio Construction: Building portfolios with targeted factor exposures</li> <li>Style Analysis: Identifying investment styles and factor tilts</li> <li>Risk Decomposition: Breaking down portfolio risk into factor components</li> <li>Alpha Generation: Identifying mispriced securities relative to their factor exposures</li> </ol>"},{"location":"user-guide/asset/fama_french/#limitations","title":"Limitations","text":"<p>The Fama-French factor models have several limitations:</p> <ol> <li>Data Requirements: Require extensive data for factor construction and beta estimation</li> <li>Parameter Instability: Factor betas and premiums may vary over time</li> <li>Factor Selection: The choice of factors is somewhat arbitrary</li> <li>Multicollinearity: Factors may be correlated with each other</li> <li>Implementation Costs: Factor portfolios may be expensive to implement in practice</li> <li>Regional Differences: Factor premiums may vary across different markets</li> </ol>"},{"location":"user-guide/asset/fama_french/#extensions","title":"Extensions","text":"<p>Several extensions to the Fama-French models address their limitations:</p> <ol> <li>Momentum Factor: Adding a momentum factor (UMD - Up Minus Down)</li> <li>Quality Factor: Incorporating a quality factor beyond profitability</li> <li>Low Volatility Factor: Adding a low volatility anomaly factor</li> <li>Conditional Models: Allowing for time-varying factor loadings and premiums</li> <li>Industry-Adjusted Factors: Controlling for industry effects in factor construction</li> <li>Macroeconomic Factors: Incorporating macroeconomic variables as additional factors </li> </ol>"},{"location":"user-guide/asset/mean_inversion/","title":"Mean Inversion Pricing","text":"<p>The mean inversion pricing module provides functions for pricing options on mean-reverting assets using the Ornstein-Uhlenbeck process. This model is particularly useful for commodities, interest rates, volatility indices, and other assets that tend to revert to a long-term mean level over time.</p>"},{"location":"user-guide/asset/mean_inversion/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.asset import mean_inversion_pricing, analytical_mean_inversion_option\n\n# Price a call option on a mean-reverting asset using Monte Carlo simulation\nmc_result = mean_inversion_pricing(\n    current_price=50,\n    long_term_mean=55,\n    mean_reversion_rate=2.5,\n    volatility=0.3,\n    time_to_expiry=1.0,\n    risk_free_rate=0.05,\n    strike_price=52,\n    option_type='call',\n    simulations=10000,\n    seed=42\n)\n\n# Price the same option using the analytical approximation\nanalytical_result = analytical_mean_inversion_option(\n    current_price=50,\n    long_term_mean=55,\n    mean_reversion_rate=2.5,\n    volatility=0.3,\n    time_to_expiry=1.0,\n    risk_free_rate=0.05,\n    strike_price=52,\n    option_type='call'\n)\n\n# Compare the results\nprint(f\"Monte Carlo Price: ${mc_result['price']:.2f}\")\nprint(f\"Analytical Price: ${analytical_result['price']:.2f}\")\nprint(f\"Difference: ${abs(mc_result['price'] - analytical_result['price']):.2f}\")\n\n# Access additional information\nprint(f\"Expected price at expiry: ${mc_result['expected_price_at_expiry']:.2f}\")\nprint(f\"Half-life of mean reversion: {mc_result['half_life']:.2f} years\")\n</code></pre>"},{"location":"user-guide/asset/mean_inversion/#functions","title":"Functions","text":""},{"location":"user-guide/asset/mean_inversion/#mean_inversion_pricing","title":"mean_inversion_pricing","text":"<p>This function implements the Ornstein-Uhlenbeck process to model mean-reverting assets and prices options using Monte Carlo simulation.</p>"},{"location":"user-guide/asset/mean_inversion/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>current_price</code> float Current price of the underlying asset Required <code>long_term_mean</code> float Long-term mean level that the asset price reverts to Required <code>mean_reversion_rate</code> float Speed at which the asset price reverts to the long-term mean (annualized) Required <code>volatility</code> float Volatility of the asset price (annualized) Required <code>time_to_expiry</code> float Time to option expiration in years Required <code>risk_free_rate</code> float Risk-free interest rate (annualized) Required <code>strike_price</code> float Strike price of the option Required <code>option_type</code> str Type of option ('call' or 'put') 'call' <code>simulations</code> int Number of Monte Carlo simulations 10000 <code>time_steps</code> int Number of time steps in each simulation 252 <code>seed</code> int Random seed for reproducibility None"},{"location":"user-guide/asset/mean_inversion/#return-value","title":"Return Value","text":"<p>The <code>mean_inversion_pricing</code> function returns a dictionary with the following keys:</p> Key Type Description <code>price</code> float Calculated option price <code>standard_error</code> float Standard error of the price estimate <code>confidence_interval</code> tuple 95% confidence interval for the price (lower, upper) <code>current_price</code> float Current price of the underlying asset <code>long_term_mean</code> float Long-term mean level that the asset price reverts to <code>mean_reversion_rate</code> float Speed at which the asset price reverts to the long-term mean <code>volatility</code> float Volatility of the asset price <code>time_to_expiry</code> float Time to option expiration in years <code>risk_free_rate</code> float Risk-free interest rate <code>strike_price</code> float Strike price of the option <code>option_type</code> str Type of option ('call' or 'put') <code>simulations</code> int Number of Monte Carlo simulations used <code>time_steps</code> int Number of time steps used in each simulation <code>expected_price_at_expiry</code> float Expected price of the asset at expiry <code>half_life</code> float Half-life of mean reversion in years <code>price_statistics</code> dict Statistics about the simulated final prices (mean, std, min, max, median) <code>sample_paths</code> list Sample price paths for visualization (first 5 simulations)"},{"location":"user-guide/asset/mean_inversion/#analytical_mean_inversion_option","title":"analytical_mean_inversion_option","text":"<p>This function implements an analytical approximation for pricing European options on mean-reverting assets based on the Ornstein-Uhlenbeck process.</p>"},{"location":"user-guide/asset/mean_inversion/#parameters_1","title":"Parameters","text":"Parameter Type Description Default <code>current_price</code> float Current price of the underlying asset Required <code>long_term_mean</code> float Long-term mean level that the asset price reverts to Required <code>mean_reversion_rate</code> float Speed at which the asset price reverts to the long-term mean (annualized) Required <code>volatility</code> float Volatility of the asset price (annualized) Required <code>time_to_expiry</code> float Time to option expiration in years Required <code>risk_free_rate</code> float Risk-free interest rate (annualized) Required <code>strike_price</code> float Strike price of the option Required <code>option_type</code> str Type of option ('call' or 'put') 'call'"},{"location":"user-guide/asset/mean_inversion/#return-value_1","title":"Return Value","text":"<p>The <code>analytical_mean_inversion_option</code> function returns a dictionary with the following keys:</p> Key Type Description <code>price</code> float Calculated option price <code>current_price</code> float Current price of the underlying asset <code>long_term_mean</code> float Long-term mean level that the asset price reverts to <code>mean_reversion_rate</code> float Speed at which the asset price reverts to the long-term mean <code>volatility</code> float Volatility of the asset price <code>time_to_expiry</code> float Time to option expiration in years <code>risk_free_rate</code> float Risk-free interest rate <code>strike_price</code> float Strike price of the option <code>option_type</code> str Type of option ('call' or 'put') <code>expected_price_at_expiry</code> float Expected price of the asset at expiry <code>variance_at_expiry</code> float Variance of the asset price at expiry <code>std_dev_at_expiry</code> float Standard deviation of the asset price at expiry <code>half_life</code> float Half-life of mean reversion in years <code>d1</code> float d1 parameter (similar to Black-Scholes) <code>d2</code> float d2 parameter (similar to Black-Scholes)"},{"location":"user-guide/asset/mean_inversion/#the-ornstein-uhlenbeck-process","title":"The Ornstein-Uhlenbeck Process","text":"<p>The Ornstein-Uhlenbeck process is a stochastic process that models mean-reverting behavior.</p> <p>The key property of this process is that it tends to pull the asset price toward the long-term mean \\(\\mu\\) at a rate proportional to the distance from the mean, controlled by the parameter \\(\\theta\\).</p>"},{"location":"user-guide/asset/mean_inversion/#mean-reversion-rate-and-half-life","title":"Mean Reversion Rate and Half-Life","text":"<p>The mean reversion rate \\(\\theta\\) determines how quickly the asset price reverts to its long-term mean. A higher value of \\(\\theta\\) indicates faster mean reversion.</p> <p>The half-life of mean reversion is the time it takes for the expected deviation from the long-term mean to decrease by half. It is calculated as:</p> \\[\\text{Half-life} = \\frac{\\ln(2)}{\\theta}\\] <p>For example, if \\(\\theta = 2.5\\), the half-life is approximately 0.28 years (about 3.3 months), meaning that after this period, the expected deviation from the long-term mean will be half of its initial value.</p>"},{"location":"user-guide/asset/mean_inversion/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to use the mean inversion pricing functions and visualize the results:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom pypulate.asset import mean_inversion_pricing, analytical_mean_inversion_option\n\n# Define parameters\ncurrent_price = 50\nlong_term_mean = 55\nmean_reversion_rate = 2.5\nvolatility = 0.3\nrisk_free_rate = 0.05\nstrike_price = 52\ntime_to_expiry = 1.0\n\n# Price options with different maturities\nmaturities = np.linspace(0.1, 2.0, 20)\nmc_call_prices = []\nmc_put_prices = []\nanalytical_call_prices = []\nanalytical_put_prices = []\n\nfor t in maturities:\n    # Monte Carlo pricing\n    mc_call = mean_inversion_pricing(\n        current_price=current_price,\n        long_term_mean=long_term_mean,\n        mean_reversion_rate=mean_reversion_rate,\n        volatility=volatility,\n        time_to_expiry=t,\n        risk_free_rate=risk_free_rate,\n        strike_price=strike_price,\n        option_type='call',\n        simulations=10000,\n        seed=42\n    )\n    mc_put = mean_inversion_pricing(\n        current_price=current_price,\n        long_term_mean=long_term_mean,\n        mean_reversion_rate=mean_reversion_rate,\n        volatility=volatility,\n        time_to_expiry=t,\n        risk_free_rate=risk_free_rate,\n        strike_price=strike_price,\n        option_type='put',\n        simulations=10000,\n        seed=42\n    )\n    mc_call_prices.append(mc_call['price'])\n    mc_put_prices.append(mc_put['price'])\n\n    # Analytical pricing\n    analytical_call = analytical_mean_inversion_option(\n        current_price=current_price,\n        long_term_mean=long_term_mean,\n        mean_reversion_rate=mean_reversion_rate,\n        volatility=volatility,\n        time_to_expiry=t,\n        risk_free_rate=risk_free_rate,\n        strike_price=strike_price,\n        option_type='call'\n    )\n    analytical_put = analytical_mean_inversion_option(\n        current_price=current_price,\n        long_term_mean=long_term_mean,\n        mean_reversion_rate=mean_reversion_rate,\n        volatility=volatility,\n        time_to_expiry=t,\n        risk_free_rate=risk_free_rate,\n        strike_price=strike_price,\n        option_type='put'\n    )\n    analytical_call_prices.append(analytical_call['price'])\n    analytical_put_prices.append(analytical_put['price'])\n\n# Plot option prices vs. maturity\nplt.figure(figsize=(12, 8))\nplt.plot(maturities, mc_call_prices, 'b-', label='MC Call', linewidth=2)\nplt.plot(maturities, mc_put_prices, 'r-', label='MC Put', linewidth=2)\nplt.plot(maturities, analytical_call_prices, 'b--', label='Analytical Call', linewidth=2)\nplt.plot(maturities, analytical_put_prices, 'r--', label='Analytical Put', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Time to Expiry (years)')\nplt.ylabel('Option Price ($)')\nplt.title('Option Prices vs. Time to Expiry')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Analyze the impact of mean reversion rate\nreversion_rates = np.linspace(0.5, 5.0, 10)\ncall_prices_by_reversion = []\nhalf_lives = []\n\nfor rate in reversion_rates:\n    result = analytical_mean_inversion_option(\n        current_price=current_price,\n        long_term_mean=long_term_mean,\n        mean_reversion_rate=rate,\n        volatility=volatility,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        strike_price=strike_price,\n        option_type='call'\n    )\n    call_prices_by_reversion.append(result['price'])\n    half_lives.append(result['half_life'])\n\n# Plot option prices vs. mean reversion rate\nplt.figure(figsize=(12, 8))\nplt.subplot(2, 1, 1)\nplt.plot(reversion_rates, call_prices_by_reversion, 'b-', marker='o', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Mean Reversion Rate (\u03b8)')\nplt.ylabel('Call Option Price ($)')\nplt.title('Call Option Price vs. Mean Reversion Rate')\n\nplt.subplot(2, 1, 2)\nplt.plot(reversion_rates, half_lives, 'g-', marker='o', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Mean Reversion Rate (\u03b8)')\nplt.ylabel('Half-Life (years)')\nplt.title('Half-Life vs. Mean Reversion Rate')\nplt.tight_layout()\nplt.show()\n\n# Visualize sample price paths\nresult = mean_inversion_pricing(\n    current_price=current_price,\n    long_term_mean=long_term_mean,\n    mean_reversion_rate=mean_reversion_rate,\n    volatility=volatility,\n    time_to_expiry=time_to_expiry,\n    risk_free_rate=risk_free_rate,\n    strike_price=strike_price,\n    option_type='call',\n    simulations=10000,\n    time_steps=252,\n    seed=42\n)\n\n# Extract sample paths\nsample_paths = result['sample_paths']\ntime_points = np.linspace(0, time_to_expiry, len(sample_paths[0]))\n\n# Plot sample paths\nplt.figure(figsize=(12, 8))\nfor i, path in enumerate(sample_paths):\n    plt.plot(time_points, path, alpha=0.7, label=f'Path {i+1}' if i == 0 else None)\n\n# Add reference lines\nplt.axhline(y=long_term_mean, color='k', linestyle='--', label='Long-term Mean')\nplt.axhline(y=strike_price, color='r', linestyle='--', label='Strike Price')\nplt.axhline(y=current_price, color='g', linestyle='--', label='Current Price')\n\nplt.grid(True, alpha=0.3)\nplt.xlabel('Time (years)')\nplt.ylabel('Asset Price ($)')\nplt.title('Sample Price Paths with Mean Reversion')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Compare with different long-term means\nlong_term_means = np.linspace(45, 65, 9)\ncall_prices_by_mean = []\nput_prices_by_mean = []\n\nfor mean in long_term_means:\n    call_result = analytical_mean_inversion_option(\n        current_price=current_price,\n        long_term_mean=mean,\n        mean_reversion_rate=mean_reversion_rate,\n        volatility=volatility,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        strike_price=strike_price,\n        option_type='call'\n    )\n    put_result = analytical_mean_inversion_option(\n        current_price=current_price,\n        long_term_mean=mean,\n        mean_reversion_rate=mean_reversion_rate,\n        volatility=volatility,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        strike_price=strike_price,\n        option_type='put'\n    )\n    call_prices_by_mean.append(call_result['price'])\n    put_prices_by_mean.append(put_result['price'])\n\n# Plot option prices vs. long-term mean\nplt.figure(figsize=(12, 6))\nplt.plot(long_term_means, call_prices_by_mean, 'b-', marker='o', label='Call Option', linewidth=2)\nplt.plot(long_term_means, put_prices_by_mean, 'r-', marker='o', label='Put Option', linewidth=2)\nplt.axvline(x=strike_price, color='k', linestyle='--', label='Strike Price')\nplt.grid(True, alpha=0.3)\nplt.xlabel('Long-term Mean ($)')\nplt.ylabel('Option Price ($)')\nplt.title('Option Prices vs. Long-term Mean')\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/asset/mean_inversion/#example-output","title":"Example Output","text":"<pre><code>Monte Carlo Price: $4.32\nAnalytical Price: $4.28\nDifference: $0.04\nExpected price at expiry: $53.09\nHalf-life of mean reversion: 0.28 years\n</code></pre>"},{"location":"user-guide/asset/mean_inversion/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/asset/mean_inversion/#option-prices-vs-time-to-expiry","title":"Option Prices vs. Time to Expiry","text":"<p>This chart shows how option prices change with time to expiry for both call and put options, comparing Monte Carlo and analytical pricing methods.</p>"},{"location":"user-guide/asset/mean_inversion/#call-option-price-vs-mean-reversion-rate","title":"Call Option Price vs. Mean Reversion Rate","text":"<p>This chart demonstrates how the call option price decreases as the mean reversion rate increases, reflecting the faster convergence to the long-term mean.</p>"},{"location":"user-guide/asset/mean_inversion/#half-life-vs-mean-reversion-rate","title":"Half-Life vs. Mean Reversion Rate","text":"<p>This chart shows the inverse relationship between the mean reversion rate and the half-life of mean reversion.</p>"},{"location":"user-guide/asset/mean_inversion/#sample-price-paths-with-mean-reversion","title":"Sample Price Paths with Mean Reversion","text":"<p>This chart displays sample price paths generated by the Ornstein-Uhlenbeck process, illustrating the mean-reverting behavior of the asset price.</p> <p></p>"},{"location":"user-guide/asset/mean_inversion/#option-prices-vs-long-term-mean","title":"Option Prices vs. Long-term Mean","text":"<p>This chart shows how call and put option prices change with the long-term mean level, with call prices increasing and put prices decreasing as the long-term mean increases.</p> <p></p>"},{"location":"user-guide/asset/mean_inversion/#practical-applications","title":"Practical Applications","text":"<p>Mean inversion pricing is used for:</p> <ol> <li>Commodity Options: Pricing options on commodities like oil, natural gas, and agricultural products that exhibit mean-reverting behavior</li> <li>Interest Rate Derivatives: Valuing options on interest rates, which tend to revert to long-term levels</li> <li>Volatility Derivatives: Pricing options on volatility indices like VIX, which are mean-reverting</li> <li>Spread Options: Valuing options on price spreads between related assets</li> <li>Real Options: Analyzing investment opportunities in industries with mean-reverting prices</li> <li>Trading Strategies: Developing trading strategies based on mean reversion</li> <li>Risk Management: Assessing risk in portfolios with exposure to mean-reverting assets</li> </ol>"},{"location":"user-guide/asset/mean_inversion/#limitations","title":"Limitations","text":"<ol> <li>Parameter Estimation: Accurately estimating the mean reversion rate and long-term mean can be challenging</li> <li>Model Risk: The Ornstein-Uhlenbeck process may not perfectly capture the dynamics of all mean-reverting assets</li> <li>Negative Prices: The basic model can allow negative prices, which may not be realistic for some assets</li> <li>Constant Parameters: The model assumes constant volatility and mean reversion rate, which may not hold in practice</li> <li>Analytical Approximation: The analytical pricing method is an approximation and may not be as accurate as Monte Carlo simulation for some parameter combinations</li> </ol>"},{"location":"user-guide/asset/mean_inversion/#extensions","title":"Extensions","text":"<ol> <li>Square-Root Process: Using a square-root process (Cox-Ingersoll-Ross) to ensure non-negative prices</li> <li>Stochastic Volatility: Incorporating stochastic volatility into the mean-reverting process</li> <li>Regime-Switching: Allowing parameters to change based on market regimes</li> <li>Jump Diffusion: Adding jumps to the mean-reverting process to capture sudden price changes</li> <li>Multi-Factor Models: Using multiple factors to model the mean-reverting behavior</li> <li>American Options: Extending the model to price American options on mean-reverting assets </li> </ol>"},{"location":"user-guide/asset/monte_carlo/","title":"Monte Carlo Option Pricing","text":"<p>The <code>monte_carlo_option_pricing</code> function implements a flexible Monte Carlo simulation approach for pricing various types of options, including European, Asian, and lookback options. This numerical method is particularly valuable for pricing exotic options where closed-form solutions may not exist. The function now also supports jump diffusion for modeling assets with sudden price jumps like cryptocurrencies.</p>"},{"location":"user-guide/asset/monte_carlo/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.asset import monte_carlo_option_pricing\n\n# Calculate price for a standard European call option\nresult = monte_carlo_option_pricing(\n    option_type='european_call',\n    underlying_price=100,\n    strike_price=100,\n    time_to_expiry=1.0,\n    risk_free_rate=0.05,\n    volatility=0.2,\n    simulations=10000,\n    time_steps=252,\n    dividend_yield=0.01,\n    antithetic=True,\n    seed=42\n)\n\n# Access the results\noption_price = result[\"price\"]\nstandard_error = result[\"standard_error\"]\nconfidence_interval = result[\"confidence_interval\"]\n\nprint(f\"Option Price: ${option_price:.2f}\")\nprint(f\"Standard Error: ${standard_error:.4f}\")\nprint(f\"95% Confidence Interval: [${confidence_interval[0]:.2f}, ${confidence_interval[1]:.2f}]\")\n\n# Price a Bitcoin option with jump diffusion\nbtc_result = monte_carlo_option_pricing(\n    option_type='european_call',\n    underlying_price=40000,\n    strike_price=45000,\n    time_to_expiry=0.25,\n    risk_free_rate=0.05,\n    volatility=0.85,\n    simulations=10000,\n    time_steps=90,\n    jump_intensity=12,    # Expected 12 jumps per year\n    jump_mean=-0.05,      # Average jump size (negative for downward bias)\n    jump_std=0.15,        # Jump size standard deviation\n    seed=42\n)\n\nprint(f\"Bitcoin Option Price: ${btc_result['price']:.2f}\")\n</code></pre>"},{"location":"user-guide/asset/monte_carlo/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>option_type</code> str Type of option ('european_call', 'european_put', 'asian_call', 'asian_put', 'lookback_call', 'lookback_put') Required <code>underlying_price</code> float Current price of the underlying asset Required <code>strike_price</code> float Strike price of the option Required <code>time_to_expiry</code> float Time to expiration in years Required <code>risk_free_rate</code> float Risk-free interest rate (annualized) Required <code>volatility</code> float Volatility of the underlying asset (annualized) Required <code>simulations</code> int Number of Monte Carlo simulations 10000 <code>time_steps</code> int Number of time steps in each simulation 252 <code>dividend_yield</code> float Continuous dividend yield 0.0 <code>antithetic</code> bool Whether to use antithetic variates for variance reduction True <code>jump_intensity</code> float Expected number of jumps per year (lambda in Poisson process) 0.0 <code>jump_mean</code> float Mean of the jump size distribution 0.0 <code>jump_std</code> float Standard deviation of the jump size distribution 0.0 <code>seed</code> int Random seed for reproducibility None"},{"location":"user-guide/asset/monte_carlo/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>price</code> float Calculated option price <code>standard_error</code> float Standard error of the price estimate <code>confidence_interval</code> tuple 95% confidence interval for the price (lower, upper) <code>underlying_price</code> float Price of the underlying asset used in calculation <code>strike_price</code> float Strike price used in calculation <code>time_to_expiry</code> float Time to expiration used in calculation <code>risk_free_rate</code> float Risk-free rate used in calculation <code>volatility</code> float Volatility used in calculation <code>simulations</code> int Number of simulations used <code>time_steps</code> int Number of time steps used <code>dividend_yield</code> float Dividend yield used in calculation <code>antithetic</code> bool Whether antithetic variates were used <code>jump_intensity</code> float Jump intensity used in calculation <code>jump_mean</code> float Jump mean used in calculation <code>jump_std</code> float Jump standard deviation used in calculation"},{"location":"user-guide/asset/monte_carlo/#option-types","title":"Option Types","text":"<p>The function supports the following option types:</p> Option Type Description <code>european_call</code> Standard call option with payoff max(S_T - K, 0) <code>european_put</code> Standard put option with payoff max(K - S_T, 0) <code>asian_call</code> Call option on the average price with payoff max(S_avg - K, 0) <code>asian_put</code> Put option on the average price with payoff max(K - S_avg, 0) <code>lookback_call</code> Call option with payoff S_T - S_min <code>lookback_put</code> Put option with payoff S_max - S_T <p>Where: - S_T is the final price at expiration - K is the strike price - S_avg is the average price over the option's life - S_min is the minimum price over the option's life - S_max is the maximum price over the option's life</p>"},{"location":"user-guide/asset/monte_carlo/#jump-diffusion-for-volatile-assets","title":"Jump Diffusion for Volatile Assets","text":"<p>The Monte Carlo option pricing function now supports the Merton jump diffusion model, which is particularly useful for pricing options on highly volatile assets like cryptocurrencies, biotech stocks, or commodities that exhibit both continuous price movements and sudden jumps.</p>"},{"location":"user-guide/asset/monte_carlo/#jump-diffusion-parameters","title":"Jump Diffusion Parameters","text":"<ul> <li> <p>Jump Intensity (\u03bb): The expected number of jumps per year. For example, a value of 12 means we expect an average of 12 jumps per year, or about one per month.</p> </li> <li> <p>Jump Mean (\u03bc<sub>J</sub>): The average size of jumps. A negative value indicates a downward bias in jumps, which is common in many assets.</p> </li> <li> <p>Jump Standard Deviation (\u03c3<sub>J</sub>): The volatility of jump sizes. Larger values indicate more unpredictable jump magnitudes.</p> </li> </ul>"},{"location":"user-guide/asset/monte_carlo/#the-jump-diffusion-model","title":"The Jump Diffusion Model","text":"<p>The standard geometric Brownian motion model is extended with a jump component:</p> \\[dS = (r - q - \\lambda \\kappa)S dt + \\sigma S dW + S dJ\\] <p>Where: - \\(S\\) is the asset price - \\(r\\) is the risk-free rate - \\(q\\) is the dividend yield - \\(\\sigma\\) is the volatility - \\(dW\\) is a Wiener process increment - \\(\\lambda\\) is the jump intensity - \\(\\kappa = E[e^J - 1]\\) is the expected percentage jump size - \\(dJ\\) is a compound Poisson process</p>"},{"location":"user-guide/asset/monte_carlo/#example-pricing-a-bitcoin-option","title":"Example: Pricing a Bitcoin Option","text":"<p>Bitcoin and other cryptocurrencies are known for their extreme volatility and tendency to experience sudden price jumps. Here's how to price a Bitcoin option using the jump diffusion model:</p> <pre><code>from pypulate.asset import monte_carlo_option_pricing\n\n# Price a Bitcoin call option with jump diffusion\nbtc_result = monte_carlo_option_pricing(\n    option_type='european_call',\n    underlying_price=40000,      # Bitcoin price of $40,000\n    strike_price=45000,          # Strike price of $45,000\n    time_to_expiry=0.25,         # 3 months to expiry\n    risk_free_rate=0.05,\n    volatility=0.85,             # Bitcoin has much higher volatility than traditional assets\n    simulations=50000,           # More simulations for better accuracy\n    time_steps=90,               # Daily steps for 3 months\n    jump_intensity=12,           # Expect 12 jumps per year (1 per month on average)\n    jump_mean=-0.05,             # Average jump of -5% (downward bias)\n    jump_std=0.15,               # Jump size standard deviation of 15%\n    seed=42\n)\n\nprint(f\"Bitcoin Option Price: ${btc_result['price']:.2f}\")\nprint(f\"95% Confidence Interval: [${btc_result['confidence_interval'][0]:.2f}, ${btc_result['confidence_interval'][1]:.2f}]\")\n</code></pre>"},{"location":"user-guide/asset/monte_carlo/#comparison-standard-vs-jump-diffusion-model","title":"Comparison: Standard vs. Jump Diffusion Model","text":"<p>For assets with significant jump risk, the standard geometric Brownian motion model can underestimate option prices, especially for out-of-the-money options. The jump diffusion model captures the fat-tailed distribution of returns that is characteristic of many volatile assets.</p> <pre><code># Standard model (no jumps)\nstandard_result = monte_carlo_option_pricing(\n    option_type='european_call',\n    underlying_price=40000,\n    strike_price=45000,\n    time_to_expiry=0.25,\n    risk_free_rate=0.05,\n    volatility=0.85,\n    simulations=50000,\n    time_steps=90,\n    seed=42\n)\n\n# Jump diffusion model\njump_result = monte_carlo_option_pricing(\n    option_type='european_call',\n    underlying_price=40000,\n    strike_price=45000,\n    time_to_expiry=0.25,\n    risk_free_rate=0.05,\n    volatility=0.85,\n    simulations=50000,\n    time_steps=90,\n    jump_intensity=12,\n    jump_mean=-0.05,\n    jump_std=0.15,\n    seed=42\n)\n\nprint(f\"Standard Model Price: ${standard_result['price']:.2f}\")\nprint(f\"Jump Diffusion Model Price: ${jump_result['price']:.2f}\")\nprint(f\"Difference: ${jump_result['price'] - standard_result['price']:.2f}\")\n</code></pre>"},{"location":"user-guide/asset/monte_carlo/#when-to-use-jump-diffusion","title":"When to Use Jump Diffusion","text":"<p>Consider using the jump diffusion model when:</p> <ol> <li> <p>The asset exhibits sudden price movements: Cryptocurrencies, certain commodities, and stocks in volatile sectors.</p> </li> <li> <p>The returns distribution has fat tails: When extreme events occur more frequently than a normal distribution would predict.</p> </li> <li> <p>The asset is subject to significant event risk: Such as regulatory announcements, earnings surprises, or technological breakthroughs.</p> </li> <li> <p>You're pricing out-of-the-money options: Jump diffusion can better capture the probability of extreme price movements that would make these options valuable.</p> </li> </ol>"},{"location":"user-guide/asset/monte_carlo/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to use the Monte Carlo simulation for option pricing and analysis:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom pypulate.asset import monte_carlo_option_pricing\n\n# Define parameters for analysis\nunderlying_price = 100\nstrike_price = 100\ntime_to_expiry = 1.0\nrisk_free_rate = 0.05\nvolatility = 0.2\ndividend_yield = 0.01\nsimulations = 10000\ntime_steps = 252\nseed = 42\n\n# Define option types to analyze\noption_types = [\n    'european_call', 'european_put', \n    'asian_call', 'asian_put', \n    'lookback_call', 'lookback_put'\n]\n\n# Calculate prices for all option types\nresults = {}\nfor option_type in option_types:\n    result = monte_carlo_option_pricing(\n        option_type=option_type,\n        underlying_price=underlying_price,\n        strike_price=strike_price,\n        time_to_expiry=time_to_expiry,\n        risk_free_rate=risk_free_rate,\n        volatility=volatility,\n        dividend_yield=dividend_yield,\n        simulations=simulations,\n        time_steps=time_steps,\n        antithetic=True,\n        seed=seed\n    )\n    results[option_type] = result\n\n# Print results\nprint(\"Monte Carlo Option Pricing Results:\")\nprint(f\"{'Option Type':&lt;15} {'Price':&lt;10} {'Std Error':&lt;10} {'95% CI':&lt;20}\")\nprint(\"-\" * 55)\nfor option_type, result in results.items():\n    ci_low, ci_high = result['confidence_interval']\n    print(f\"{option_type:&lt;15} ${result['price']:&lt;9.2f} ${result['standard_error']:&lt;9.4f} [${ci_low:.2f}, ${ci_high:.2f}]\")\n\n# Analyze impact of volatility on option prices\nvolatilities = np.linspace(0.1, 0.5, 9)  # 10% to 50% volatility\nvol_prices = {option_type: [] for option_type in option_types}\n\nfor vol in volatilities:\n    for option_type in option_types:\n        result = monte_carlo_option_pricing(\n            option_type=option_type,\n            underlying_price=underlying_price,\n            strike_price=strike_price,\n            time_to_expiry=time_to_expiry,\n            risk_free_rate=risk_free_rate,\n            volatility=vol,\n            dividend_yield=dividend_yield,\n            simulations=simulations,\n            time_steps=time_steps,\n            antithetic=True,\n            seed=seed\n        )\n        vol_prices[option_type].append(result['price'])\n\n# Visualize results\nplt.figure(figsize=(15, 10))\n\n# Plot option prices\nplt.subplot(2, 2, 1)\nfor option_type in option_types:\n    plt.plot(volatilities, vol_prices[option_type], marker='o', label=option_type.replace('_', ' ').title())\nplt.grid(True, alpha=0.3)\nplt.xlabel('Volatility')\nplt.ylabel('Option Price ($)')\nplt.title('Option Prices vs. Volatility')\nplt.legend()\n\n# Analyze impact of time to expiry\ntimes = np.linspace(0.25, 2.0, 8)  # 3 months to 2 years\ntime_prices = {option_type: [] for option_type in option_types}\n\nfor t in times:\n    for option_type in option_types:\n        result = monte_carlo_option_pricing(\n            option_type=option_type,\n            underlying_price=underlying_price,\n            strike_price=strike_price,\n            time_to_expiry=t,\n            risk_free_rate=risk_free_rate,\n            volatility=volatility,\n            dividend_yield=dividend_yield,\n            simulations=simulations,\n            time_steps=max(int(t * 252), 10),  # Scale time steps with expiry\n            antithetic=True,\n            seed=seed\n        )\n        time_prices[option_type].append(result['price'])\n\n# Plot impact of time to expiry\nplt.subplot(2, 2, 2)\nfor option_type in option_types:\n    plt.plot(times, time_prices[option_type], marker='o', label=option_type.replace('_', ' ').title())\nplt.grid(True, alpha=0.3)\nplt.xlabel('Time to Expiry (years)')\nplt.ylabel('Option Price ($)')\nplt.title('Option Prices vs. Time to Expiry')\nplt.legend()\n\n# Analyze impact of moneyness (strike price relative to underlying)\nmoneyness_levels = np.linspace(0.7, 1.3, 7)  # 70% to 130% of underlying price\nstrikes = underlying_price * moneyness_levels\nmoneyness_prices = {option_type: [] for option_type in option_types[:2]}  # Just European options\n\nfor k in strikes:\n    for option_type in list(moneyness_prices.keys()):\n        result = monte_carlo_option_pricing(\n            option_type=option_type,\n            underlying_price=underlying_price,\n            strike_price=k,\n            time_to_expiry=time_to_expiry,\n            risk_free_rate=risk_free_rate,\n            volatility=volatility,\n            dividend_yield=dividend_yield,\n            simulations=simulations,\n            time_steps=time_steps,\n            antithetic=True,\n            seed=seed\n        )\n        moneyness_prices[option_type].append(result['price'])\n\n# Plot impact of moneyness\nplt.subplot(2, 2, 3)\nfor option_type in moneyness_prices.keys():\n    plt.plot(moneyness_levels, moneyness_prices[option_type], marker='o', label=option_type.replace('_', ' ').title())\nplt.grid(True, alpha=0.3)\nplt.xlabel('Moneyness (Strike/Underlying)')\nplt.ylabel('Option Price ($)')\nplt.title('Option Prices vs. Moneyness')\nplt.legend()\n\n# Analyze impact of jump parameters on Bitcoin option\njump_intensities = np.linspace(0, 24, 9)  # 0 to 24 jumps per year\njump_prices = []\n\nfor intensity in jump_intensities:\n    result = monte_carlo_option_pricing(\n        option_type='european_call',\n        underlying_price=40000,\n        strike_price=45000,\n        time_to_expiry=0.25,\n        risk_free_rate=0.05,\n        volatility=0.85,\n        simulations=10000,\n        time_steps=90,\n        jump_intensity=intensity,\n        jump_mean=-0.05,\n        jump_std=0.15,\n        seed=seed\n    )\n    jump_prices.append(result['price'])\n\n# Plot impact of jump intensity\nplt.subplot(2, 2, 4)\nplt.plot(jump_intensities, jump_prices, 'r-', marker='o', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Jump Intensity (jumps per year)')\nplt.ylabel('Option Price ($)')\nplt.title('Bitcoin Option Price vs. Jump Intensity')\n\nplt.tight_layout()\nplt.show()\n\n# Generate and visualize sample price paths\nnp.random.seed(seed)\nsample_paths = 5\ndt = time_to_expiry / time_steps\ndrift = (risk_free_rate - dividend_yield - 0.5 * volatility**2) * dt\nvol_sqrt_dt = volatility * np.sqrt(dt)\n\n# Initialize price paths\npaths = np.zeros((sample_paths, time_steps + 1))\npaths[:, 0] = underlying_price\n\n# Generate random samples\nrandom_samples = np.random.normal(0, 1, (sample_paths, time_steps))\n\n# Simulate price paths\nfor t in range(1, time_steps + 1):\n    paths[:, t] = paths[:, t-1] * np.exp(drift + vol_sqrt_dt * random_samples[:, t-1])\n\n# Create time array\ntime_array = np.linspace(0, time_to_expiry, time_steps + 1)\n\n# Plot sample price paths\nplt.figure(figsize=(10, 6))\nfor i in range(sample_paths):\n    plt.plot(time_array, paths[i, :], alpha=0.7)\nplt.axhline(y=strike_price, color='r', linestyle='--', label='Strike Price')\nplt.grid(True, alpha=0.3)\nplt.xlabel('Time (years)')\nplt.ylabel('Asset Price ($)')\nplt.title(f'Sample Price Paths (S\u2080=${underlying_price}, K=${strike_price}, \u03c3={volatility:.1f})')\nplt.legend()\nplt.show()\n\n# Generate and visualize Bitcoin price paths with jumps\nnp.random.seed(seed)\nbtc_price = 40000\nbtc_vol = 0.85\nbtc_time = 0.25\nbtc_steps = 90\nbtc_drift = (risk_free_rate - 0.5 * btc_vol**2) * (btc_time / btc_steps)\nbtc_vol_sqrt_dt = btc_vol * np.sqrt(btc_time / btc_steps)\njump_intensity = 12\njump_mean = -0.05\njump_std = 0.15\n\n# Initialize price paths\nbtc_paths_no_jumps = np.zeros((sample_paths, btc_steps + 1))\nbtc_paths_with_jumps = np.zeros((sample_paths, btc_steps + 1))\nbtc_paths_no_jumps[:, 0] = btc_price\nbtc_paths_with_jumps[:, 0] = btc_price\n\n# Generate random samples\nbtc_random_samples = np.random.normal(0, 1, (sample_paths, btc_steps))\n\n# Simulate price paths\nfor t in range(1, btc_steps + 1):\n    # Without jumps\n    btc_paths_no_jumps[:, t] = btc_paths_no_jumps[:, t-1] * np.exp(btc_drift + btc_vol_sqrt_dt * btc_random_samples[:, t-1])\n\n    # With jumps\n    btc_paths_with_jumps[:, t] = btc_paths_with_jumps[:, t-1] * np.exp(btc_drift + btc_vol_sqrt_dt * btc_random_samples[:, t-1])\n\n    # Add jumps\n    lambda_dt = jump_intensity * (btc_time / btc_steps)\n    for i in range(sample_paths):\n        jump_count = np.random.poisson(lambda_dt)\n        if jump_count &gt; 0:\n            jump_sizes = np.random.normal(jump_mean, jump_std, jump_count)\n            total_jump = np.sum(jump_sizes)\n            btc_paths_with_jumps[i, t] *= np.exp(total_jump)\n\n# Create time array\nbtc_time_array = np.linspace(0, btc_time, btc_steps + 1)\n\n# Plot Bitcoin price paths\nplt.figure(figsize=(12, 8))\nplt.subplot(1, 2, 1)\nfor i in range(sample_paths):\n    plt.plot(btc_time_array, btc_paths_no_jumps[i, :], alpha=0.7)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Time (years)')\nplt.ylabel('Bitcoin Price ($)')\nplt.title('Bitcoin Price Paths - No Jumps')\n\nplt.subplot(1, 2, 2)\nfor i in range(sample_paths):\n    plt.plot(btc_time_array, btc_paths_with_jumps[i, :], alpha=0.7)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Time (years)')\nplt.ylabel('Bitcoin Price ($)')\nplt.title('Bitcoin Price Paths - With Jumps')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/asset/monte_carlo/#example-output","title":"Example Output","text":"<pre><code>Monte Carlo Option Pricing Results:\nOption Type     Price      Std Error  95% CI              \n-------------------------------------------------------\neuropean_call   $9.73      $0.1428    [$9.45, $10.01]\neuropean_put    $5.86      $0.0891    [$5.69, $6.03]\nasian_call      $5.53      $0.0791    [$5.38, $5.69]\nasian_put       $3.58      $0.0549    [$3.48, $3.69]\nlookback_call   $15.95     $0.1418    [$15.67, $16.23]\nlookback_put    $13.82     $0.0996    [$13.62, $14.01]\n</code></pre>"},{"location":"user-guide/asset/monte_carlo/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/asset/monte_carlo/#option-prices-vs-volatility","title":"Option Prices vs. Volatility","text":"<p>This chart shows how option prices change with volatility for different option types. All option prices increase with volatility, but at different rates.</p> <p></p>"},{"location":"user-guide/asset/monte_carlo/#option-prices-vs-time-to-expiry","title":"Option Prices vs. Time to Expiry","text":"<p>This chart illustrates how option prices change with time to expiration. European and Asian options typically increase with time, while lookback options show a more pronounced increase.</p>"},{"location":"user-guide/asset/monte_carlo/#option-prices-vs-moneyness","title":"Option Prices vs. Moneyness","text":"<p>This chart demonstrates how call and put option prices change with moneyness (strike price relative to underlying price). Call options decrease with increasing strike price, while put options increase.</p>"},{"location":"user-guide/asset/monte_carlo/#bitcoin-option-price-vs-jump-intensity","title":"Bitcoin Option Price vs. Jump Intensity","text":"<p>This chart shows how the price of a Bitcoin option increases with jump intensity, reflecting the higher risk premium required for assets with more frequent jumps.</p> <p></p>"},{"location":"user-guide/asset/monte_carlo/#sample-price-paths","title":"Sample Price Paths","text":"<p>These charts display sample price paths generated by the Monte Carlo simulation, illustrating the difference between standard geometric Brownian motion and jump diffusion models.</p>"},{"location":"user-guide/asset/monte_carlo/#theoretical-background","title":"Theoretical Background","text":"<p>The Monte Carlo method for option pricing is based on the risk-neutral valuation principle, which states that the price of an option is the expected value of its future payoff, discounted at the risk-free rate.</p> <p>For the standard model, the underlying asset price follows geometric Brownian motion.</p>"},{"location":"user-guide/asset/monte_carlo/#variance-reduction-techniques","title":"Variance Reduction Techniques","text":"<p>The implementation uses antithetic variates as a variance reduction technique. This method generates pairs of negatively correlated paths by using both \\(Z\\) and \\(-Z\\) in the simulation, which helps reduce the standard error of the price estimate.</p>"},{"location":"user-guide/asset/monte_carlo/#practical-applications","title":"Practical Applications","text":"<p>Monte Carlo option pricing is used for:</p> <ol> <li>Exotic Option Pricing: Valuing options with complex payoff structures</li> <li>Path-Dependent Options: Pricing options whose payoff depends on the price path (e.g., Asian, lookback)</li> <li>Multi-Asset Options: Valuing options on multiple underlying assets</li> <li>Risk Management: Assessing option portfolio risk under various scenarios</li> <li>Model Validation: Benchmarking against closed-form solutions where available</li> <li>Sensitivity Analysis: Analyzing option price sensitivity to various parameters</li> <li>Cryptocurrency Options: Pricing options on highly volatile digital assets using jump diffusion</li> </ol>"},{"location":"user-guide/asset/monte_carlo/#limitations","title":"Limitations","text":"<p>The Monte Carlo method has several limitations:</p> <ol> <li>Computational Intensity: Requires many simulations for accurate pricing</li> <li>Convergence Rate: Converges relatively slowly (error decreases with \\(1/\\sqrt{n}\\))</li> <li>Model Risk: Results depend on the assumed price process</li> <li>Parameter Sensitivity: Sensitive to volatility and other input parameters</li> <li>Early Exercise: Basic implementation doesn't handle American options</li> <li>Greeks Calculation: Calculating Greeks requires additional techniques</li> <li>Jump Parameter Estimation: For jump diffusion, accurately estimating jump parameters can be challenging</li> </ol>"},{"location":"user-guide/asset/monte_carlo/#extensions","title":"Extensions","text":"<p>Several extensions to the basic Monte Carlo method address its limitations:</p> <ol> <li>Quasi-Monte Carlo: Using low-discrepancy sequences for faster convergence</li> <li>Additional Variance Reduction: Control variates, importance sampling, stratified sampling</li> <li>American Option Pricing: Least-squares Monte Carlo (Longstaff-Schwartz method)</li> <li>Greeks Calculation: Pathwise derivatives, likelihood ratio method</li> <li>Jump Diffusion: Incorporating jumps in the price process (implemented)</li> <li>Stochastic Volatility: Allowing for time-varying volatility</li> <li>Local Volatility: Using volatility that depends on both time and asset price</li> <li>Multi-Factor Models: Incorporating multiple sources of uncertainty </li> </ol>"},{"location":"user-guide/asset/risk_neutral/","title":"Risk-Neutral Valuation","text":"<p>The <code>risk_neutral_valuation</code> function implements a flexible Monte Carlo simulation approach for pricing derivatives using the risk-neutral valuation principle. This function allows you to price any derivative by providing a custom payoff function, making it extremely versatile for various financial instruments.</p>"},{"location":"user-guide/asset/risk_neutral/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.asset import risk_neutral_valuation\n\n# Define a call option payoff function\ndef call_payoff(s, k=100):\n    return max(0, s - k)\n\n# Price a call option\nresult = risk_neutral_valuation(\n    payoff_function=call_payoff,\n    underlying_price=100,\n    risk_free_rate=0.05,\n    volatility=0.2,\n    time_to_expiry=1.0,\n    steps=1000,\n    simulations=10000,\n    dividend_yield=0.0,\n    seed=42\n)\n\n# Access the results\noption_price = result[\"price\"]\nstandard_error = result[\"standard_error\"]\nconfidence_interval = result[\"confidence_interval\"]\n\nprint(f\"Option Price: ${option_price:.2f}\")\nprint(f\"Standard Error: ${standard_error:.4f}\")\nprint(f\"95% Confidence Interval: [${confidence_interval[0]:.2f}, ${confidence_interval[1]:.2f}]\")\n</code></pre>"},{"location":"user-guide/asset/risk_neutral/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>payoff_function</code> callable Function that takes the final underlying price and returns the payoff Required <code>underlying_price</code> float Current price of the underlying asset Required <code>risk_free_rate</code> float Risk-free interest rate (annualized) Required <code>volatility</code> float Volatility of the underlying asset (annualized) Required <code>time_to_expiry</code> float Time to expiration in years Required <code>steps</code> int Number of time steps in the simulation 1000 <code>simulations</code> int Number of Monte Carlo simulations 10000 <code>dividend_yield</code> float Continuous dividend yield 0.0 <code>seed</code> int Random seed for reproducibility None"},{"location":"user-guide/asset/risk_neutral/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>price</code> float Calculated derivative price <code>standard_error</code> float Standard error of the price estimate <code>confidence_interval</code> tuple 95% confidence interval for the price (lower, upper) <code>underlying_price</code> float Price of the underlying asset used in calculation <code>risk_free_rate</code> float Risk-free rate used in calculation <code>volatility</code> float Volatility used in calculation <code>time_to_expiry</code> float Time to expiration used in calculation <code>steps</code> int Number of time steps used <code>simulations</code> int Number of simulations used <code>dividend_yield</code> float Dividend yield used in calculation <code>price_statistics</code> dict Statistics about the simulated final prices (mean, std, min, max, median) <code>payoff_statistics</code> dict Statistics about the payoffs (mean, std, min, max, median, zero_proportion)"},{"location":"user-guide/asset/risk_neutral/#risk-level-classification","title":"Risk Level Classification","text":"<p>The risk level of a derivative can be classified based on its price volatility relative to the underlying asset:</p> Risk Level Description Volatility Ratio Very Low Minimal price fluctuation relative to underlying &lt; 0.5 Low Limited price fluctuation relative to underlying 0.5 - 1.0 Moderate Similar price fluctuation to underlying 1.0 - 1.5 High Greater price fluctuation than underlying 1.5 - 2.5 Very High Significantly greater price fluctuation than underlying &gt; 2.5 <p>Where the volatility ratio is calculated as: (Payoff Standard Deviation / Mean Final Price) / (Underlying Volatility)</p>"},{"location":"user-guide/asset/risk_neutral/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to use risk-neutral valuation for various derivative types and analysis:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom pypulate.asset import risk_neutral_valuation\n\n# Define parameters for analysis\nunderlying_price = 100\nstrike_price = 100\nrisk_free_rate = 0.05\nvolatility = 0.2\ntime_to_expiry = 1.0\ndividend_yield = 0.01\nsteps = 100\nsimulations = 1000\nseed = 42\n\n# Define different payoff functions\ndef european_call(s, k=strike_price):\n    return max(0, s - k)\n\ndef european_put(s, k=strike_price):\n    return max(0, k - s)\n\ndef binary_call(s, k=strike_price):\n    return 1 if s &gt; k else 0\n\ndef binary_put(s, k=strike_price):\n    return 1 if s &lt; k else 0\n\ndef straddle(s, k=strike_price):\n    return abs(s - k)\n\ndef strangle(s, k1=strike_price-10, k2=strike_price+10):\n    return max(0, s - k2) + max(0, k1 - s)\n\ndef butterfly(s, k=strike_price, width=10):\n    return max(0, s - (k-width)) - 2*max(0, s - k) + max(0, s - (k+width))\n\n# Dictionary of payoff functions\npayoff_functions = {\n    \"European Call\": european_call,\n    \"European Put\": european_put,\n    \"Binary Call\": binary_call,\n    \"Binary Put\": binary_put,\n    \"Straddle\": straddle,\n    \"Strangle\": strangle,\n    \"Butterfly\": butterfly\n}\n\n# Calculate prices for all derivative types\nresults = {}\nfor name, payoff_function in payoff_functions.items():\n    result = risk_neutral_valuation(\n        payoff_function=payoff_function,\n        underlying_price=underlying_price,\n        risk_free_rate=risk_free_rate,\n        volatility=volatility,\n        time_to_expiry=time_to_expiry,\n        steps=steps,\n        simulations=simulations,\n        dividend_yield=dividend_yield,\n        seed=seed\n    )\n    results[name] = result\n\n# Print results\nprint(\"Risk-Neutral Valuation Results:\")\nprint(f\"{'Derivative Type':&lt;15} {'Price':&lt;10} {'Std Error':&lt;10} {'95% CI':&lt;20}\")\nprint(\"-\" * 55)\nfor name, result in results.items():\n    ci_low, ci_high = result['confidence_interval']\n    print(f\"{name:&lt;15} ${result['price']:&lt;9.2f} ${result['standard_error']:&lt;9.4f} [${ci_low:.2f}, ${ci_high:.2f}]\")\n\n# Analyze impact of volatility on derivative prices\nvolatilities = np.linspace(0.1, 0.5, 9)  # 10% to 50% volatility\nvol_prices = {name: [] for name in payoff_functions.keys()}\n\nfor vol in volatilities:\n    for name, payoff_function in payoff_functions.items():\n        result = risk_neutral_valuation(\n            payoff_function=payoff_function,\n            underlying_price=underlying_price,\n            risk_free_rate=risk_free_rate,\n            volatility=vol,\n            time_to_expiry=time_to_expiry,\n            steps=steps,\n            simulations=simulations,\n            dividend_yield=dividend_yield,\n            seed=seed\n        )\n        vol_prices[name].append(result['price'])\n\n# Visualize results\nplt.figure(figsize=(15, 10))\n\n# Plot derivative prices vs volatility\nplt.subplot(2, 2, 1)\nfor name, prices in vol_prices.items():\n    plt.plot(volatilities, prices, marker='o', label=name)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Volatility')\nplt.ylabel('Derivative Price ($)')\nplt.title('Derivative Prices vs. Volatility')\nplt.legend()\n\n# Analyze impact of time to expiry\ntimes = np.linspace(0.25, 2.0, 8)  # 3 months to 2 years\ntime_prices = {name: [] for name in payoff_functions.keys()}\n\nfor t in times:\n    for name, payoff_function in payoff_functions.items():\n        result = risk_neutral_valuation(\n            payoff_function=payoff_function,\n            underlying_price=underlying_price,\n            risk_free_rate=risk_free_rate,\n            volatility=volatility,\n            time_to_expiry=t,\n            steps=max(int(t * 1000), 10),  # Scale time steps with expiry\n            simulations=simulations,\n            dividend_yield=dividend_yield,\n            seed=seed\n        )\n        time_prices[name].append(result['price'])\n\n# Plot impact of time to expiry\nplt.subplot(2, 2, 2)\nfor name, prices in time_prices.items():\n    plt.plot(times, prices, marker='o', label=name)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Time to Expiry (years)')\nplt.ylabel('Derivative Price ($)')\nplt.title('Derivative Prices vs. Time to Expiry')\nplt.legend()\n\n# Analyze impact of moneyness (strike price relative to underlying)\nmoneyness_levels = np.linspace(0.7, 1.3, 7)  # 70% to 130% of underlying price\nstrikes = underlying_price * moneyness_levels\nmoneyness_prices = {name: [] for name in [\"European Call\", \"European Put\"]}  # Just analyze these two\n\nfor k in strikes:\n    for name in moneyness_prices.keys():\n        if name == \"European Call\":\n            payoff_function = lambda s, k=k: max(0, s - k)\n        else:  # European Put\n            payoff_function = lambda s, k=k: max(0, k - s)\n\n        result = risk_neutral_valuation(\n            payoff_function=payoff_function,\n            underlying_price=underlying_price,\n            risk_free_rate=risk_free_rate,\n            volatility=volatility,\n            time_to_expiry=time_to_expiry,\n            steps=steps,\n            simulations=simulations,\n            dividend_yield=dividend_yield,\n            seed=seed\n        )\n        moneyness_prices[name].append(result['price'])\n\n# Plot impact of moneyness\nplt.subplot(2, 2, 3)\nfor name, prices in moneyness_prices.items():\n    plt.plot(moneyness_levels, prices, marker='o', label=name)\nplt.grid(True, alpha=0.3)\nplt.xlabel('Moneyness (Strike/Underlying)')\nplt.ylabel('Option Price ($)')\nplt.title('Option Prices vs. Moneyness')\nplt.legend()\n\n# Analyze convergence with increasing simulations\nsim_counts = [100, 500, 1000, 5000, 10000, 50000]\nconvergence = {name: {'prices': [], 'errors': []} for name in [\"European Call\", \"European Put\"]}\n\nfor sim_count in sim_counts:\n    for name in convergence.keys():\n        if name == \"European Call\":\n            payoff_function = european_call\n        else:  # European Put\n            payoff_function = european_put\n\n        result = risk_neutral_valuation(\n            payoff_function=payoff_function,\n            underlying_price=underlying_price,\n            risk_free_rate=risk_free_rate,\n            volatility=volatility,\n            time_to_expiry=time_to_expiry,\n            steps=steps,\n            simulations=sim_count,\n            dividend_yield=dividend_yield,\n            seed=seed\n        )\n        convergence[name]['prices'].append(result['price'])\n        convergence[name]['errors'].append(result['standard_error'])\n\n# Plot convergence\nplt.subplot(2, 2, 4)\nfor name in convergence.keys():\n    plt.errorbar(\n        sim_counts, \n        convergence[name]['prices'], \n        yerr=convergence[name]['errors'],\n        marker='o', \n        label=name\n    )\nplt.xscale('log')\nplt.grid(True, alpha=0.3)\nplt.xlabel('Number of Simulations (log scale)')\nplt.ylabel('Option Price ($)')\nplt.title('Convergence with Increasing Simulations')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Generate and visualize sample price paths\nnp.random.seed(seed)\nsample_paths = 5\ndt = time_to_expiry / steps\ndrift = (risk_free_rate - dividend_yield - 0.5 * volatility**2) * dt\nvol_sqrt_dt = volatility * np.sqrt(dt)\n\n# Initialize price paths\npaths = np.zeros((sample_paths, steps + 1))\npaths[:, 0] = underlying_price\n\n# Generate random samples\nrandom_samples = np.random.normal(0, 1, (sample_paths, steps))\n\n# Simulate price paths\nfor t in range(1, steps + 1):\n    paths[:, t] = paths[:, t-1] * np.exp(drift + vol_sqrt_dt * random_samples[:, t-1])\n\n# Create time array\ntime_array = np.linspace(0, time_to_expiry, steps + 1)\n\n# Plot sample price paths\nplt.figure(figsize=(10, 6))\nfor i in range(sample_paths):\n    plt.plot(time_array, paths[i, :], alpha=0.7)\nplt.axhline(y=strike_price, color='r', linestyle='--', label='Strike Price')\nplt.grid(True, alpha=0.3)\nplt.xlabel('Time (years)')\nplt.ylabel('Asset Price ($)')\nplt.title(f'Sample Price Paths (S\u2080=${underlying_price}, K=${strike_price}, \u03c3={volatility:.1f})')\nplt.legend()\nplt.show()\n\n# Visualize payoff functions\nspot_prices = np.linspace(50, 150, 100)\npayoffs = {name: [func(s) for s in spot_prices] for name, func in payoff_functions.items()}\n\nplt.figure(figsize=(15, 10))\nfor i, (name, values) in enumerate(payoffs.items()):\n    plt.subplot(2, 4, i+1)\n    plt.plot(spot_prices, values)\n    plt.axvline(x=strike_price, color='r', linestyle='--', alpha=0.5)\n    plt.grid(True, alpha=0.3)\n    plt.xlabel('Spot Price at Expiry')\n    plt.ylabel('Payoff')\n    plt.title(name)\n\nplt.tight_layout()\nplt.show()\n\n# Compare derivative prices\nderivative_names = list(results.keys())\nprices = [results[name]['price'] for name in derivative_names]\n\nplt.figure(figsize=(12, 6))\nbars = plt.bar(derivative_names, prices, color='skyblue')\nplt.grid(True, alpha=0.3, axis='y')\nplt.xlabel('Derivative Type')\nplt.ylabel('Price ($)')\nplt.title('Comparison of Derivative Prices')\n\n# Add price labels on top of bars\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n             f'${height:.2f}', ha='center', va='bottom')\n\nplt.show()\n</code></pre>"},{"location":"user-guide/asset/risk_neutral/#example-output","title":"Example Output","text":"<pre><code>Risk-Neutral Valuation Results:\nDerivative Type Price      Std Error  95% CI              \n-------------------------------------------------------\nEuropean Call   $10.25     $0.4843    [$9.30, $11.20]\nEuropean Put    $6.00      $0.2929    [$5.42, $6.57]\nBinary Call     $0.52      $0.0150    [$0.49, $0.55]\nBinary Put      $0.43      $0.0150    [$0.41, $0.46]\nStraddle        $16.24     $0.4443    [$15.37, $17.11]\nStrangle        $8.68      $0.3933    [$7.91, $9.45]\nButterfly       $1.95      $0.0943    [$1.76, $2.13]\n</code></pre>"},{"location":"user-guide/asset/risk_neutral/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/asset/risk_neutral/#derivative-prices-vs-volatility","title":"Derivative Prices vs. Volatility","text":"<p>This chart shows how derivative prices change with volatility. Options like calls, puts, and straddles increase with volatility, while binary options show less sensitivity.</p> <p></p>"},{"location":"user-guide/asset/risk_neutral/#derivative-prices-vs-time-to-expiry","title":"Derivative Prices vs. Time to Expiry","text":"<p>This chart illustrates how derivative prices change with time to expiration. European options typically increase with time, while binary options may decrease.</p>"},{"location":"user-guide/asset/risk_neutral/#option-prices-vs-moneyness","title":"Option Prices vs. Moneyness","text":"<p>This chart demonstrates how call and put option prices change with moneyness (strike price relative to underlying price). Call options decrease with increasing strike price, while put options increase.</p>"},{"location":"user-guide/asset/risk_neutral/#convergence-with-increasing-simulations","title":"Convergence with Increasing Simulations","text":"<p>This chart shows how derivative price estimates converge and standard errors decrease as the number of simulations increases.</p>"},{"location":"user-guide/asset/risk_neutral/#sample-price-paths","title":"Sample Price Paths","text":"<p>This chart displays sample price paths generated by the Monte Carlo simulation, illustrating the stochastic nature of asset price movements.</p>"},{"location":"user-guide/asset/risk_neutral/#payoff-functions","title":"Payoff Functions","text":"<p>These charts visualize the payoff functions for different derivative types at expiration, showing how the payoff varies with the final underlying price.</p>"},{"location":"user-guide/asset/risk_neutral/#comparison-of-derivative-prices","title":"Comparison of Derivative Prices","text":"<p>This bar chart compares the prices of different derivative types under the same market conditions, highlighting their relative values.</p> <p></p>"},{"location":"user-guide/asset/risk_neutral/#theoretical-background","title":"Theoretical Background","text":"<p>Risk-neutral valuation is based on the fundamental principle that in a risk-neutral world, all assets earn the risk-free rate, and the price of a derivative is the expected value of its future payoff, discounted at the risk-free rate.</p>"},{"location":"user-guide/asset/risk_neutral/#practical-applications","title":"Practical Applications","text":"<p>Risk-neutral valuation is used for:</p> <ol> <li>Exotic Derivative Pricing: Valuing derivatives with complex payoff structures</li> <li>Path-Dependent Derivatives: Pricing derivatives whose payoff depends on the price path</li> <li>Multi-Asset Derivatives: Valuing derivatives on multiple underlying assets</li> <li>Risk Management: Assessing derivative portfolio risk under various scenarios</li> <li>Model Validation: Benchmarking against closed-form solutions where available</li> <li>Sensitivity Analysis: Analyzing derivative price sensitivity to various parameters</li> <li>Structured Products: Pricing complex financial products with embedded derivatives</li> <li>Real Options: Valuing real investment opportunities with option-like characteristics</li> </ol>"},{"location":"user-guide/asset/risk_neutral/#limitations","title":"Limitations","text":"<p>The risk-neutral valuation method has several limitations:</p> <ol> <li>Computational Intensity: Requires many simulations for accurate pricing</li> <li>Convergence Rate: Converges relatively slowly (error decreases with 1/\u221an)</li> <li>Model Risk: Results depend on the assumed price process</li> <li>Parameter Sensitivity: Sensitive to volatility and other input parameters</li> <li>Early Exercise: Basic implementation doesn't handle American options</li> <li>Greeks Calculation: Calculating Greeks requires additional techniques</li> </ol>"},{"location":"user-guide/asset/risk_neutral/#extensions","title":"Extensions","text":"<p>Several extensions to the basic risk-neutral valuation method address its limitations:</p> <ol> <li>Quasi-Monte Carlo: Using low-discrepancy sequences for faster convergence</li> <li>Additional Variance Reduction: Control variates, importance sampling, stratified sampling</li> <li>American Option Pricing: Least-squares Monte Carlo (Longstaff-Schwartz method)</li> <li>Greeks Calculation: Pathwise derivatives, likelihood ratio method</li> <li>Jump Diffusion: Incorporating jumps in the price process</li> <li>Stochastic Volatility: Allowing for time-varying volatility</li> <li>Local Volatility: Using volatility that depends on both time and asset price</li> <li>Multi-Factor Models: Incorporating multiple sources of uncertainty </li> </ol>"},{"location":"user-guide/asset/term_structure/","title":"Term Structure Models","text":"<p>The term structure module provides functions for fitting yield curve models to market data. These models are essential for understanding the relationship between interest rates and maturities, which is crucial for bond pricing, risk management, and monetary policy analysis.</p>"},{"location":"user-guide/asset/term_structure/#models-available","title":"Models Available","text":"<p>Pypulate implements two widely-used parametric models for yield curve fitting:</p> <ol> <li>Nelson-Siegel Model: A three-factor model that can capture monotonic, humped, and S-shaped yield curves.</li> <li>Svensson Model: An extension of the Nelson-Siegel model with an additional factor to better fit complex yield curve shapes.</li> </ol>"},{"location":"user-guide/asset/term_structure/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.asset import nelson_siegel, svensson\n\n# Sample yield curve data\nmaturities = [0.25, 0.5, 1, 2, 3, 5, 7, 10, 20, 30]  # in years\nrates = [0.01, 0.015, 0.02, 0.025, 0.028, 0.03, 0.031, 0.032, 0.033, 0.034]  # as decimals\n\n# Fit the Nelson-Siegel model\nns_result = nelson_siegel(maturities, rates)\n\n# Get the fitted parameters\nbeta0, beta1, beta2, tau = ns_result['parameters']\nprint(f\"Nelson-Siegel parameters:\")\nprint(f\"\u03b2\u2080 (long-term rate): {beta0:.4f}\")\nprint(f\"\u03b2\u2081 (short-term component): {beta1:.4f}\")\nprint(f\"\u03b2\u2082 (medium-term component): {beta2:.4f}\")\nprint(f\"\u03c4 (time decay factor): {tau:.4f}\")\n\n# Predict rates at specific maturities\npredict_ns = ns_result['predict_func']\nrate_4y = predict_ns(4)\nprint(f\"Predicted 4-year rate: {rate_4y:.2%}\")\n\n# Fit the Svensson model (for more complex yield curves)\nsv_result = svensson(maturities, rates)\n\n# Get the fitted parameters\nbeta0, beta1, beta2, beta3, tau1, tau2 = sv_result['parameters']\nprint(f\"\\nSvensson parameters:\")\nprint(f\"\u03b2\u2080 (long-term rate): {beta0:.4f}\")\nprint(f\"\u03b2\u2081 (short-term component): {beta1:.4f}\")\nprint(f\"\u03b2\u2082 (medium-term component): {beta2:.4f}\")\nprint(f\"\u03b2\u2083 (additional medium-term component): {beta3:.4f}\")\nprint(f\"\u03c4\u2081 (first time decay factor): {tau1:.4f}\")\nprint(f\"\u03c4\u2082 (second time decay factor): {tau2:.4f}\")\n\n# Predict rates using the Svensson model\npredict_sv = sv_result['predict_func']\nrate_4y_sv = predict_sv(4)\nprint(f\"Predicted 4-year rate (Svensson): {rate_4y_sv:.2%}\")\n\n# Compare model fit\nprint(f\"\\nModel fit comparison:\")\nprint(f\"Nelson-Siegel R\u00b2: {ns_result['r_squared']:.4f}, RMSE: {ns_result['rmse']:.6f}\")\nprint(f\"Svensson R\u00b2: {sv_result['r_squared']:.4f}, RMSE: {sv_result['rmse']:.6f}\")\n</code></pre>"},{"location":"user-guide/asset/term_structure/#nelson-siegel-model","title":"Nelson-Siegel Model","text":""},{"location":"user-guide/asset/term_structure/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>maturities</code> list of float Maturities in years for the observed rates Required <code>rates</code> list of float Observed interest rates (as decimals) Required <code>initial_params</code> list of float Initial parameters [\u03b2\u2080, \u03b2\u2081, \u03b2\u2082, \u03c4] [0.03, -0.02, -0.01, 1.5]"},{"location":"user-guide/asset/term_structure/#return-value","title":"Return Value","text":"<p>The <code>nelson_siegel</code> function returns a dictionary with the following keys:</p> Key Type Description <code>parameters</code> list of float Fitted parameters [\u03b2\u2080, \u03b2\u2081, \u03b2\u2082, \u03c4] <code>parameter_names</code> list of str Names of the parameters [\"beta0\", \"beta1\", \"beta2\", \"tau\"] <code>predict_func</code> callable Function that takes a maturity and returns the predicted rate <code>fitted_rates</code> list of float Predicted rates at the input maturities <code>residuals</code> list of float Differences between observed and fitted rates <code>r_squared</code> float Coefficient of determination (goodness of fit) <code>rmse</code> float Root mean square error <code>short_rate</code> float Estimated short-term interest rate <code>long_rate</code> float Estimated long-term interest rate (equals \u03b2\u2080) <code>maturities</code> list of float Input maturities <code>rates</code> list of float Input rates"},{"location":"user-guide/asset/term_structure/#model-formula","title":"Model Formula","text":"<p>The Nelson-Siegel model is defined by the following equation:</p> \\[r(t) = \\beta_0 + \\beta_1 \\cdot \\frac{1 - e^{-t/\\tau}}{t/\\tau} + \\beta_2 \\cdot \\left(\\frac{1 - e^{-t/\\tau}}{t/\\tau} - e^{-t/\\tau}\\right)\\] <p>Where: - \\(r(t)\\) is the interest rate at maturity \\(t\\) - \\(\\beta_0\\) represents the long-term interest rate - \\(\\beta_1\\) represents the short-term component - \\(\\beta_2\\) represents the medium-term component - \\(\\tau\\) is the time decay factor that determines the rate at which the short and medium-term components decay</p>"},{"location":"user-guide/asset/term_structure/#svensson-model","title":"Svensson Model","text":""},{"location":"user-guide/asset/term_structure/#parameters_1","title":"Parameters","text":"Parameter Type Description Default <code>maturities</code> list of float Maturities in years for the observed rates Required <code>rates</code> list of float Observed interest rates (as decimals) Required <code>initial_params</code> list of float Initial parameters [\u03b2\u2080, \u03b2\u2081, \u03b2\u2082, \u03b2\u2083, \u03c4\u2081, \u03c4\u2082] [0.03, -0.02, -0.01, 0.01, 1.5, 10]"},{"location":"user-guide/asset/term_structure/#return-value_1","title":"Return Value","text":"<p>The <code>svensson</code> function returns a dictionary with the following keys:</p> Key Type Description <code>parameters</code> list of float Fitted parameters [\u03b2\u2080, \u03b2\u2081, \u03b2\u2082, \u03b2\u2083, \u03c4\u2081, \u03c4\u2082] <code>parameter_names</code> list of str Names of the parameters [\"beta0\", \"beta1\", \"beta2\", \"beta3\", \"tau1\", \"tau2\"] <code>predict_func</code> callable Function that takes a maturity and returns the predicted rate <code>fitted_rates</code> list of float Predicted rates at the input maturities <code>residuals</code> list of float Differences between observed and fitted rates <code>r_squared</code> float Coefficient of determination (goodness of fit) <code>rmse</code> float Root mean square error <code>short_rate</code> float Estimated short-term interest rate <code>long_rate</code> float Estimated long-term interest rate (equals \u03b2\u2080) <code>maturities</code> list of float Input maturities <code>rates</code> list of float Input rates"},{"location":"user-guide/asset/term_structure/#model-formula_1","title":"Model Formula","text":"<p>The Svensson model extends the Nelson-Siegel model with an additional term:</p> \\[r(t) = \\beta_0 + \\beta_1 \\cdot \\frac{1 - e^{-t/\\tau_1}}{t/\\tau_1} + \\beta_2 \\cdot \\left(\\frac{1 - e^{-t/\\tau_1}}{t/\\tau_1} - e^{-t/\\tau_1}\\right) + \\beta_3 \\cdot \\left(\\frac{1 - e^{-t/\\tau_2}}{t/\\tau_2} - e^{-t/\\tau_2}\\right)\\] <p>Where: - \\(r(t)\\) is the interest rate at maturity \\(t\\) - \\(\\beta_0\\) represents the long-term interest rate - \\(\\beta_1\\) represents the short-term component - \\(\\beta_2\\) and \\(\\beta_3\\) represent medium-term components - \\(\\tau_1\\) and \\(\\tau_2\\) are time decay factors</p>"},{"location":"user-guide/asset/term_structure/#model-selection","title":"Model Selection","text":"Model Complexity Data Points Required Best Used For Nelson-Siegel Lower (4 parameters) At least 4 Simple yield curves with one hump Svensson Higher (6 parameters) At least 6 Complex yield curves with multiple humps"},{"location":"user-guide/asset/term_structure/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to fit yield curve models and visualize the results:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom pypulate.asset import nelson_siegel, svensson\n\n# Sample yield curve data (US Treasury yields as of a sample date)\nmaturities = [0.25, 0.5, 1, 2, 3, 5, 7, 10, 20, 30]\nrates = [0.01, 0.015, 0.02, 0.025, 0.028, 0.03, 0.031, 0.032, 0.033, 0.034]\n\n# Fit both models\nns_result = nelson_siegel(maturities, rates)\nsv_result = svensson(maturities, rates)\n\n# Create a dense set of maturities for smooth curve plotting\ndense_maturities = np.linspace(0.1, 30, 100)\n\n# Get predicted rates for both models\nns_rates = [ns_result['predict_func'](t) for t in dense_maturities]\nsv_rates = [sv_result['predict_func'](t) for t in dense_maturities]\n\n# Plot the results\nplt.figure(figsize=(12, 8))\n\n# Plot the actual data points\nplt.scatter(maturities, rates, color='black', label='Observed Rates', zorder=3)\n\n# Plot the fitted curves\nplt.plot(dense_maturities, ns_rates, 'b-', label=f'Nelson-Siegel (R\u00b2={ns_result[\"r_squared\"]:.4f})', linewidth=2)\nplt.plot(dense_maturities, sv_rates, 'r--', label=f'Svensson (R\u00b2={sv_result[\"r_squared\"]:.4f})', linewidth=2)\n\n# Add labels and title\nplt.xlabel('Maturity (years)')\nplt.ylabel('Interest Rate')\nplt.title('Yield Curve Fitting with Nelson-Siegel and Svensson Models')\nplt.grid(True, alpha=0.3)\nplt.legend()\n\n# Format y-axis as percentage\nplt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.1%}'))\n\n# Add annotations for model parameters\nns_params = ns_result['parameters']\nsv_params = sv_result['parameters']\n\nns_text = f\"Nelson-Siegel Parameters:\\n\u03b2\u2080={ns_params[0]:.4f}, \u03b2\u2081={ns_params[1]:.4f}\\n\u03b2\u2082={ns_params[2]:.4f}, \u03c4={ns_params[3]:.4f}\"\nsv_text = f\"Svensson Parameters:\\n\u03b2\u2080={sv_params[0]:.4f}, \u03b2\u2081={sv_params[1]:.4f}\\n\u03b2\u2082={sv_params[2]:.4f}, \u03b2\u2083={sv_params[3]:.4f}\\n\u03c4\u2081={sv_params[4]:.4f}, \u03c4\u2082={sv_params[5]:.4f}\"\n\nplt.annotate(ns_text, xy=(0.02, 0.98), xycoords='axes fraction', \n             bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", alpha=0.8),\n             va='top', fontsize=9)\n\nplt.annotate(sv_text, xy=(0.02, 0.80), xycoords='axes fraction', \n             bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", alpha=0.8),\n             va='top', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# Analyze the components of the Nelson-Siegel model\nbeta0, beta1, beta2, tau = ns_result['parameters']\n\n# Calculate the three components\ncomponent1 = np.ones_like(dense_maturities) * beta0  # Long-term component\ncomponent2 = beta1 * (1 - np.exp(-dense_maturities / tau)) / (dense_maturities / tau)  # Short-term component\ncomponent3 = beta2 * ((1 - np.exp(-dense_maturities / tau)) / (dense_maturities / tau) - np.exp(-dense_maturities / tau))  # Medium-term component\n\n# Plot the components\nplt.figure(figsize=(12, 8))\n\nplt.plot(dense_maturities, component1, 'g-', label='Long-term (\u03b2\u2080)', linewidth=2)\nplt.plot(dense_maturities, component2, 'b-', label='Short-term (\u03b2\u2081 term)', linewidth=2)\nplt.plot(dense_maturities, component3, 'r-', label='Medium-term (\u03b2\u2082 term)', linewidth=2)\nplt.plot(dense_maturities, component1 + component2 + component3, 'k--', label='Combined (Nelson-Siegel)', linewidth=2)\nplt.scatter(maturities, rates, color='black', label='Observed Rates', zorder=3)\n\nplt.xlabel('Maturity (years)')\nplt.ylabel('Interest Rate')\nplt.title('Components of the Nelson-Siegel Model')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.1%}'))\n\nplt.tight_layout()\nplt.show()\n\n# Calculate forward rates from the Nelson-Siegel model\ndef ns_forward_rate(t, params):\n    beta0, beta1, beta2, tau = params\n    return beta0 + beta1 * np.exp(-t / tau) + beta2 * (t / tau) * np.exp(-t / tau)\n\nforward_rates = [ns_forward_rate(t, ns_result['parameters']) for t in dense_maturities]\n\n# Plot spot rates vs forward rates\nplt.figure(figsize=(12, 8))\n\nplt.plot(dense_maturities, ns_rates, 'b-', label='Spot Rates (Nelson-Siegel)', linewidth=2)\nplt.plot(dense_maturities, forward_rates, 'r-', label='Forward Rates (Nelson-Siegel)', linewidth=2)\nplt.scatter(maturities, rates, color='black', label='Observed Spot Rates', zorder=3)\n\nplt.xlabel('Maturity (years)')\nplt.ylabel('Interest Rate')\nplt.title('Spot Rates vs. Forward Rates from Nelson-Siegel Model')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.1%}'))\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/asset/term_structure/#example-output","title":"Example Output","text":"<pre><code>Nelson-Siegel parameters:\n\u03b2\u2080 (long-term rate): 0.0340\n\u03b2\u2081 (short-term component): -0.0240\n\u03b2\u2082 (medium-term component): -0.0100\n\u03c4 (time decay factor): 1.5000\n\nPredicted 4-year rate: 2.93%\n\nSvensson parameters:\n\u03b2\u2080 (long-term rate): 0.0340\n\u03b2\u2081 (short-term component): -0.0240\n\u03b2\u2082 (medium-term component): -0.0100\n\u03b2\u2083 (additional medium-term component): 0.0000\n\u03c4\u2081 (first time decay factor): 1.5000\n\u03c4\u2082 (second time decay factor): 10.0000\n\nPredicted 4-year rate (Svensson): 2.93%\n\nModel fit comparison:\nNelson-Siegel R\u00b2: 0.9998, RMSE: 0.000045\nSvensson R\u00b2: 0.9999, RMSE: 0.000030\n</code></pre>"},{"location":"user-guide/asset/term_structure/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/asset/term_structure/#yield-curve-fitting","title":"Yield Curve Fitting","text":"<p>This chart shows the observed yield curve data points and the fitted Nelson-Siegel and Svensson models.</p> <p></p>"},{"location":"user-guide/asset/term_structure/#components-of-the-nelson-siegel-model","title":"Components of the Nelson-Siegel Model","text":"<p>This chart breaks down the Nelson-Siegel model into its three components: long-term (\u03b2\u2080), short-term (\u03b2\u2081 term), and medium-term (\u03b2\u2082 term).</p>"},{"location":"user-guide/asset/term_structure/#spot-rates-vs-forward-rates","title":"Spot Rates vs. Forward Rates","text":"<p>This chart compares the spot rates (yield curve) with the implied forward rates derived from the Nelson-Siegel model.</p> <p></p>"},{"location":"user-guide/asset/term_structure/#practical-applications","title":"Practical Applications","text":"<p>Term structure models are used for:</p> <ol> <li>Yield Curve Construction: Creating a continuous yield curve from discrete market observations</li> <li>Bond Pricing: Discounting cash flows at the appropriate spot rates</li> <li>Risk Management: Analyzing interest rate risk and duration</li> <li>Monetary Policy Analysis: Understanding the impact of central bank actions</li> <li>Forecasting: Predicting future interest rate movements</li> <li>Derivatives Pricing: Valuing interest rate derivatives</li> <li>Economic Research: Studying the relationship between short and long-term rates</li> </ol>"},{"location":"user-guide/asset/term_structure/#limitations","title":"Limitations","text":"<ol> <li>Parameter Stability: The fitted parameters can be sensitive to the initial values</li> <li>Extrapolation Risk: Caution should be used when extrapolating beyond the range of observed maturities</li> <li>Model Selection: The Nelson-Siegel model may not fit complex yield curves as well as the Svensson model</li> <li>Data Quality: The models are sensitive to outliers and noisy data</li> <li>Optimization Challenges: The optimization process may converge to local minima</li> </ol>"},{"location":"user-guide/asset/term_structure/#extensions","title":"Extensions","text":"<ol> <li>Dynamic Nelson-Siegel: Time-varying parameters to model yield curve evolution</li> <li>Nelson-Siegel-Svensson with Constraints: Adding constraints to ensure economically meaningful parameters</li> <li>Arbitrage-Free Nelson-Siegel: Ensuring the model is consistent with no-arbitrage conditions</li> <li>Bayesian Estimation: Incorporating prior beliefs about parameter values</li> <li>Regime-Switching Models: Allowing parameters to change based on economic regimes </li> </ol>"},{"location":"user-guide/asset/yield_curve/","title":"Yield Curve Construction","text":"<p>The yield curve module provides functions for constructing and interpolating yield curves from market data. A yield curve represents the relationship between interest rates and maturities, which is essential for bond pricing, risk management, and economic analysis.</p>"},{"location":"user-guide/asset/yield_curve/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.asset import construct_yield_curve, interpolate_rate\n\n# Sample yield curve data\nmaturities = [0.1, 0.25, 0.5, 1, 2, 3, 5, 7, 10, 20, 30]  # in years\nrates = [0.008, 0.01, 0.015, 0.02, 0.025, 0.028, 0.03, 0.031, 0.032, 0.033, 0.034]  # as decimals\n\n# Construct a yield curve with cubic spline interpolation\nyield_curve = construct_yield_curve(\n    maturities=maturities,\n    rates=rates,\n    interpolation_method='cubic',\n    extrapolate=False\n)\n\n# Interpolate rates at specific maturities\nrate_4y = interpolate_rate(yield_curve, 4)\nrate_15y = interpolate_rate(yield_curve, 15)\n\nprint(f\"4-year rate: {rate_4y:.2%}\")\nprint(f\"15-year rate: {rate_15y:.2%}\")\n\n# Access forward rates\nforward_rates = yield_curve['forward_rates']\nfor fr in forward_rates:\n    print(f\"Forward rate from {fr['start_maturity']}Y to {fr['end_maturity']}Y: {fr['forward_rate']:.2%}\")\n\n# Access curve characteristics\nprint(f\"Yield curve steepness: {yield_curve['steepness']:.2%}\")\nprint(f\"Average rate: {yield_curve['average_rate']:.2%}\")\n\n# Check if maturity is within range before interpolating\nmin_maturity = yield_curve['min_maturity']\nmax_maturity = yield_curve['max_maturity']\n\nmaturity = 0.1  # The maturity you want to interpolate\nif min_maturity &lt;= maturity &lt;= max_maturity:\n    rate = interpolate_rate(yield_curve, maturity)\n    print(f\"{maturity}-year rate: {rate:.2%}\")\nelse:\n    print(f\"Cannot interpolate at maturity {maturity} (outside range [{min_maturity}, {max_maturity}])\")\n</code></pre>"},{"location":"user-guide/asset/yield_curve/#functions","title":"Functions","text":""},{"location":"user-guide/asset/yield_curve/#construct_yield_curve","title":"construct_yield_curve","text":""},{"location":"user-guide/asset/yield_curve/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>maturities</code> list of float Maturities in years for the observed rates Required <code>rates</code> list of float Observed interest rates (as decimals) Required <code>interpolation_method</code> str Method for interpolation ('linear', 'cubic', 'monotonic') 'cubic' <code>extrapolate</code> bool Whether to allow extrapolation beyond observed maturities False"},{"location":"user-guide/asset/yield_curve/#return-value","title":"Return Value","text":"<p>The <code>construct_yield_curve</code> function returns a dictionary with the following keys:</p> Key Type Description <code>maturities</code> list of float Input maturities <code>rates</code> list of float Input rates <code>interpolation_method</code> str Method used for interpolation <code>extrapolate</code> bool Whether extrapolation is allowed <code>interpolate_func</code> callable Function that takes a maturity and returns the interpolated rate <code>min_maturity</code> float Minimum maturity in the input data <code>max_maturity</code> float Maximum maturity in the input data <code>steepness</code> float Steepness of the yield curve (long-term rate minus short-term rate) <code>average_rate</code> float Average of the input rates <code>forward_rates</code> list of dict Implied forward rates between consecutive maturities <p>Each element in the <code>forward_rates</code> list is a dictionary with the following keys: - <code>start_maturity</code>: Starting maturity for the forward rate - <code>end_maturity</code>: Ending maturity for the forward rate - <code>forward_rate</code>: Implied forward rate for the period</p>"},{"location":"user-guide/asset/yield_curve/#interpolate_rate","title":"interpolate_rate","text":""},{"location":"user-guide/asset/yield_curve/#parameters_1","title":"Parameters","text":"Parameter Type Description Default <code>yield_curve</code> dict Yield curve object from <code>construct_yield_curve</code> Required <code>maturity</code> float Maturity in years for which to interpolate the rate Required"},{"location":"user-guide/asset/yield_curve/#return-value_1","title":"Return Value","text":"<p>The <code>interpolate_rate</code> function returns a float representing the interpolated interest rate at the specified maturity.</p>"},{"location":"user-guide/asset/yield_curve/#interpolation-methods","title":"Interpolation Methods","text":"<p>The <code>construct_yield_curve</code> function supports three interpolation methods:</p> Method Description Best Used For <code>linear</code> Simple linear interpolation between points Quick approximations, when simplicity is preferred <code>cubic</code> Cubic spline interpolation with smooth derivatives Most yield curves, provides smooth transitions <code>monotonic</code> Monotonic cubic interpolation (PCHIP) When preserving monotonicity is important"},{"location":"user-guide/asset/yield_curve/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to construct yield curves with different interpolation methods and visualize the results:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom pypulate.asset import construct_yield_curve, interpolate_rate\n\n# Sample yield curve data (US Treasury yields as of a sample date)\nmaturities = [0.1, 0.25, 0.5, 1, 2, 3, 5, 7, 10, 20, 30]\nrates = [0.008, 0.01, 0.015, 0.02, 0.025, 0.028, 0.03, 0.031, 0.032, 0.033, 0.034]\n\n# Construct yield curves with different interpolation methods\nlinear_curve = construct_yield_curve(maturities, rates, interpolation_method='linear')\ncubic_curve = construct_yield_curve(maturities, rates, interpolation_method='cubic')\nmonotonic_curve = construct_yield_curve(maturities, rates, interpolation_method='monotonic')\n\n# Create a dense set of maturities for smooth curve plotting\n# Ensure we don't go below the minimum observed maturity\nmin_maturity = min(maturities)\nmax_maturity = max(maturities)\ndense_maturities = np.linspace(min_maturity, max_maturity, 100)\n\n# Get interpolated rates for each method\nlinear_rates = [interpolate_rate(linear_curve, t) for t in dense_maturities]\ncubic_rates = [interpolate_rate(cubic_curve, t) for t in dense_maturities]\nmonotonic_rates = [interpolate_rate(monotonic_curve, t) for t in dense_maturities]\n\n# Plot the results\nplt.figure(figsize=(12, 8))\n\n# Plot the actual data points\nplt.scatter(maturities, rates, color='black', label='Observed Rates', zorder=3)\n\n# Plot the interpolated curves\nplt.plot(dense_maturities, linear_rates, 'r-', label='Linear Interpolation', linewidth=2)\nplt.plot(dense_maturities, cubic_rates, 'b-', label='Cubic Spline Interpolation', linewidth=2)\nplt.plot(dense_maturities, monotonic_rates, 'g--', label='Monotonic Interpolation', linewidth=2)\n\n# Add labels and title\nplt.xlabel('Maturity (years)')\nplt.ylabel('Interest Rate')\nplt.title('Yield Curve Construction with Different Interpolation Methods')\nplt.grid(True, alpha=0.3)\nplt.legend()\n\n# Format y-axis as percentage\nplt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.1%}'))\n\nplt.tight_layout()\nplt.show()\n\n# Calculate and plot forward rates\nplt.figure(figsize=(12, 8))\n\n# Extract forward rates\nforward_maturities = [(fr['start_maturity'] + fr['end_maturity']) / 2 for fr in cubic_curve['forward_rates']]\nforward_rates_values = [fr['forward_rate'] for fr in cubic_curve['forward_rates']]\n\n# Plot spot rates and forward rates\nplt.plot(dense_maturities, cubic_rates, 'b-', label='Spot Rates (Cubic)', linewidth=2)\nplt.step(forward_maturities, forward_rates_values, 'r-', label='Forward Rates', linewidth=2, where='mid')\nplt.scatter(maturities, rates, color='black', label='Observed Spot Rates', zorder=3)\n\nplt.xlabel('Maturity (years)')\nplt.ylabel('Interest Rate')\nplt.title('Spot Rates vs. Forward Rates')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.1%}'))\n\nplt.tight_layout()\nplt.show()\n\n# Demonstrate extrapolation\n# Create a yield curve that allows extrapolation\nextrapolating_curve = construct_yield_curve(maturities, rates, interpolation_method='cubic', extrapolate=True)\nnon_extrapolating_curve = construct_yield_curve(maturities, rates, interpolation_method='cubic', extrapolate=False)\n\n# Create extended maturities for extrapolation demonstration\n# Extend beyond the observed range (both shorter and longer maturities)\nextended_maturities = np.linspace(0.05, 50, 100)\n\n# Get interpolated/extrapolated rates with proper error handling\nextrapolated_rates = []\nfor t in extended_maturities:\n    try:\n        # Try to get the rate, which will work for the extrapolating curve\n        rate = interpolate_rate(extrapolating_curve, t)\n        extrapolated_rates.append(rate)\n    except ValueError:\n        # If extrapolation is not allowed and we're outside the range, use NaN\n        extrapolated_rates.append(np.nan)\n\n# Plot extrapolation\nplt.figure(figsize=(12, 8))\n\nplt.scatter(maturities, rates, color='black', label='Observed Rates', zorder=3)\nplt.plot(extended_maturities, extrapolated_rates, 'b-', label='Cubic Spline with Extrapolation', linewidth=2)\n\n# Add vertical lines at the minimum and maximum observed maturities\nplt.axvline(x=min_maturity, color='r', linestyle='--', label=f'Min Observed Maturity ({min_maturity}Y)')\nplt.axvline(x=max_maturity, color='r', linestyle='--', label=f'Max Observed Maturity ({max_maturity}Y)')\n\nplt.xlabel('Maturity (years)')\nplt.ylabel('Interest Rate')\nplt.title('Yield Curve Extrapolation')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.1%}'))\n\nplt.tight_layout()\nplt.show()\n\n# Calculate discount factors from the yield curve\ndef calculate_discount_factors(yield_curve, maturities):\n    discount_factors = []\n    for t in maturities:\n        # Only calculate for maturities within the valid range\n        if yield_curve['min_maturity'] &lt;= t &lt;= yield_curve['max_maturity']:\n            rate = interpolate_rate(yield_curve, t)\n            discount_factor = 1 / (1 + rate) ** t\n            discount_factors.append((t, discount_factor))\n    return discount_factors\n\n# Calculate discount factors for the cubic yield curve\ndiscount_factors = calculate_discount_factors(cubic_curve, dense_maturities)\ndiscount_maturities = [df[0] for df in discount_factors]\ndiscount_values = [df[1] for df in discount_factors]\n\n# Plot discount factors\nplt.figure(figsize=(12, 8))\n\nplt.plot(discount_maturities, discount_values, 'g-', linewidth=2)\nplt.xlabel('Maturity (years)')\nplt.ylabel('Discount Factor')\nplt.title('Discount Factors Derived from the Yield Curve')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Example of safely handling extrapolation\ndef safe_interpolate(yield_curve, maturity):\n    \"\"\"Safely interpolate a rate, handling out-of-range maturities.\"\"\"\n    min_maturity = yield_curve['min_maturity']\n    max_maturity = yield_curve['max_maturity']\n\n    if min_maturity &lt;= maturity &lt;= max_maturity:\n        return interpolate_rate(yield_curve, maturity)\n    elif yield_curve['extrapolate']:\n        # Extrapolation is allowed, but we'll print a warning\n        rate = interpolate_rate(yield_curve, maturity)\n        print(f\"Warning: Extrapolating at maturity {maturity}Y (outside [{min_maturity}Y, {max_maturity}Y])\")\n        return rate\n    else:\n        # Extrapolation not allowed, return None or raise an exception\n        print(f\"Cannot interpolate at maturity {maturity}Y (outside [{min_maturity}Y, {max_maturity}Y])\")\n        return None\n\n# Example usage of safe interpolation\ntest_maturities = [0.05, 0.1, 1, 5, 30, 40]\nfor m in test_maturities:\n    rate = safe_interpolate(cubic_curve, m)\n    if rate is not None:\n        print(f\"{m}-year rate: {rate:.2%}\")\n</code></pre>"},{"location":"user-guide/asset/yield_curve/#example-output","title":"Example Output","text":"<pre><code>4-year rate: 2.93%\n15-year rate: 3.25%\n\nForward rate from 0.25Y to 0.5Y: 2.01%\nForward rate from 0.5Y to 1Y: 2.51%\nForward rate from 1Y to 2Y: 3.01%\nForward rate from 2Y to 3Y: 3.41%\nForward rate from 3Y to 5Y: 3.44%\nForward rate from 5Y to 7Y: 3.21%\nForward rate from 7Y to 10Y: 3.37%\nForward rate from 10Y to 20Y: 3.40%\nForward rate from 20Y to 30Y: 3.50%\n\nYield curve steepness: 2.40%\nAverage rate: 2.63%\n</code></pre>"},{"location":"user-guide/asset/yield_curve/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/asset/yield_curve/#yield-curve-construction_1","title":"Yield Curve Construction","text":"<p>This chart shows the observed yield curve data points and the interpolated curves using different methods (linear, cubic spline, and monotonic).</p> <p></p>"},{"location":"user-guide/asset/yield_curve/#spot-rates-vs-forward-rates","title":"Spot Rates vs. Forward Rates","text":"<p>This chart compares the spot rates (yield curve) with the implied forward rates between consecutive maturities.</p> <p></p>"},{"location":"user-guide/asset/yield_curve/#yield-curve-extrapolation","title":"Yield Curve Extrapolation","text":"<p>This chart demonstrates how the yield curve can be extrapolated beyond the range of observed maturities when the <code>extrapolate</code> parameter is set to <code>True</code>.</p> <p></p>"},{"location":"user-guide/asset/yield_curve/#discount-factors","title":"Discount Factors","text":"<p>This chart shows the discount factors derived from the yield curve, which are used for discounting future cash flows in bond pricing and other applications.</p>"},{"location":"user-guide/asset/yield_curve/#practical-applications","title":"Practical Applications","text":"<p>Yield curve construction and interpolation are used for:</p> <ol> <li>Bond Pricing: Discounting cash flows at the appropriate spot rates</li> <li>Fixed Income Analysis: Analyzing the term structure of interest rates</li> <li>Risk Management: Calculating duration, convexity, and other risk measures</li> <li>Derivatives Pricing: Valuing interest rate derivatives</li> <li>Economic Analysis: Interpreting the shape of the yield curve as an economic indicator</li> <li>Monetary Policy: Understanding the impact of central bank actions</li> <li>Portfolio Management: Constructing fixed income portfolios with specific duration targets</li> </ol>"},{"location":"user-guide/asset/yield_curve/#limitations","title":"Limitations","text":"<ol> <li>Interpolation Artifacts: Different interpolation methods can produce different results, especially between observed data points</li> <li>Extrapolation Risk: Extrapolating beyond the range of observed maturities can lead to unrealistic rates</li> <li>Data Quality: The quality of the constructed yield curve depends on the quality and coverage of the input data</li> <li>Cubic Spline Oscillations: Cubic spline interpolation can sometimes produce oscillations that don't reflect market expectations</li> <li>Forward Rate Calculation: The calculation of forward rates assumes a specific compounding convention</li> </ol>"},{"location":"user-guide/asset/yield_curve/#extensions","title":"Extensions","text":"<ol> <li>Zero-Coupon Yield Curve: Converting coupon bond yields to zero-coupon yields</li> <li>Bootstrapping: Building a yield curve from coupon-bearing instruments</li> <li>Parametric Models: Using models like Nelson-Siegel or Svensson for smoother yield curves</li> <li>Real Yield Curves: Constructing inflation-adjusted yield curves</li> <li>Multi-Currency Yield Curves: Building and comparing yield curves across different currencies </li> </ol>"},{"location":"user-guide/credit/altman_zscore/","title":"Altman Z-Score","text":"<p>The Altman Z-Score is a financial metric used to predict the probability of a company going bankrupt within the next two years. Developed by Edward I. Altman in 1968, it combines five financial ratios with weighted coefficients to produce a single score that indicates financial health.</p>"},{"location":"user-guide/credit/altman_zscore/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.credit import altman_z_score\n\n\n# Calculate Altman Z-Score\nresult = altman_z_score(\n    working_capital=120000000,        # Working capital\n    retained_earnings=200000000,      # Retained earnings\n    ebit=80000000,                    # Earnings before interest and taxes\n    market_value_equity=500000000,    # Market value of equity\n    sales=350000000,                  # Sales\n    total_assets=400000000,           # Total assets\n    total_liabilities=150000000       # Total liabilities\n)\n\n# Access the results\nz_score = result['z_score']\nrisk_assessment = result['risk_assessment']\nzone = result['zone']\ncomponents = result['components']\n</code></pre>"},{"location":"user-guide/credit/altman_zscore/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>working_capital</code> float Working capital (current assets - current liabilities) Required <code>retained_earnings</code> float Retained earnings Required <code>ebit</code> float Earnings before interest and taxes Required <code>market_value_equity</code> float Market value of equity Required <code>sales</code> float Annual sales Required <code>total_assets</code> float Total assets Required <code>total_liabilities</code> float Total liabilities Required"},{"location":"user-guide/credit/altman_zscore/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>z_score</code> float The calculated Z-Score value <code>risk_assessment</code> str Text description of the bankruptcy risk <code>zone</code> str Classification zone (\"Safe\", \"Grey\", or \"Distress\") <code>interpretation</code> str Same as risk_assessment (for compatibility) <code>components</code> dict Dictionary containing the individual ratio components <p>The <code>components</code> dictionary includes:</p> <ul> <li><code>x1</code>: Working capital / Total assets</li> <li><code>x2</code>: Retained earnings / Total assets</li> <li><code>x3</code>: EBIT / Total assets</li> <li><code>x4</code>: Market value of equity / Total liabilities</li> <li><code>x5</code>: Sales / Total assets</li> </ul>"},{"location":"user-guide/credit/altman_zscore/#risk-level-classification","title":"Risk Level Classification","text":"<p>The Z-Score is categorized into risk zones:</p> Z-Score Range Risk Zone &gt; 2.99 Safe Zone 1.81 - 2.99 Grey Zone &lt; 1.81 Distress Zone"},{"location":"user-guide/credit/altman_zscore/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example analyzing three companies with different financial profiles:</p> <pre><code>from pypulate.credit import altman_z_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Example 1: Financially healthy manufacturing company\nhealthy_company = altman_z_score(\n    working_capital=120000000,        # $120M working capital\n    retained_earnings=200000000,      # $200M retained earnings\n    ebit=80000000,                    # $80M earnings before interest and taxes\n    market_value_equity=500000000,    # $500M market value of equity\n    sales=350000000,                  # $350M annual sales\n    total_assets=400000000,           # $400M total assets\n    total_liabilities=150000000       # $150M total liabilities\n)\n\n# Example 2: Company in the \"grey zone\"\ngrey_zone_company = altman_z_score(\n    working_capital=30000000,         # $30M working capital\n    retained_earnings=40000000,       # $40M retained earnings\n    ebit=25000000,                    # $25M earnings before interest and taxes\n    market_value_equity=120000000,    # $120M market value of equity\n    sales=200000000,                  # $200M annual sales\n    total_assets=250000000,           # $250M total assets\n    total_liabilities=150000000       # $150M total liabilities\n)\n\n# Example 3: Financially distressed company\ndistressed_company = altman_z_score(\n    working_capital=5000000,          # $5M working capital\n    retained_earnings=-20000000,      # -$20M retained earnings (accumulated losses)\n    ebit=-8000000,                    # -$8M earnings before interest and taxes (operating loss)\n    market_value_equity=30000000,     # $30M market value of equity\n    sales=100000000,                  # $100M annual sales\n    total_assets=150000000,           # $150M total assets\n    total_liabilities=120000000       # $120M total liabilities\n)\n\n# Print the results\nprint(\"Altman Z-Score Analysis\")\nprint(\"======================\")\n\nprint(\"\\nExample 1: Financially Healthy Company\")\nprint(f\"Z-Score: {healthy_company['z_score']:.2f}\")\nprint(f\"Risk Assessment: {healthy_company['risk_assessment']}\")\nprint(f\"Zone: {healthy_company['zone']}\")\nprint(\"Component Values:\")\nfor component, value in healthy_company['components'].items():\n    print(f\"  {component}: {value:.4f}\")\n\nprint(\"\\nExample 2: Grey Zone Company\")\nprint(f\"Z-Score: {grey_zone_company['z_score']:.2f}\")\nprint(f\"Risk Assessment: {grey_zone_company['risk_assessment']}\")\nprint(f\"Zone: {grey_zone_company['zone']}\")\nprint(\"Component Values:\")\nfor component, value in grey_zone_company['components'].items():\n    print(f\"  {component}: {value:.4f}\")\n\nprint(\"\\nExample 3: Financially Distressed Company\")\nprint(f\"Z-Score: {distressed_company['z_score']:.2f}\")\nprint(f\"Risk Assessment: {distressed_company['risk_assessment']}\")\nprint(f\"Zone: {distressed_company['zone']}\")\nprint(\"Component Values:\")\nfor component, value in distressed_company['components'].items():\n    print(f\"  {component}: {value:.4f}\")\n\n# Visualize the results\ncompanies = ['Healthy', 'Grey Zone', 'Distressed']\nz_scores = [\n    healthy_company['z_score'],\n    grey_zone_company['z_score'],\n    distressed_company['z_score']\n]\n\n# Create a bar chart\nplt.figure(figsize=(10, 6))\nbars = plt.bar(companies, z_scores, color=['green', 'orange', 'red'])\n\n# Add horizontal lines for the Z-score thresholds\nplt.axhline(y=1.81, color='r', linestyle='--', label='Distress Zone (Z &lt; 1.81)')\nplt.axhline(y=2.99, color='g', linestyle='--', label='Safe Zone (Z &gt; 2.99)')\nplt.axhspan(1.81, 2.99, alpha=0.2, color='orange', label='Grey Zone (1.81 &lt; Z &lt; 2.99)')\n\n# Add labels and title\nplt.ylabel('Altman Z-Score')\nplt.title('Altman Z-Score Comparison')\nplt.ylim(bottom=0)  # Start y-axis at 0\n\n# Add the Z-score values on top of the bars\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n             f'{height:.2f}', ha='center', va='bottom')\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/credit/altman_zscore/#example-output","title":"Example Output","text":"<pre><code>Altman Z-Score Analysis\n======================\nExample 1: Financially Healthy Company\nZ-Score: 4.59\nRisk Assessment: Low risk of bankruptcy\nZone: Safe\nComponent Values:\n  x1: 0.3000\n  x2: 0.5000\n  x3: 0.2000\n  x4: 3.3333\n  x5: 0.8750\nExample 2: Grey Zone Company\nZ-Score: 1.98\nRisk Assessment: Grey area, moderate risk\nZone: Grey\nComponent Values:\n  x1: 0.1200\n  x2: 0.1600\n  x3: 0.1000\n  x4: 0.8000\n  x5: 0.8000\nExample 3: Financially Distressed Company\nZ-Score: 0.49\nRisk Assessment: High risk of bankruptcy\nZone: Distress\nComponent Values:\n  x1: 0.0333\n  x2: -0.1333\n  x3: -0.0533\n  x4: 0.2500\n  x5: 0.6667\n</code></pre>"},{"location":"user-guide/credit/altman_zscore/#visualization","title":"Visualization","text":"<p>The visualization shows the Z-scores for three example companies, with horizontal lines indicating the threshold values that separate the Safe, Grey, and Distress zones.</p>"},{"location":"user-guide/credit/altman_zscore/#component-analysis","title":"Component Analysis","text":"<p>Each component of the Z-Score provides insight into different aspects of a company's financial health:</p> <ol> <li> <p>X\u2081 (Working Capital / Total Assets)</p> <ul> <li>Measures liquidity relative to company size</li> <li>Higher values indicate better short-term financial health</li> <li>Weight: 1.2</li> </ul> </li> <li> <p>X\u2082 (Retained Earnings / Total Assets)</p> <ul> <li>Measures cumulative profitability and company age</li> <li>Higher values indicate stronger historical performance</li> <li>Weight: 1.4</li> </ul> </li> <li> <p>X\u2083 (EBIT / Total Assets)</p> <ul> <li>Measures operating efficiency independent of tax and leverage</li> <li>Has the highest weight (3.3), indicating its importance</li> <li>Higher values indicate better operational performance</li> </ul> </li> <li> <p>X\u2084 (Market Value of Equity / Total Liabilities)</p> <ul> <li>Measures financial leverage and market confidence</li> <li>Higher values indicate lower financial risk</li> <li>Weight: 0.6</li> </ul> </li> <li> <p>X\u2085 (Sales / Total Assets)</p> <ul> <li>Measures asset turnover and management efficiency</li> <li>Higher values indicate better utilization of assets</li> <li>Weight: 0.999</li> </ul> </li> </ol>"},{"location":"user-guide/credit/altman_zscore/#practical-applications","title":"Practical Applications","text":"<p>The Altman Z-Score can be used for:</p> <ol> <li>Credit Risk Assessment: Evaluating potential borrowers' bankruptcy risk</li> <li>Investment Screening: Identifying financially stable companies</li> <li>Portfolio Risk Management: Monitoring existing investments</li> <li>Supplier Evaluation: Assessing the financial stability of key suppliers</li> <li>Merger &amp; Acquisition Analysis: Evaluating target companies</li> </ol>"},{"location":"user-guide/credit/altman_zscore/#limitations","title":"Limitations","text":"<p>While the Altman Z-Score is a powerful tool, it has some limitations:</p> <ol> <li>Originally developed for manufacturing companies</li> <li>May need industry-specific adjustments</li> <li>Works best for public companies with market value data</li> <li>Should be used alongside other financial metrics</li> <li>Historical performance may not predict future outcomes</li> </ol>"},{"location":"user-guide/credit/debt_service_coverage_ratio/","title":"Debt Service Coverage Ratio (DSCR)","text":"<p>The <code>debt_service_coverage_ratio</code> function calculates the Debt Service Coverage Ratio (DSCR), a key financial metric used to assess a borrower's ability to repay debt. This ratio is widely used in commercial real estate lending, corporate finance, and credit risk assessment.</p>"},{"location":"user-guide/credit/debt_service_coverage_ratio/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.credit import debt_service_coverage_ratio\n\n# Calculate DSCR\nresult = debt_service_coverage_ratio(\n    net_operating_income=500000,  # $500,000 net operating income\n    total_debt_service=300000     # $300,000 total debt service\n)\n\n# Access the results\ndscr = result[\"dscr\"]\nassessment = result[\"assessment\"]\nrating = result[\"rating\"]\n</code></pre>"},{"location":"user-guide/credit/debt_service_coverage_ratio/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>net_operating_income</code> float Net operating income of the borrower Required <code>total_debt_service</code> float Total debt service obligations (principal, interest, lease payments, etc.) Required"},{"location":"user-guide/credit/debt_service_coverage_ratio/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>dscr</code> float The calculated Debt Service Coverage Ratio <code>assessment</code> str Text description of the risk level <code>rating</code> str Categorical rating (\"Poor\", \"Fair\", \"Good\", or \"Excellent\")"},{"location":"user-guide/credit/debt_service_coverage_ratio/#risk-level-classification","title":"Risk Level Classification","text":"<p>The DSCR is categorized into risk levels:</p> DSCR Range Risk Level &lt; 1.0 Poor 1.0 - 1.25 Fair 1.25 - 1.5 Good &gt; 1.5 Excellent"},{"location":"user-guide/credit/debt_service_coverage_ratio/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to calculate and analyze DSCR for different borrowers:</p> <pre><code>from pypulate.credit import debt_service_coverage_ratio\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Example 1: Strong borrower with high income relative to debt\nstrong_borrower = debt_service_coverage_ratio(\n    net_operating_income=800000,  # $800,000 net operating income\n    total_debt_service=400000     # $400,000 total debt service\n)\n\n# Example 2: Average borrower with moderate coverage\naverage_borrower = debt_service_coverage_ratio(\n    net_operating_income=450000,  # $450,000 net operating income\n    total_debt_service=350000     # $350,000 total debt service\n)\n\n# Example 3: Weak borrower with insufficient coverage\nweak_borrower = debt_service_coverage_ratio(\n    net_operating_income=280000,  # $280,000 net operating income\n    total_debt_service=300000     # $300,000 total debt service\n)\n\n# Print the results\nprint(\"Debt Service Coverage Ratio Analysis\")\nprint(\"===================================\")\n\nprint(\"\\nExample 1: Strong Borrower\")\nprint(f\"DSCR: {strong_borrower['dscr']:.2f}\")\nprint(f\"Assessment: {strong_borrower['assessment']}\")\nprint(f\"Rating: {strong_borrower['rating']}\")\n\nprint(\"\\nExample 2: Average Borrower\")\nprint(f\"DSCR: {average_borrower['dscr']:.2f}\")\nprint(f\"Assessment: {average_borrower['assessment']}\")\nprint(f\"Rating: {average_borrower['rating']}\")\n\nprint(\"\\nExample 3: Weak Borrower\")\nprint(f\"DSCR: {weak_borrower['dscr']:.2f}\")\nprint(f\"Assessment: {weak_borrower['assessment']}\")\nprint(f\"Rating: {weak_borrower['rating']}\")\n\n# Visualize the results\nborrowers = ['Weak', 'Average', 'Strong']\ndscr_values = [\n    weak_borrower['dscr'],\n    average_borrower['dscr'],\n    strong_borrower['dscr']\n]\n\n# Create a bar chart\nplt.figure(figsize=(10, 6))\nbars = plt.bar(borrowers, dscr_values, color=['red', 'orange', 'green'])\n\n# Add horizontal lines for the DSCR thresholds\nplt.axhline(y=1.0, color='r', linestyle='--', label='Poor/Fair Threshold (1.0)')\nplt.axhline(y=1.25, color='orange', linestyle='--', label='Fair/Good Threshold (1.25)')\nplt.axhline(y=1.5, color='g', linestyle='--', label='Good/Excellent Threshold (1.5)')\n\n# Add labels and title\nplt.ylabel('Debt Service Coverage Ratio')\nplt.title('DSCR Comparison')\nplt.ylim(bottom=0)  # Start y-axis at 0\n\n# Add the DSCR values on top of the bars\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n             f'{height:.2f}', ha='center', va='bottom')\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Create a sensitivity analysis\nincome_values = np.linspace(200000, 1000000, 100)  # Range of income values\ndebt_service = 400000  # Fixed debt service\n\ndscr_values = [income / debt_service for income in income_values]\nratings = []\n\nfor dscr in dscr_values:\n    if dscr &lt; 1.0:\n        ratings.append(\"Poor\")\n    elif dscr &lt; 1.25:\n        ratings.append(\"Fair\")\n    elif dscr &lt; 1.5:\n        ratings.append(\"Good\")\n    else:\n        ratings.append(\"Excellent\")\n\n# Create a plot showing how DSCR changes with income\nplt.figure(figsize=(12, 6))\n\n# Plot DSCR curve\nplt.plot(income_values, dscr_values, 'b-', linewidth=2)\n\n# Add colored regions for different ratings\npoor_indices = [i for i, r in enumerate(ratings) if r == \"Poor\"]\nfair_indices = [i for i, r in enumerate(ratings) if r == \"Fair\"]\ngood_indices = [i for i, r in enumerate(ratings) if r == \"Good\"]\nexcellent_indices = [i for i, r in enumerate(ratings) if r == \"Excellent\"]\n\nif poor_indices:\n    plt.fill_between(income_values[min(poor_indices):max(poor_indices)+1], \n                     0, dscr_values[min(poor_indices):max(poor_indices)+1], \n                     color='red', alpha=0.3, label='Poor')\nif fair_indices:\n    plt.fill_between(income_values[min(fair_indices):max(fair_indices)+1], \n                     0, dscr_values[min(fair_indices):max(fair_indices)+1], \n                     color='orange', alpha=0.3, label='Fair')\nif good_indices:\n    plt.fill_between(income_values[min(good_indices):max(good_indices)+1], \n                     0, dscr_values[min(good_indices):max(good_indices)+1], \n                     color='yellow', alpha=0.3, label='Good')\nif excellent_indices:\n    plt.fill_between(income_values[min(excellent_indices):max(excellent_indices)+1], \n                     0, dscr_values[min(excellent_indices):max(excellent_indices)+1], \n                     color='green', alpha=0.3, label='Excellent')\n\n# Add horizontal lines for the DSCR thresholds\nplt.axhline(y=1.0, color='r', linestyle='--')\nplt.axhline(y=1.25, color='orange', linestyle='--')\nplt.axhline(y=1.5, color='g', linestyle='--')\n\n# Add labels and title\nplt.xlabel('Net Operating Income ($)')\nplt.ylabel('Debt Service Coverage Ratio')\nplt.title('DSCR Sensitivity to Income (Fixed Debt Service: $400,000)')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/credit/debt_service_coverage_ratio/#example-output","title":"Example Output","text":"<pre><code>Debt Service Coverage Ratio Analysis\n===================================\n\nExample 1: Strong Borrower\nDSCR: 2.00\nAssessment: Strong coverage, low risk\nRating: Excellent\n\nExample 2: Average Borrower\nDSCR: 1.29\nAssessment: Sufficient coverage, acceptable risk\nRating: Good\n\nExample 3: Weak Borrower\nDSCR: 0.93\nAssessment: Negative cash flow, high risk\nRating: Poor\n</code></pre>"},{"location":"user-guide/credit/debt_service_coverage_ratio/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/credit/debt_service_coverage_ratio/#dscr-comparison","title":"DSCR Comparison","text":"<p>This visualization shows the DSCR values for three example borrowers, with horizontal lines indicating the threshold values that separate the rating categories.</p>"},{"location":"user-guide/credit/debt_service_coverage_ratio/#dscr-sensitivity-analysis","title":"DSCR Sensitivity Analysis","text":"<p>This visualization demonstrates how the DSCR changes with varying levels of net operating income while keeping debt service constant, highlighting the income thresholds for different rating categories.</p> <p></p>"},{"location":"user-guide/credit/debt_service_coverage_ratio/#practical-applications","title":"Practical Applications","text":"<p>The DSCR can be used for:</p> <ol> <li>Commercial Real Estate Lending: Evaluating property cash flow relative to debt obligations</li> <li>Corporate Credit Analysis: Assessing a company's ability to service its debt</li> <li>Project Finance: Determining the financial viability of infrastructure projects</li> <li>Small Business Lending: Evaluating loan applications from small businesses</li> <li>Risk-Based Pricing: Setting interest rates based on the borrower's repayment capacity</li> </ol>"},{"location":"user-guide/credit/debt_service_coverage_ratio/#industry-standards","title":"Industry Standards","text":"<p>Different lenders and industries may use slightly different DSCR thresholds:</p> <ol> <li> <p>Commercial Real Estate:</p> <ul> <li>Typically requires DSCR \u2265 1.25</li> <li>Premium properties may require DSCR \u2265 1.5</li> <li>Riskier properties may accept DSCR \u2265 1.15</li> </ul> </li> <li> <p>Corporate Lending:</p> <ul> <li>Investment grade: DSCR \u2265 1.5</li> <li>Non-investment grade: DSCR \u2265 1.2</li> <li>Distressed: DSCR &lt; 1.0</li> </ul> </li> <li> <p>Small Business Administration (SBA):</p> <ul> <li>Generally requires DSCR \u2265 1.15</li> <li>May consider global DSCR including owner's personal income</li> </ul> </li> </ol>"},{"location":"user-guide/credit/debt_service_coverage_ratio/#best-practices","title":"Best Practices","text":"<ol> <li>Historical Analysis: Calculate DSCR using historical data to establish trends</li> <li>Stress Testing: Test DSCR under adverse scenarios (e.g., reduced income, increased interest rates)</li> <li>Industry Comparison: Compare DSCR to industry benchmarks</li> <li>Global DSCR: Consider all sources of income and all debt obligations</li> <li>Forward-Looking: Project future DSCR based on expected changes in income and debt</li> </ol>"},{"location":"user-guide/credit/expected_credit_loss/","title":"Expected Credit Loss (ECL)","text":"<p>The <code>expected_credit_loss</code> function calculates the Expected Credit Loss (ECL), a critical metric in credit risk management that estimates the probability-weighted loss on a financial asset. This metric is widely used in banking, lending, and financial risk management, especially since the introduction of IFRS 9 and similar accounting standards.</p>","boost":2},{"location":"user-guide/credit/expected_credit_loss/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.credit import expected_credit_loss\n\n# Calculate ECL\nresult = expected_credit_loss(\n    pd=0.05,           # 5% probability of default\n    lgd=0.4,           # 40% loss given default\n    ead=1000000,       # $1,000,000 exposure at default\n    time_horizon=1.0,  # 1 year time horizon\n    discount_rate=0.03 # 3% discount rate\n)\n\n# Access the results\necl = result[\"expected_credit_loss\"]\nlifetime_ecl = result[\"lifetime_ecl\"]\nrisk_level = result[\"risk_level\"]\n</code></pre>","boost":2},{"location":"user-guide/credit/expected_credit_loss/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>pd</code> float Probability of default (between 0 and 1) Required <code>lgd</code> float Loss given default (between 0 and 1) Required <code>ead</code> float Exposure at default Required <code>time_horizon</code> float Time horizon in years 1.0 <code>discount_rate</code> float Discount rate for future losses 0.0","boost":2},{"location":"user-guide/credit/expected_credit_loss/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>expected_credit_loss</code> float The calculated ECL value <code>lifetime_ecl</code> float The lifetime expected credit loss <code>expected_loss_rate</code> float The product of PD and LGD <code>risk_level</code> str Risk level categorization (\"Very Low\", \"Low\", \"Moderate\", \"High\", or \"Very High\") <code>components</code> dict Dictionary containing calculation components <p>The <code>components</code> dictionary includes:</p> <ul> <li><code>pd</code>: Probability of default</li> <li><code>lgd</code>: Loss given default</li> <li><code>ead</code>: Exposure at default</li> <li><code>time_horizon</code>: Time horizon in years</li> <li><code>discount_rate</code>: Discount rate for future losses</li> <li><code>discount_factor</code>: Calculated discount factor based on time horizon and discount rate</li> </ul>","boost":2},{"location":"user-guide/credit/expected_credit_loss/#risk-level-classification","title":"Risk Level Classification","text":"<p>The Expected Loss Rate (PD \u00d7 LGD) is categorized into risk levels:</p> Expected Loss Rate Range Risk Level &lt; 1% Very Low 1% - 3% Low 3% - 7% Moderate 7% - 15% High &gt; 15% Very High","boost":2},{"location":"user-guide/credit/expected_credit_loss/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to calculate and analyze ECL for different loan portfolios:</p> <pre><code>from pypulate.credit import expected_credit_loss\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Example 1: Low-risk corporate loan\nlow_risk_loan = expected_credit_loss(\n    pd=0.01,           # 1% probability of default\n    lgd=0.3,           # 30% loss given default\n    ead=2000000,       # $2,000,000 exposure\n    time_horizon=1.0,  # 1 year\n    discount_rate=0.02 # 2% discount rate\n)\n\n# Example 2: Medium-risk SME loan\nmedium_risk_loan = expected_credit_loss(\n    pd=0.05,           # 5% probability of default\n    lgd=0.45,          # 45% loss given default\n    ead=500000,        # $500,000 exposure\n    time_horizon=1.0,  # 1 year\n    discount_rate=0.02 # 2% discount rate\n)\n\n# Example 3: High-risk unsecured consumer loan\nhigh_risk_loan = expected_credit_loss(\n    pd=0.15,           # 15% probability of default\n    lgd=0.65,          # 65% loss given default\n    ead=50000,         # $50,000 exposure\n    time_horizon=1.0,  # 1 year\n    discount_rate=0.02 # 2% discount rate\n)\n\n# Print the results\nprint(\"Expected Credit Loss Analysis\")\nprint(\"============================\")\n\nprint(\"\\nExample 1: Low-risk Corporate Loan\")\nprint(f\"ECL: ${low_risk_loan['expected_credit_loss']:.2f}\")\nprint(f\"Expected Loss Rate: {low_risk_loan['expected_loss_rate']:.2%}\")\nprint(f\"Risk Level: {low_risk_loan['risk_level']}\")\n\nprint(\"\\nExample 2: Medium-risk SME Loan\")\nprint(f\"ECL: ${medium_risk_loan['expected_credit_loss']:.2f}\")\nprint(f\"Expected Loss Rate: {medium_risk_loan['expected_loss_rate']:.2%}\")\nprint(f\"Risk Level: {medium_risk_loan['risk_level']}\")\n\nprint(\"\\nExample 3: High-risk Consumer Loan\")\nprint(f\"ECL: ${high_risk_loan['expected_credit_loss']:.2f}\")\nprint(f\"Expected Loss Rate: {high_risk_loan['expected_loss_rate']:.2%}\")\nprint(f\"Risk Level: {high_risk_loan['risk_level']}\")\n\n# Create a DataFrame for visualization\nloan_types = ['Corporate Loan', 'SME Loan', 'Consumer Loan']\necl_values = [\n    low_risk_loan['expected_credit_loss'],\n    medium_risk_loan['expected_credit_loss'],\n    high_risk_loan['expected_credit_loss']\n]\nloss_rates = [\n    low_risk_loan['expected_loss_rate'],\n    medium_risk_loan['expected_loss_rate'],\n    high_risk_loan['expected_loss_rate']\n]\nexposures = [2000000, 500000, 50000]\nrisk_levels = [\n    low_risk_loan['risk_level'],\n    medium_risk_loan['risk_level'],\n    high_risk_loan['risk_level']\n]\n\n# Create a bar chart for ECL comparison\nplt.figure(figsize=(12, 6))\nbars = plt.bar(loan_types, ecl_values, color=['green', 'orange', 'red'])\n\n# Add labels and title\nplt.ylabel('Expected Credit Loss ($)')\nplt.title('Expected Credit Loss Comparison')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Add the ECL values on top of the bars\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height + 0.05 * max(ecl_values),\n             f'${height:.2f}', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n# Create a scatter plot showing the relationship between exposure and ECL\nplt.figure(figsize=(12, 6))\n\n# Create a scatter plot with size proportional to exposure\nsizes = [exposure/5000 for exposure in exposures]  # Scale for better visualization\ncolors = ['green', 'orange', 'red']\n\nfor i, loan_type in enumerate(loan_types):\n    plt.scatter(loss_rates[i], ecl_values[i], s=sizes[i], \n                color=colors[i], alpha=0.7, label=f\"{loan_type} (EAD: ${exposures[i]:,})\")\n\n# Add labels and title\nplt.xlabel('Expected Loss Rate (PD \u00d7 LGD)')\nplt.ylabel('Expected Credit Loss ($)')\nplt.title('ECL vs. Expected Loss Rate (Size represents Exposure at Default)')\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Add risk level regions\nplt.axvspan(0, 0.01, alpha=0.2, color='green', label='Very Low Risk')\nplt.axvspan(0.01, 0.03, alpha=0.2, color='lightgreen', label='Low Risk')\nplt.axvspan(0.03, 0.07, alpha=0.2, color='yellow', label='Moderate Risk')\nplt.axvspan(0.07, 0.15, alpha=0.2, color='orange', label='High Risk')\nplt.axvspan(0.15, 1, alpha=0.2, color='red', label='Very High Risk')\n\nplt.legend(loc='right', bbox_to_anchor=(1.25, 0.5), fontsize=9)\nplt.tight_layout()\nplt.show()\n\n# Create a sensitivity analysis for PD and LGD\npd_values = np.linspace(0.01, 0.2, 20)\nlgd_values = np.linspace(0.1, 0.9, 20)\nPD, LGD = np.meshgrid(pd_values, lgd_values)\nELR = PD * LGD  # Expected Loss Rate\n\n# Create risk level categories\nrisk_levels = np.zeros_like(ELR, dtype=str)\nrisk_levels = np.where(ELR &lt; 0.01, 'Very Low', risk_levels)\nrisk_levels = np.where((ELR &gt;= 0.01) &amp; (ELR &lt; 0.03), 'Low', risk_levels)\nrisk_levels = np.where((ELR &gt;= 0.03) &amp; (ELR &lt; 0.07), 'Moderate', risk_levels)\nrisk_levels = np.where((ELR &gt;= 0.07) &amp; (ELR &lt; 0.15), 'High', risk_levels)\nrisk_levels = np.where(ELR &gt;= 0.15, 'Very High', risk_levels)\n\n# Create a heatmap\nplt.figure(figsize=(12, 8))\ncontour = plt.contourf(PD, LGD, ELR, levels=20, cmap='RdYlGn_r')\nplt.colorbar(contour, label='Expected Loss Rate (PD \u00d7 LGD)')\n\n# Add contour lines for risk level boundaries\nplt.contour(PD, LGD, ELR, levels=[0.01, 0.03, 0.07, 0.15], colors='black', linestyles='dashed')\n\n# Add labels for risk regions\nplt.text(0.005, 0.5, 'Very Low Risk', rotation=90, va='center', ha='center', color='black', fontweight='bold')\nplt.text(0.02, 0.5, 'Low Risk', rotation=90, va='center', ha='center', color='black', fontweight='bold')\nplt.text(0.05, 0.5, 'Moderate Risk', rotation=90, va='center', ha='center', color='black', fontweight='bold')\nplt.text(0.11, 0.5, 'High Risk', rotation=90, va='center', ha='center', color='black', fontweight='bold')\nplt.text(0.175, 0.5, 'Very High Risk', rotation=90, va='center', ha='center', color='black', fontweight='bold')\n\n# Add labels and title\nplt.xlabel('Probability of Default (PD)')\nplt.ylabel('Loss Given Default (LGD)')\nplt.title('Risk Level Sensitivity to PD and LGD')\nplt.grid(True, linestyle='--', alpha=0.3)\nplt.tight_layout()\nplt.show()\n</code></pre>","boost":2},{"location":"user-guide/credit/expected_credit_loss/#example-output","title":"Example Output","text":"<pre><code>Expected Credit Loss Analysis\n============================\n\nExample 1: Low-risk Corporate Loan\nECL: $5882.35\nExpected Loss Rate: 0.30%\nRisk Level: Very Low\n\nExample 2: Medium-risk SME Loan\nECL: $11029.41\nExpected Loss Rate: 2.25%\nRisk Level: Low\n\nExample 3: High-risk Consumer Loan\nECL: $4779.41\nExpected Loss Rate: 9.75%\nRisk Level: High\n</code></pre>","boost":2},{"location":"user-guide/credit/expected_credit_loss/#visualizations","title":"Visualizations","text":"","boost":2},{"location":"user-guide/credit/expected_credit_loss/#ecl-comparison","title":"ECL Comparison","text":"<p>This visualization compares the Expected Credit Loss for three different loan types, showing how the combination of risk factors and exposure amounts affects the total expected loss.</p>","boost":2},{"location":"user-guide/credit/expected_credit_loss/#ecl-vs-expected-loss-rate","title":"ECL vs. Expected Loss Rate","text":"<p>This scatter plot shows the relationship between the Expected Loss Rate (PD \u00d7 LGD) and the resulting ECL, with the size of each point representing the Exposure at Default. The background is color-coded to indicate different risk level regions.</p> <p></p>","boost":2},{"location":"user-guide/credit/expected_credit_loss/#risk-level-sensitivity","title":"Risk Level Sensitivity","text":"<p>This heatmap demonstrates how the risk level changes with different combinations of PD and LGD, helping to visualize the sensitivity of the Expected Loss Rate to these two key parameters.</p> <p></p>","boost":2},{"location":"user-guide/credit/expected_credit_loss/#practical-applications","title":"Practical Applications","text":"<p>The Expected Credit Loss can be used for:</p> <ol> <li>IFRS 9 / CECL Compliance: Meeting accounting standards for loan loss provisioning</li> <li>Credit Risk Management: Quantifying and managing credit risk in loan portfolios</li> <li>Loan Pricing: Incorporating expected losses into loan pricing models</li> <li>Capital Allocation: Determining economic capital requirements for credit risk</li> <li>Portfolio Management: Optimizing the risk-return profile of loan portfolios</li> <li>Stress Testing: Assessing the impact of adverse economic scenarios on credit losses</li> <li>Risk-Based Limits: Setting exposure limits based on expected loss considerations</li> </ol>","boost":2},{"location":"user-guide/credit/expected_credit_loss/#industry-standards","title":"Industry Standards","text":"<p>Different financial institutions and regulatory frameworks may use slightly different approaches:</p> <ol> <li> <p>Banking (Basel Framework):</p> <ul> <li>Uses PD, LGD, and EAD for regulatory capital calculations</li> <li>Typically requires through-the-cycle PD estimates</li> <li>Downturn LGD estimates for capital adequacy</li> </ul> </li> <li> <p>Accounting Standards:</p> <ul> <li>IFRS 9: Forward-looking, point-in-time estimates with multiple economic scenarios</li> <li>CECL: Lifetime expected losses from origination</li> <li>Both require consideration of past events, current conditions, and reasonable forecasts</li> </ul> </li> <li> <p>Credit Rating Agencies:</p> <ul> <li>Provide expected loss estimates based on historical default and recovery data</li> <li>Often use through-the-cycle methodologies for stability</li> </ul> </li> </ol>","boost":2},{"location":"user-guide/credit/expected_credit_loss/#best-practices","title":"Best Practices","text":"<ol> <li>Data Quality: Ensure high-quality historical data for PD and LGD estimation</li> <li>Forward-Looking Adjustments: Incorporate macroeconomic forecasts into PD and LGD estimates</li> <li>Segmentation: Group exposures with similar risk characteristics</li> <li>Multiple Scenarios: Consider various economic scenarios and their probabilities</li> <li>Regular Validation: Backtest and validate ECL models regularly</li> <li>Expert Judgment: Complement quantitative models with expert judgment</li> <li>Documentation: Maintain comprehensive documentation of methodologies and assumptions</li> </ol>","boost":2},{"location":"user-guide/credit/exposure_at_default/","title":"Exposure at Default (EAD)","text":"<p>The <code>exposure_at_default</code> function calculates the Exposure at Default (EAD), a key metric in credit risk management that estimates the expected amount outstanding when a borrower defaults. This metric is essential for credit risk modeling, regulatory capital calculations, and expected credit loss estimation under frameworks like Basel and IFRS 9.</p>"},{"location":"user-guide/credit/exposure_at_default/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.credit import exposure_at_default\n\n# Calculate EAD\nresult = exposure_at_default(\n    current_balance=500000,    # $500,000 drawn amount\n    undrawn_amount=500000,     # $500,000 undrawn commitment\n    credit_conversion_factor=0.5  # 50% CCF\n)\n\n# Access the results\nead = result[\"ead\"]\nregulatory_ead = result[\"regulatory_ead\"]\nstressed_ead = result[\"stressed_ead\"]\nrisk_level = result[\"risk_level\"]\n</code></pre>"},{"location":"user-guide/credit/exposure_at_default/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>current_balance</code> float Current drawn amount of the credit facility Required <code>undrawn_amount</code> float Undrawn commitment available to the borrower Required <code>credit_conversion_factor</code> float Factor to convert undrawn amounts to exposure (between 0 and 1) 0.5 (50%)"},{"location":"user-guide/credit/exposure_at_default/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>ead</code> float Exposure at Default using the provided CCF <code>regulatory_ead</code> float EAD calculated using regulatory CCF based on utilization rate <code>stressed_ead</code> float EAD calculated using a stressed CCF (1.5x the provided CCF, capped at 1.0) <code>ead_percentage</code> float EAD as a percentage of total facility <code>risk_level</code> str Risk level categorization (\"Low\", \"Moderate\", or \"High\") <code>components</code> dict Dictionary containing calculation components <p>The <code>components</code> dictionary includes:</p> <ul> <li><code>current_balance</code>: The provided current balance</li> <li><code>undrawn_amount</code>: The provided undrawn amount</li> <li><code>total_facility</code>: Sum of current balance and undrawn amount</li> <li><code>utilization_rate</code>: Current balance divided by total facility</li> <li><code>credit_conversion_factor</code>: The provided CCF</li> <li><code>regulatory_ccf</code>: CCF based on regulatory guidelines</li> <li><code>stress_ccf</code>: Stressed CCF for scenario analysis</li> </ul>"},{"location":"user-guide/credit/exposure_at_default/#risk-level-classification","title":"Risk Level Classification","text":"<p>The Utilization Rate is categorized into risk levels:</p> Utilization Rate Range Risk Level &lt; 30% Low 30% - 70% Moderate &gt; 70% High"},{"location":"user-guide/credit/exposure_at_default/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to calculate and analyze EAD for different credit facilities:</p> <pre><code>from pypulate.credit import exposure_at_default\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Example 1: Low utilization corporate credit line\nlow_util_facility = exposure_at_default(\n    current_balance=200000,    # $200,000 drawn\n    undrawn_amount=800000,     # $800,000 undrawn\n    credit_conversion_factor=0.5  # 50% CCF\n)\n\n# Example 2: Medium utilization SME credit line\nmedium_util_facility = exposure_at_default(\n    current_balance=500000,    # $500,000 drawn\n    undrawn_amount=500000,     # $500,000 undrawn\n    credit_conversion_factor=0.5  # 50% CCF\n)\n\n# Example 3: High utilization retail credit card\nhigh_util_facility = exposure_at_default(\n    current_balance=45000,     # $45,000 drawn\n    undrawn_amount=5000,       # $5,000 undrawn\n    credit_conversion_factor=0.5  # 50% CCF\n)\n\n# Print the results\nprint(\"Exposure at Default Analysis\")\nprint(\"============================\")\n\nprint(\"\\nExample 1: Low Utilization Corporate Credit Line\")\nprint(f\"EAD: ${low_util_facility['ead']:.2f}\")\nprint(f\"Regulatory EAD: ${low_util_facility['regulatory_ead']:.2f}\")\nprint(f\"Stressed EAD: ${low_util_facility['stressed_ead']:.2f}\")\nprint(f\"Utilization Rate: {low_util_facility['components']['utilization_rate']:.2%}\")\nprint(f\"Risk Level: {low_util_facility['risk_level']}\")\n\nprint(\"\\nExample 2: Medium Utilization SME Credit Line\")\nprint(f\"EAD: ${medium_util_facility['ead']:.2f}\")\nprint(f\"Regulatory EAD: ${medium_util_facility['regulatory_ead']:.2f}\")\nprint(f\"Stressed EAD: ${medium_util_facility['stressed_ead']:.2f}\")\nprint(f\"Utilization Rate: {medium_util_facility['components']['utilization_rate']:.2%}\")\nprint(f\"Risk Level: {medium_util_facility['risk_level']}\")\n\nprint(\"\\nExample 3: High Utilization Retail Credit Card\")\nprint(f\"EAD: ${high_util_facility['ead']:.2f}\")\nprint(f\"Regulatory EAD: ${high_util_facility['regulatory_ead']:.2f}\")\nprint(f\"Stressed EAD: ${high_util_facility['stressed_ead']:.2f}\")\nprint(f\"Utilization Rate: {high_util_facility['components']['utilization_rate']:.2%}\")\nprint(f\"Risk Level: {high_util_facility['risk_level']}\")\n\n# Create a DataFrame for visualization\nfacility_types = ['Corporate Credit Line', 'SME Credit Line', 'Retail Credit Card']\nead_values = [\n    low_util_facility['ead'],\n    medium_util_facility['ead'],\n    high_util_facility['ead']\n]\nregulatory_ead_values = [\n    low_util_facility['regulatory_ead'],\n    medium_util_facility['regulatory_ead'],\n    high_util_facility['regulatory_ead']\n]\nstressed_ead_values = [\n    low_util_facility['stressed_ead'],\n    medium_util_facility['stressed_ead'],\n    high_util_facility['stressed_ead']\n]\nutilization_rates = [\n    low_util_facility['components']['utilization_rate'],\n    medium_util_facility['components']['utilization_rate'],\n    high_util_facility['components']['utilization_rate']\n]\ntotal_facilities = [\n    low_util_facility['components']['total_facility'],\n    medium_util_facility['components']['total_facility'],\n    high_util_facility['components']['total_facility']\n]\n\n# Create a bar chart for EAD comparison\nplt.figure(figsize=(12, 6))\nx = np.arange(len(facility_types))\nwidth = 0.25\n\nplt.bar(x - width, ead_values, width, label='Standard EAD', color='blue')\nplt.bar(x, regulatory_ead_values, width, label='Regulatory EAD', color='green')\nplt.bar(x + width, stressed_ead_values, width, label='Stressed EAD', color='red')\n\nplt.xlabel('Facility Type')\nplt.ylabel('Exposure at Default ($)')\nplt.title('EAD Comparison Across Different Facility Types')\nplt.xticks(x, facility_types)\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Add the utilization rate on top of each group\nfor i, rate in enumerate(utilization_rates):\n    plt.text(i, max(ead_values[i], regulatory_ead_values[i], stressed_ead_values[i]) + 20000, \n             f'Utilization: {rate:.1%}', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n# Create a scatter plot showing the relationship between utilization rate and EAD percentage\nplt.figure(figsize=(12, 6))\n\n# Create a scatter plot with size proportional to total facility\nsizes = [facility/10000 for facility in total_facilities]  # Scale for better visualization\ncolors = ['green', 'orange', 'red']\n\nfor i, facility_type in enumerate(facility_types):\n    plt.scatter(utilization_rates[i], \n                ead_values[i]/total_facilities[i], \n                s=sizes[i], \n                color=colors[i], \n                alpha=0.7, \n                label=f\"{facility_type} (Total: ${total_facilities[i]:,})\")\n\n# Add labels and title\nplt.xlabel('Utilization Rate')\nplt.ylabel('EAD as % of Total Facility')\nplt.title('EAD Percentage vs. Utilization Rate (Size represents Total Facility)')\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Add risk level regions\nplt.axvspan(0, 0.3, alpha=0.2, color='green', label='Low Risk')\nplt.axvspan(0.3, 0.7, alpha=0.2, color='yellow', label='Moderate Risk')\nplt.axvspan(0.7, 1, alpha=0.2, color='red', label='High Risk')\n\nplt.legend(loc='upper left')\nplt.tight_layout()\nplt.show()\n\n# Create a sensitivity analysis for utilization rate and CCF\nutilization_values = np.linspace(0.1, 0.9, 9)\nccf_values = np.linspace(0.1, 1.0, 10)\n\n# Create a matrix to store EAD percentages\nead_percentages = np.zeros((len(utilization_values), len(ccf_values)))\n\n# Calculate EAD percentage for each combination\nfor i, util in enumerate(utilization_values):\n    for j, ccf in enumerate(ccf_values):\n        # For a total facility of 1.0, calculate EAD percentage\n        current_balance = util * 1.0\n        undrawn_amount = (1.0 - util)\n        ead = current_balance + (undrawn_amount * ccf)\n        ead_percentages[i, j] = ead\n\n# Create a heatmap\nplt.figure(figsize=(12, 8))\ncontour = plt.contourf(ccf_values, utilization_values, ead_percentages, levels=20, cmap='viridis')\nplt.colorbar(contour, label='EAD as % of Total Facility')\n\n# Add contour lines\nplt.contour(ccf_values, utilization_values, ead_percentages, levels=10, colors='white', linestyles='dashed', linewidths=0.5)\n\n# Add labels and title\nplt.xlabel('Credit Conversion Factor (CCF)')\nplt.ylabel('Utilization Rate')\nplt.title('EAD Sensitivity to Utilization Rate and CCF')\nplt.grid(True, linestyle='--', alpha=0.3)\n\n# Add risk level indicators\nplt.axhspan(0, 0.3, alpha=0.1, color='green', label='Low Risk')\nplt.axhspan(0.3, 0.7, alpha=0.1, color='yellow', label='Moderate Risk')\nplt.axhspan(0.7, 1, alpha=0.1, color='red', label='High Risk')\n\nplt.legend(loc='upper right')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/credit/exposure_at_default/#example-output","title":"Example Output","text":"<pre><code>Exposure at Default Analysis\n============================\n\nExample 1: Low Utilization Corporate Credit Line\nEAD: $600000.00\nRegulatory EAD: $360000.00\nStressed EAD: $800000.00\nUtilization Rate: 20.00%\nRisk Level: Low\n\nExample 2: Medium Utilization SME Credit Line\nEAD: $750000.00\nRegulatory EAD: $800000.00\nStressed EAD: $875000.00\nUtilization Rate: 50.00%\nRisk Level: Moderate\n\nExample 3: High Utilization Retail Credit Card\nEAD: $47500.00\nRegulatory EAD: $49000.00\nStressed EAD: $48750.00\nUtilization Rate: 90.00%\nRisk Level: High\n</code></pre>"},{"location":"user-guide/credit/exposure_at_default/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/credit/exposure_at_default/#ead-comparison","title":"EAD Comparison","text":"<p>This visualization compares the standard EAD, regulatory EAD, and stressed EAD for three different facility types, showing how the utilization rate affects the exposure calculations.</p> <p></p>"},{"location":"user-guide/credit/exposure_at_default/#ead-percentage-vs-utilization-rate","title":"EAD Percentage vs. Utilization Rate","text":"<p>This scatter plot shows the relationship between the utilization rate and the EAD as a percentage of the total facility, with the size of each point representing the total facility amount. The background is color-coded to indicate different risk level regions.</p> <p></p>"},{"location":"user-guide/credit/exposure_at_default/#ead-sensitivity","title":"EAD Sensitivity","text":"<p>This heatmap demonstrates how the EAD percentage changes with different combinations of utilization rate and credit conversion factor, helping to visualize the sensitivity of the exposure calculation to these two key parameters.</p> <p></p>"},{"location":"user-guide/credit/exposure_at_default/#practical-applications","title":"Practical Applications","text":"<p>Exposure at Default calculations can be used for:</p> <ol> <li>Regulatory Capital: Calculating regulatory capital requirements under Basel frameworks</li> <li>IFRS 9 / CECL: Determining exposure inputs for expected credit loss calculations</li> <li>Credit Risk Management: Quantifying potential exposure in credit facilities</li> <li>Limit Setting: Establishing appropriate credit limits for different facility types</li> <li>Stress Testing: Assessing the impact of increased drawdowns during stress scenarios</li> <li>Portfolio Management: Understanding the risk profile of credit portfolios</li> <li>Pricing: Incorporating potential exposure into risk-based pricing models</li> </ol>"},{"location":"user-guide/credit/exposure_at_default/#industry-standards","title":"Industry Standards","text":"<p>Different regulatory frameworks provide guidance on EAD calculation:</p> <ol> <li> <p>Basel Framework:</p> <ul> <li>Standardized Approach: Prescribes fixed CCFs based on facility type</li> <li>Internal Ratings-Based Approach: Allows banks to estimate their own CCFs</li> <li>Typically differentiates between committed and uncommitted facilities</li> </ul> </li> <li> <p>Accounting Standards:</p> <ul> <li>IFRS 9: Requires consideration of expected drawdowns over the lifetime of the facility</li> <li>CECL: Similar approach, focusing on lifetime exposure estimates</li> </ul> </li> <li> <p>Industry Practice:</p> <ul> <li>CCFs typically range from 0% (for uncommitted facilities) to 100% (for fully committed facilities)</li> <li>Higher CCFs are applied to facilities with longer tenors and fewer covenants</li> <li>Retail products often have product-specific CCFs based on historical behavior</li> </ul> </li> </ol>"},{"location":"user-guide/credit/exposure_at_default/#best-practices","title":"Best Practices","text":"<ol> <li>Historical Analysis: Base CCF estimates on historical drawdown behavior</li> <li>Segmentation: Group facilities with similar characteristics for CCF estimation</li> <li>Stress Scenarios: Consider increased drawdowns during economic downturns</li> <li>Facility Characteristics: Account for commitment type, covenants, and maturity</li> <li>Regular Monitoring: Track utilization rates and update CCF estimates periodically</li> <li>Conservative Approach: Apply higher CCFs for facilities with uncertain drawdown patterns</li> <li>Documentation: Maintain comprehensive documentation of methodologies and assumptions</li> </ol>"},{"location":"user-guide/credit/financial_ratios/","title":"Financial Ratios","text":"<p>The <code>financial_ratios</code> function calculates key financial ratios used in credit assessment and provides an overall assessment of a company's financial health. These ratios are grouped into categories including liquidity, solvency, profitability, coverage, and efficiency.</p>"},{"location":"user-guide/credit/financial_ratios/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.credit import financial_ratios\n\n# Calculate financial ratios\nresult = financial_ratios(\n    current_assets=250000,         # $250,000 current assets\n    current_liabilities=100000,    # $100,000 current liabilities\n    total_assets=1000000,          # $1,000,000 total assets\n    total_liabilities=400000,      # $400,000 total liabilities\n    ebit=150000,                   # $150,000 earnings before interest and taxes\n    interest_expense=30000,        # $30,000 interest expense\n    net_income=100000,             # $100,000 net income\n    total_equity=600000,           # $600,000 total equity\n    sales=800000                   # $800,000 sales\n)\n\n# Access the results\nliquidity = result[\"liquidity\"]\nsolvency = result[\"solvency\"]\nprofitability = result[\"profitability\"]\ncoverage = result[\"coverage\"]\nefficiency = result[\"efficiency\"]\noverall = result[\"overall_assessment\"]\n</code></pre>"},{"location":"user-guide/credit/financial_ratios/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>current_assets</code> float The company's current assets Required <code>current_liabilities</code> float The company's current liabilities Required <code>total_assets</code> float The company's total assets Required <code>total_liabilities</code> float The company's total liabilities Required <code>ebit</code> float Earnings before interest and taxes Required <code>interest_expense</code> float Interest expense Required <code>net_income</code> float Net income Required <code>total_equity</code> float Total equity Required <code>sales</code> float Total sales Required"},{"location":"user-guide/credit/financial_ratios/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>liquidity</code> dict Dictionary containing liquidity ratios and assessment <code>solvency</code> dict Dictionary containing solvency ratios and assessment <code>profitability</code> dict Dictionary containing profitability ratios and assessment <code>coverage</code> dict Dictionary containing coverage ratios and assessment <code>efficiency</code> dict Dictionary containing efficiency ratios <code>overall_assessment</code> str Overall assessment of financial health <p>The <code>liquidity</code> dictionary includes: - <code>current_ratio</code>: Current assets divided by current liabilities - <code>assessment</code>: Assessment of liquidity (\"Strong\", \"Adequate\", or \"Weak\")</p> <p>The <code>solvency</code> dictionary includes: - <code>debt_ratio</code>: Total liabilities divided by total assets - <code>debt_to_equity</code>: Total liabilities divided by total equity - <code>assessment</code>: Assessment of solvency (\"Strong\", \"Adequate\", or \"Weak\")</p> <p>The <code>profitability</code> dictionary includes: - <code>return_on_assets</code>: Net income divided by total assets - <code>return_on_equity</code>: Net income divided by total equity - <code>assessment</code>: Assessment of profitability (\"Strong\", \"Adequate\", or \"Weak\")</p> <p>The <code>coverage</code> dictionary includes: - <code>interest_coverage</code>: EBIT divided by interest expense - <code>assessment</code>: Assessment of coverage (\"Strong\", \"Adequate\", or \"Weak\")</p> <p>The <code>efficiency</code> dictionary includes: - <code>asset_turnover</code>: Sales divided by total assets - <code>assessment</code>: Assessment of efficiency (\"Strong\", \"Adequate\", or \"Weak\")</p>"},{"location":"user-guide/credit/financial_ratios/#risk-level-classification","title":"Risk Level Classification","text":"<p>The financial ratios are categorized into assessment levels:</p> Ratio Range Assessment Current Ratio &lt; 1.0 Weak 1.0 - 2.0 Adequate &gt; 2.0 Strong Debt Ratio &gt; 0.6 Weak 0.4 - 0.6 Adequate &lt; 0.4 Strong Debt-to-Equity &gt; 1.5 Weak 0.5 - 1.5 Adequate &lt; 0.5 Strong Return on Assets &lt; 0.02 Weak 0.02 - 0.05 Adequate &gt; 0.05 Strong Return on Equity &lt; 0.05 Weak 0.05 - 0.15 Adequate &gt; 0.15 Strong Interest Coverage &lt; 1.5 Weak 1.5 - 3.0 Adequate &gt; 3.0 Strong Asset Turnover &lt; 0.5 Weak 0.5 - 1.0 Adequate &gt; 1.0 Strong"},{"location":"user-guide/credit/financial_ratios/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to calculate and analyze financial ratios for different companies:</p> <pre><code>from pypulate.credit import financial_ratios\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Example 1: Financially strong company\nstrong_company = financial_ratios(\n    current_assets=500000,         # $500,000 current assets\n    current_liabilities=200000,    # $200,000 current liabilities\n    total_assets=2000000,          # $2,000,000 total assets\n    total_liabilities=600000,      # $600,000 total liabilities\n    ebit=400000,                   # $400,000 earnings before interest and taxes\n    interest_expense=50000,        # $50,000 interest expense\n    net_income=300000,             # $300,000 net income\n    total_equity=1400000,          # $1,400,000 total equity\n    sales=2500000                  # $2,500,000 sales\n)\n\n# Example 2: Company with adequate financial health\nadequate_company = financial_ratios(\n    current_assets=300000,         # $300,000 current assets\n    current_liabilities=200000,    # $200,000 current liabilities\n    total_assets=1500000,          # $1,500,000 total assets\n    total_liabilities=750000,      # $750,000 total liabilities\n    ebit=200000,                   # $200,000 earnings before interest and taxes\n    interest_expense=80000,        # $80,000 interest expense\n    net_income=100000,             # $100,000 net income\n    total_equity=750000,           # $750,000 total equity\n    sales=1200000                  # $1,200,000 sales\n)\n\n# Example 3: Financially weak company\nweak_company = financial_ratios(\n    current_assets=150000,         # $150,000 current assets\n    current_liabilities=200000,    # $200,000 current liabilities\n    total_assets=1000000,          # $1,000,000 total assets\n    total_liabilities=700000,      # $700,000 total liabilities\n    ebit=50000,                    # $50,000 earnings before interest and taxes\n    interest_expense=60000,        # $60,000 interest expense\n    net_income=20000,              # $20,000 net income\n    total_equity=300000,           # $300,000 total equity\n    sales=600000                   # $600,000 sales\n)\n\n# Print the results\nprint(\"Financial Ratios Analysis\")\nprint(\"========================\")\n\nprint(\"\\nExample 1: Strong Company\")\nprint(f\"Current Ratio: {strong_company['liquidity']['current_ratio']:.2f} ({strong_company['liquidity']['assessment']})\")\nprint(f\"Debt Ratio: {strong_company['solvency']['debt_ratio']:.2f} ({strong_company['solvency']['assessment']})\")\nprint(f\"Return on Equity: {strong_company['profitability']['return_on_equity']:.2f} ({strong_company['profitability']['assessment']})\")\nprint(f\"Interest Coverage: {strong_company['coverage']['interest_coverage']:.2f} ({strong_company['coverage']['assessment']})\")\nprint(f\"Asset Turnover: {strong_company['efficiency']['asset_turnover']:.2f}\")\nprint(f\"Overall Assessment: {strong_company['overall_assessment']}\")\n\nprint(\"\\nExample 2: Adequate Company\")\nprint(f\"Current Ratio: {adequate_company['liquidity']['current_ratio']:.2f} ({adequate_company['liquidity']['assessment']})\")\nprint(f\"Debt Ratio: {adequate_company['solvency']['debt_ratio']:.2f} ({adequate_company['solvency']['assessment']})\")\nprint(f\"Return on Equity: {adequate_company['profitability']['return_on_equity']:.2f} ({adequate_company['profitability']['assessment']})\")\nprint(f\"Interest Coverage: {adequate_company['coverage']['interest_coverage']:.2f} ({adequate_company['coverage']['assessment']})\")\nprint(f\"Asset Turnover: {adequate_company['efficiency']['asset_turnover']:.2f}\")\nprint(f\"Overall Assessment: {adequate_company['overall_assessment']}\")\n\nprint(\"\\nExample 3: Weak Company\")\nprint(f\"Current Ratio: {weak_company['liquidity']['current_ratio']:.2f} ({weak_company['liquidity']['assessment']})\")\nprint(f\"Debt Ratio: {weak_company['solvency']['debt_ratio']:.2f} ({weak_company['solvency']['assessment']})\")\nprint(f\"Return on Equity: {weak_company['profitability']['return_on_equity']:.2f} ({weak_company['profitability']['assessment']})\")\nprint(f\"Interest Coverage: {weak_company['coverage']['interest_coverage']:.2f} ({weak_company['coverage']['assessment']})\")\nprint(f\"Asset Turnover: {weak_company['efficiency']['asset_turnover']:.2f}\")\nprint(f\"Overall Assessment: {weak_company['overall_assessment']}\")\n\n# Visualize the results\ncompanies = ['Strong', 'Adequate', 'Weak']\ncurrent_ratios = [\n    strong_company['liquidity']['current_ratio'],\n    adequate_company['liquidity']['current_ratio'],\n    weak_company['liquidity']['current_ratio']\n]\ndebt_ratios = [\n    strong_company['solvency']['debt_ratio'],\n    adequate_company['solvency']['debt_ratio'],\n    weak_company['solvency']['debt_ratio']\n]\ninterest_coverages = [\n    strong_company['coverage']['interest_coverage'],\n    adequate_company['coverage']['interest_coverage'],\n    weak_company['coverage']['interest_coverage']\n]\nreturn_on_equities = [\n    strong_company['profitability']['return_on_equity'],\n    adequate_company['profitability']['return_on_equity'],\n    weak_company['profitability']['return_on_equity']\n]\n\n# Create a comparison chart\nfig, axs = plt.subplots(2, 2, figsize=(14, 10))\nfig.suptitle('Financial Ratios Comparison', fontsize=16)\n\n# Current Ratio\naxs[0, 0].bar(companies, current_ratios, color=['green', 'orange', 'red'])\naxs[0, 0].axhline(y=2, color='g', linestyle='--', label='Strong (\u2265 2)')\naxs[0, 0].axhline(y=1, color='r', linestyle='--', label='Weak (&lt; 1)')\naxs[0, 0].set_title('Current Ratio')\naxs[0, 0].set_ylabel('Ratio')\naxs[0, 0].legend()\n\n# Debt Ratio\naxs[0, 1].bar(companies, debt_ratios, color=['green', 'orange', 'red'])\naxs[0, 1].axhline(y=0.4, color='g', linestyle='--', label='Strong (\u2264 0.4)')\naxs[0, 1].axhline(y=0.6, color='r', linestyle='--', label='Weak (&gt; 0.6)')\naxs[0, 1].set_title('Debt Ratio')\naxs[0, 1].set_ylabel('Ratio')\naxs[0, 1].legend()\n\n# Interest Coverage\naxs[1, 0].bar(companies, interest_coverages, color=['green', 'orange', 'red'])\naxs[1, 0].axhline(y=3, color='g', linestyle='--', label='Strong (\u2265 3)')\naxs[1, 0].axhline(y=1.5, color='r', linestyle='--', label='Weak (&lt; 1.5)')\naxs[1, 0].set_title('Interest Coverage Ratio')\naxs[1, 0].set_ylabel('Ratio')\naxs[1, 0].legend()\n\n# Return on Equity\naxs[1, 1].bar(companies, return_on_equities, color=['green', 'orange', 'red'])\naxs[1, 1].axhline(y=0.15, color='g', linestyle='--', label='Strong (\u2265 0.15)')\naxs[1, 1].axhline(y=0.08, color='r', linestyle='--', label='Weak (&lt; 0.08)')\naxs[1, 1].set_title('Return on Equity')\naxs[1, 1].set_ylabel('Ratio')\naxs[1, 1].legend()\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.9)\nplt.show()\n\n# Create a sensitivity analysis for current ratio\ncurrent_assets_values = np.linspace(100000, 500000, 100)  # Range of current assets values\ncurrent_liabilities = 200000  # Fixed current liabilities\n\ncurrent_ratios = [ca / current_liabilities for ca in current_assets_values]\nassessments = []\n\nfor ratio in current_ratios:\n    if ratio &gt;= 2:\n        assessments.append(\"Strong\")\n    elif ratio &gt;= 1:\n        assessments.append(\"Adequate\")\n    else:\n        assessments.append(\"Weak\")\n\n# Create a plot showing how current ratio changes with current assets\nplt.figure(figsize=(12, 6))\n\n# Plot current ratio curve\nplt.plot(current_assets_values, current_ratios, 'b-', linewidth=2)\n\n# Add colored regions for different assessments\nweak_indices = [i for i, r in enumerate(assessments) if r == \"Weak\"]\nadequate_indices = [i for i, r in enumerate(assessments) if r == \"Adequate\"]\nstrong_indices = [i for i, r in enumerate(assessments) if r == \"Strong\"]\n\nif weak_indices:\n    plt.fill_between(current_assets_values[min(weak_indices):max(weak_indices)+1], \n                     0, current_ratios[min(weak_indices):max(weak_indices)+1], \n                     color='red', alpha=0.3, label='Weak')\nif adequate_indices:\n    plt.fill_between(current_assets_values[min(adequate_indices):max(adequate_indices)+1], \n                     0, current_ratios[min(adequate_indices):max(adequate_indices)+1], \n                     color='orange', alpha=0.3, label='Adequate')\nif strong_indices:\n    plt.fill_between(current_assets_values[min(strong_indices):max(strong_indices)+1], \n                     0, current_ratios[min(strong_indices):max(strong_indices)+1], \n                     color='green', alpha=0.3, label='Strong')\n\n# Add horizontal lines for the ratio thresholds\nplt.axhline(y=1, color='r', linestyle='--')\nplt.axhline(y=2, color='g', linestyle='--')\n\n# Add labels and title\nplt.xlabel('Current Assets ($)')\nplt.ylabel('Current Ratio')\nplt.title('Current Ratio Sensitivity to Current Assets (Fixed Current Liabilities: $200,000)')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/credit/financial_ratios/#example-output","title":"Example Output","text":"<pre><code>Financial Ratios Analysis\n========================\n\nExample 1: Strong Company\nCurrent Ratio: 2.50 (Strong)\nDebt Ratio: 0.30 (Strong)\nReturn on Equity: 0.21 (Strong)\nInterest Coverage: 8.00 (Strong)\nAsset Turnover: 1.25\nOverall Assessment: Strong financial position\n\nExample 2: Adequate Company\nCurrent Ratio: 1.50 (Adequate)\nDebt Ratio: 0.50 (Adequate)\nReturn on Equity: 0.13 (Adequate)\nInterest Coverage: 2.50 (Adequate)\nAsset Turnover: 0.80\nOverall Assessment: Adequate financial position\n\nExample 3: Weak Company\nCurrent Ratio: 0.75 (Weak)\nDebt Ratio: 0.70 (Weak)\nReturn on Equity: 0.07 (Weak)\nInterest Coverage: 0.83 (Weak)\nAsset Turnover: 0.60\nOverall Assessment: Weak financial position\n</code></pre>"},{"location":"user-guide/credit/financial_ratios/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/credit/financial_ratios/#financial-ratios-comparison","title":"Financial Ratios Comparison","text":"<p>The following visualization shows a comparison of key financial ratios across three different companies (strong, adequate, and weak):</p> <p></p> <p>This chart displays four key financial ratios: - Current Ratio: Shows liquidity with thresholds at 2.0 (strong) and 1.0 (weak) - Debt Ratio: Shows solvency with thresholds at 0.4 (strong) and 0.6 (weak) - Interest Coverage Ratio: Shows debt service ability with thresholds at 3.0 (strong) and 1.5 (weak) - Return on Equity: Shows profitability with thresholds at 0.15 (strong) and 0.08 (weak)</p>"},{"location":"user-guide/credit/financial_ratios/#ratio-sensitivity-analysis","title":"Ratio Sensitivity Analysis","text":"<p>The following visualization demonstrates how the current ratio changes as current assets increase, while keeping current liabilities constant:</p> <p></p> <p>This sensitivity analysis shows: - The blue line represents the current ratio as current assets increase - The red region represents the \"Weak\" assessment zone (ratio &lt; 1.0) - The orange region represents the \"Adequate\" assessment zone (1.0 \u2264 ratio &lt; 2.0) - The green region represents the \"Strong\" assessment zone (ratio \u2265 2.0) - The horizontal dashed lines mark the threshold values at 1.0 and 2.0</p>"},{"location":"user-guide/credit/financial_ratios/#ratio-categories-and-thresholds","title":"Ratio Categories and Thresholds","text":""},{"location":"user-guide/credit/financial_ratios/#1-liquidity-ratios","title":"1. Liquidity Ratios","text":"<p>Measure a company's ability to pay short-term obligations.</p> <ul> <li>Current Ratio = Current Assets / Current Liabilities</li> <li>Strong: \u2265 2.0</li> <li>Adequate: 1.0 - 2.0</li> <li>Weak: &lt; 1.0</li> </ul>"},{"location":"user-guide/credit/financial_ratios/#2-solvency-ratios","title":"2. Solvency Ratios","text":"<p>Measure a company's ability to meet long-term obligations.</p> <ul> <li>Debt Ratio = Total Liabilities / Total Assets</li> <li>Strong: \u2264 0.4</li> <li>Adequate: 0.4 - 0.6</li> <li> <p>Weak: &gt; 0.6</p> </li> <li> <p>Debt-to-Equity Ratio = Total Liabilities / Total Equity</p> </li> <li>Lower values indicate better solvency</li> </ul>"},{"location":"user-guide/credit/financial_ratios/#3-profitability-ratios","title":"3. Profitability Ratios","text":"<p>Measure a company's ability to generate earnings relative to its assets and equity.</p> <ul> <li>Return on Assets (ROA) = Net Income / Total Assets</li> <li> <p>Higher values indicate better profitability</p> </li> <li> <p>Return on Equity (ROE) = Net Income / Total Equity</p> </li> <li>Strong: \u2265 0.15 (15%)</li> <li>Adequate: 0.08 - 0.15 (8% - 15%)</li> <li>Weak: &lt; 0.08 (8%)</li> </ul>"},{"location":"user-guide/credit/financial_ratios/#4-coverage-ratios","title":"4. Coverage Ratios","text":"<p>Measure a company's ability to service its debt.</p> <ul> <li>Interest Coverage Ratio = EBIT / Interest Expense</li> <li>Strong: \u2265 3.0</li> <li>Adequate: 1.5 - 3.0</li> <li>Weak: &lt; 1.5</li> </ul>"},{"location":"user-guide/credit/financial_ratios/#5-efficiency-ratios","title":"5. Efficiency Ratios","text":"<p>Measure how effectively a company uses its assets.</p> <ul> <li>Asset Turnover Ratio = Sales / Total Assets</li> <li>Higher values indicate better efficiency</li> </ul>"},{"location":"user-guide/credit/financial_ratios/#practical-applications","title":"Practical Applications","text":"<p>Financial ratios can be used for:</p> <ol> <li>Credit Risk Assessment: Evaluating a borrower's financial health</li> <li>Investment Analysis: Identifying financially stable companies</li> <li>Benchmarking: Comparing a company's performance against industry peers</li> <li>Trend Analysis: Monitoring changes in a company's financial health over time</li> <li>Covenant Compliance: Ensuring borrowers maintain acceptable financial metrics</li> </ol>"},{"location":"user-guide/credit/financial_ratios/#limitations","title":"Limitations","text":"<p>When using financial ratios, consider these limitations:</p> <ol> <li>Industry differences may affect appropriate ratio values</li> <li>Seasonal variations can impact short-term ratios</li> <li>Accounting methods can affect ratio calculations</li> <li>Historical ratios may not predict future performance</li> <li>Ratios should be used alongside other financial metrics for comprehensive analysis </li> </ol>"},{"location":"user-guide/credit/loan_pricing/","title":"Loan Pricing","text":"<p>The <code>loan_pricing</code> function implements a risk-based loan pricing model that calculates the appropriate interest rate for a loan based on various risk factors and cost components. This model helps lenders determine fair and profitable loan terms while accounting for the borrower's risk profile.</p>"},{"location":"user-guide/credit/loan_pricing/#components-of-loan-pricing","title":"Components of Loan Pricing","text":"<p>The loan pricing model considers several key components:</p> <ol> <li>Expected Loss Component: Accounts for the probability of default and the expected loss given default</li> <li>Funding Cost Component: Reflects the lender's cost of obtaining funds</li> <li>Operating Cost Component: Covers the administrative costs of originating and servicing the loan</li> <li>Capital Cost Component: Accounts for the required return on the capital allocated to support the loan</li> </ol>"},{"location":"user-guide/credit/loan_pricing/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.credit import loan_pricing\n\n# Calculate risk-based loan pricing\nresult = loan_pricing(\n    loan_amount=100000,           # $100,000 loan\n    term=5,                       # 5-year term\n    pd=0.02,                      # 2% annual probability of default\n    lgd=0.4,                      # 40% loss given default\n    funding_cost=0.03,            # 3% cost of funds\n    operating_cost=0.01,          # 1% operating costs\n    capital_requirement=0.08,     # 8% capital requirement\n    target_roe=0.15               # 15% target return on equity\n)\n\n# Access the results\nrecommended_rate = result[\"recommended_rate\"]\nmonthly_payment = result[\"monthly_payment\"]\ncomponents = result[\"components\"]\n</code></pre>"},{"location":"user-guide/credit/loan_pricing/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>loan_amount</code> float The principal amount of the loan Required <code>term</code> float The loan term in years Required <code>pd</code> float Probability of default (annual rate, between 0 and 1) Required <code>lgd</code> float Loss given default (as a decimal, between 0 and 1) Required <code>funding_cost</code> float Cost of funds (annual rate) Required <code>operating_cost</code> float Operating costs as percentage of loan amount Required <code>capital_requirement</code> float Capital requirement as percentage of loan amount Required <code>target_roe</code> float Target return on equity (annual rate) Required"},{"location":"user-guide/credit/loan_pricing/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>recommended_rate</code> float The calculated interest rate for the loan <code>effective_annual_rate</code> float The effective annual rate (APR) <code>monthly_payment</code> float The calculated monthly payment amount <code>total_interest</code> float Total interest paid over the life of the loan <code>expected_profit</code> float Expected profit after accounting for losses and costs <code>return_on_investment</code> float Expected return on the allocated capital <code>components</code> dict Dictionary containing the individual pricing components <p>The <code>components</code> dictionary includes: - <code>expected_loss</code>: Component accounting for credit risk - <code>funding_cost</code>: Component accounting for cost of funds - <code>operating_cost</code>: Component accounting for operational expenses - <code>capital_cost</code>: Component accounting for capital allocation - <code>risk_premium</code>: Combined risk-related components (expected_loss + capital_cost)</p>"},{"location":"user-guide/credit/loan_pricing/#risk-level-classification","title":"Risk Level Classification","text":"<p>The loan risk is categorized based on the expected loss rate (PD \u00d7 LGD):</p> Expected Loss Rate Range Risk Level &lt; 1% Very Low 1% - 3% Low 3% - 7% Moderate 7% - 15% High &gt; 15% Very High"},{"location":"user-guide/credit/loan_pricing/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to calculate and analyze loan pricing for different risk profiles:</p> <pre><code>from pypulate.credit import loan_pricing\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Example 1: Low-risk borrower\nlow_risk_loan = loan_pricing(\n    loan_amount=100000,           # $100,000 loan\n    term=5,                       # 5-year term\n    pd=0.01,                      # 1% probability of default\n    lgd=0.3,                      # 30% loss given default\n    funding_cost=0.03,            # 3% cost of funds\n    operating_cost=0.01,          # 1% operating costs\n    capital_requirement=0.08,     # 8% capital requirement\n    target_roe=0.15               # 15% target return on equity\n)\n\n# Example 2: Medium-risk borrower\nmedium_risk_loan = loan_pricing(\n    loan_amount=100000,           # $100,000 loan\n    term=5,                       # 5-year term\n    pd=0.03,                      # 3% probability of default\n    lgd=0.4,                      # 40% loss given default\n    funding_cost=0.03,            # 3% cost of funds\n    operating_cost=0.01,          # 1% operating costs\n    capital_requirement=0.08,     # 8% capital requirement\n    target_roe=0.15               # 15% target return on equity\n)\n\n# Example 3: High-risk borrower\nhigh_risk_loan = loan_pricing(\n    loan_amount=100000,           # $100,000 loan\n    term=5,                       # 5-year term\n    pd=0.08,                      # 8% probability of default\n    lgd=0.5,                      # 50% loss given default\n    funding_cost=0.03,            # 3% cost of funds\n    operating_cost=0.01,          # 1% operating costs\n    capital_requirement=0.08,     # 8% capital requirement\n    target_roe=0.15               # 15% target return on equity\n)\n\n# Print the results\nprint(\"Risk-Based Loan Pricing Analysis\")\nprint(\"===============================\")\n\nprint(\"\\nExample 1: Low-Risk Borrower\")\nprint(f\"Recommended Interest Rate: {low_risk_loan['recommended_rate']:.2%}\")\nprint(f\"Effective Annual Rate: {low_risk_loan['effective_annual_rate']:.2%}\")\nprint(f\"Monthly Payment: ${low_risk_loan['monthly_payment']:.2f}\")\nprint(f\"Total Interest: ${low_risk_loan['total_interest']:.2f}\")\nprint(f\"Expected Profit: ${low_risk_loan['expected_profit']:.2f}\")\nprint(f\"Return on Investment: {low_risk_loan['return_on_investment']:.2%}\")\nprint(\"Pricing Components:\")\nfor component, value in low_risk_loan['components'].items():\n    print(f\"  {component}: {value:.2%}\")\n\nprint(\"\\nExample 2: Medium-Risk Borrower\")\nprint(f\"Recommended Interest Rate: {medium_risk_loan['recommended_rate']:.2%}\")\nprint(f\"Effective Annual Rate: {medium_risk_loan['effective_annual_rate']:.2%}\")\nprint(f\"Monthly Payment: ${medium_risk_loan['monthly_payment']:.2f}\")\nprint(f\"Total Interest: ${medium_risk_loan['total_interest']:.2f}\")\nprint(f\"Expected Profit: ${medium_risk_loan['expected_profit']:.2f}\")\nprint(f\"Return on Investment: {medium_risk_loan['return_on_investment']:.2%}\")\nprint(\"Pricing Components:\")\nfor component, value in medium_risk_loan['components'].items():\n    print(f\"  {component}: {value:.2%}\")\n\nprint(\"\\nExample 3: High-Risk Borrower\")\nprint(f\"Recommended Interest Rate: {high_risk_loan['recommended_rate']:.2%}\")\nprint(f\"Effective Annual Rate: {high_risk_loan['effective_annual_rate']:.2%}\")\nprint(f\"Monthly Payment: ${high_risk_loan['monthly_payment']:.2f}\")\nprint(f\"Total Interest: ${high_risk_loan['total_interest']:.2f}\")\nprint(f\"Expected Profit: ${high_risk_loan['expected_profit']:.2f}\")\nprint(f\"Return on Investment: {high_risk_loan['return_on_investment']:.2%}\")\nprint(\"Pricing Components:\")\nfor component, value in high_risk_loan['components'].items():\n    print(f\"  {component}: {value:.2%}\")\n\n# Visualize the results - Interest Rate Comparison\nrisk_profiles = ['Low Risk', 'Medium Risk', 'High Risk']\ninterest_rates = [\n    low_risk_loan['recommended_rate'],\n    medium_risk_loan['recommended_rate'],\n    high_risk_loan['recommended_rate']\n]\n\nplt.figure(figsize=(10, 6))\nbars = plt.bar(risk_profiles, interest_rates, color=['green', 'orange', 'red'])\n\n# Add the rate values on top of the bars\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n             f'{height:.2%}', ha='center', va='bottom')\n\nplt.ylabel('Recommended Interest Rate')\nplt.title('Risk-Based Interest Rate Comparison')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n# Visualize the pricing components\ncomponents = ['Expected Loss', 'Funding Cost', 'Operating Cost', 'Capital Cost']\nlow_risk_components = [\n    low_risk_loan['components']['expected_loss'],\n    low_risk_loan['components']['funding_cost'],\n    low_risk_loan['components']['operating_cost'],\n    low_risk_loan['components']['capital_cost']\n]\nmedium_risk_components = [\n    medium_risk_loan['components']['expected_loss'],\n    medium_risk_loan['components']['funding_cost'],\n    medium_risk_loan['components']['operating_cost'],\n    medium_risk_loan['components']['capital_cost']\n]\nhigh_risk_components = [\n    high_risk_loan['components']['expected_loss'],\n    high_risk_loan['components']['funding_cost'],\n    high_risk_loan['components']['operating_cost'],\n    high_risk_loan['components']['capital_cost']\n]\n\nx = np.arange(len(components))  # the label locations\nwidth = 0.25  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(12, 7))\nrects1 = ax.bar(x - width, low_risk_components, width, label='Low Risk', color='green')\nrects2 = ax.bar(x, medium_risk_components, width, label='Medium Risk', color='orange')\nrects3 = ax.bar(x + width, high_risk_components, width, label='High Risk', color='red')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Rate Component')\nax.set_title('Loan Pricing Components by Risk Profile')\nax.set_xticks(x)\nax.set_xticklabels(components)\nax.legend()\n\n# Add value labels\ndef autolabel(rects):\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate(f'{height:.2%}',\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\nautolabel(rects1)\nautolabel(rects2)\nautolabel(rects3)\n\nfig.tight_layout()\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n\n# Create a sensitivity analysis for PD\npd_values = np.linspace(0.01, 0.10, 10)  # Range of PD values from 1% to 10%\ninterest_rates = []\nmonthly_payments = []\nexpected_profits = []\n\nfor pd_value in pd_values:\n    result = loan_pricing(\n        loan_amount=100000,\n        term=5,\n        pd=pd_value,\n        lgd=0.4,\n        funding_cost=0.03,\n        operating_cost=0.01,\n        capital_requirement=0.08,\n        target_roe=0.15\n    )\n    interest_rates.append(result['recommended_rate'])\n    monthly_payments.append(result['monthly_payment'])\n    expected_profits.append(result['expected_profit'])\n\n# Plot interest rate sensitivity to PD\nplt.figure(figsize=(12, 6))\nplt.plot(pd_values * 100, np.array(interest_rates) * 100, 'b-', linewidth=2, marker='o')\nplt.xlabel('Probability of Default (%)')\nplt.ylabel('Recommended Interest Rate (%)')\nplt.title('Interest Rate Sensitivity to Probability of Default')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n# Plot monthly payment sensitivity to PD\nplt.figure(figsize=(12, 6))\nplt.plot(pd_values * 100, monthly_payments, 'g-', linewidth=2, marker='o')\nplt.xlabel('Probability of Default (%)')\nplt.ylabel('Monthly Payment ($)')\nplt.title('Monthly Payment Sensitivity to Probability of Default')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n# Plot expected profit sensitivity to PD\nplt.figure(figsize=(12, 6))\nplt.plot(pd_values * 100, expected_profits, 'r-', linewidth=2, marker='o')\nplt.xlabel('Probability of Default (%)')\nplt.ylabel('Expected Profit ($)')\nplt.title('Expected Profit Sensitivity to Probability of Default')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/credit/loan_pricing/#example-output","title":"Example Output","text":"<pre><code>Risk-Based Loan Pricing Analysis\n===============================\n\nExample 1: Low-Risk Borrower\nRecommended Interest Rate: 4.46%\nEffective Annual Rate: 4.55%\nMonthly Payment: $1862.48\nTotal Interest: $11749.02\nExpected Profit: $6449.02\nReturn on Investment: 80.61%\nPricing Components:\n  expected_loss: 0.06%\n  funding_cost: 3.00%\n  operating_cost: 0.20%\n  capital_cost: 1.20%\n  risk_premium: 1.26%\n\nExample 2: Medium-Risk Borrower\nRecommended Interest Rate: 4.64%\nEffective Annual Rate: 4.74%\nMonthly Payment: $1870.67\nTotal Interest: $12240.48\nExpected Profit: $6040.48\nReturn on Investment: 75.51%\nPricing Components:\n  expected_loss: 0.24%\n  funding_cost: 3.00%\n  operating_cost: 0.20%\n  capital_cost: 1.20%\n  risk_premium: 1.44%\n\nExample 3: High-Risk Borrower\nRecommended Interest Rate: 5.20%\nEffective Annual Rate: 5.33%\nMonthly Payment: $1896.30\nTotal Interest: $13778.00\nExpected Profit: $4778.00\nReturn on Investment: 59.72%\nPricing Components:\n  expected_loss: 0.80%\n  funding_cost: 3.00%\n  operating_cost: 0.20%\n  capital_cost: 1.20%\n  risk_premium: 2.00%\n</code></pre>"},{"location":"user-guide/credit/loan_pricing/#pricing-component-analysis","title":"Pricing Component Analysis","text":"<p>Each component of the loan pricing model serves a specific purpose:</p> <ol> <li> <p>Expected Loss Component (pd \u00d7 lgd / term)</p> <ul> <li>Compensates for the expected credit losses</li> <li>Directly proportional to both probability of default and loss severity</li> <li>Higher risk borrowers have significantly higher expected loss components</li> </ul> </li> <li> <p>Funding Cost Component</p> <ul> <li>Represents the lender's cost of obtaining the funds to lend</li> <li>Typically based on market interest rates</li> <li>Generally consistent across borrowers regardless of risk</li> </ul> </li> <li> <p>Operating Cost Component (operating_cost / term)</p> <ul> <li>Covers origination, servicing, and administrative costs</li> <li>Spread over the life of the loan</li> <li>May vary slightly based on loan complexity</li> </ul> </li> <li> <p>Capital Cost Component (capital_requirement \u00d7 target_roe)</p> <ul> <li>Compensates for the opportunity cost of capital allocated to the loan</li> <li>Higher risk loans may require more capital allocation</li> <li>Reflects the lender's required return on invested capital</li> </ul> </li> <li> <p>Risk Premium (expected_loss + capital_cost)</p> <ul> <li>The combined risk-related components</li> <li>Represents the additional return required to compensate for risk</li> <li>Primary differentiator in pricing between low and high-risk borrowers</li> </ul> </li> </ol>"},{"location":"user-guide/credit/loan_pricing/#practical-applications","title":"Practical Applications","text":"<p>Risk-based loan pricing can be used for:</p> <ol> <li>Consumer Lending: Setting appropriate rates for personal loans, auto loans, and mortgages</li> <li>Commercial Lending: Pricing business loans based on company financial health</li> <li>Credit Card Pricing: Determining APRs for different customer segments</li> <li>Loan Portfolio Management: Ensuring adequate returns across a portfolio of loans</li> <li>Competitive Analysis: Benchmarking pricing against market competitors</li> </ol>"},{"location":"user-guide/credit/loan_pricing/#limitations-and-considerations","title":"Limitations and Considerations","text":"<p>When using risk-based loan pricing, consider these limitations:</p> <ol> <li>Model Assumptions: The accuracy depends on reliable estimates of PD and LGD</li> <li>Market Constraints: Competitive pressures may limit the ability to charge risk-appropriate rates</li> <li>Regulatory Considerations: Fair lending laws may restrict risk-based pricing in some markets</li> <li>Customer Acceptance: Very high rates may lead to adverse selection or reduced demand</li> <li>Economic Cycles: Risk parameters should be adjusted for changing economic conditions </li> </ol>"},{"location":"user-guide/credit/logistic_regression_score/","title":"Logistic Regression Score","text":"<p>The <code>logistic_regression_score</code> function implements a credit scoring model based on logistic regression, which is widely used in credit risk assessment. This function converts logistic regression outputs into a credit score on a standard scale (300-850), making it easier to interpret and use in credit decisions.</p>"},{"location":"user-guide/credit/logistic_regression_score/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.credit import logistic_regression_score\n\n# Calculate credit score using logistic regression\nresult = logistic_regression_score(\n    coefficients=[0.5, -0.3, 0.8, -0.4],  # Coefficients from logistic regression model\n    features=[25000, 0.3, 5, 2],          # Feature values (e.g., income, DTI, years employed, inquiries)\n    intercept=-2.5                        # Intercept term from logistic regression model\n)\n\n# Access the results\nprobability = result[\"probability_of_default\"]\nscore = result[\"credit_score\"]\nrisk_category = result[\"risk_category\"]\nlog_odds = result[\"log_odds\"]\n</code></pre>"},{"location":"user-guide/credit/logistic_regression_score/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>coefficients</code> array_like Coefficients from the logistic regression model Required <code>features</code> array_like Feature values for the borrower being scored Required <code>intercept</code> float Intercept term from the logistic regression model 0"},{"location":"user-guide/credit/logistic_regression_score/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>probability_of_default</code> float The calculated probability of default (between 0 and 1) <code>credit_score</code> int The credit score on a 300-850 scale <code>risk_category</code> str Categorical risk assessment (\"Excellent\", \"Good\", \"Fair\", \"Poor\", or \"Very Poor\") <code>log_odds</code> float The log odds from the logistic regression calculation"},{"location":"user-guide/credit/logistic_regression_score/#risk-level-classification","title":"Risk Level Classification","text":"<p>The credit score is categorized into risk levels:</p> Credit Score Range Risk Level 750-850 Excellent 700-749 Good 650-699 Fair 600-649 Poor 300-599 Very Poor"},{"location":"user-guide/credit/logistic_regression_score/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to calculate and analyze credit scores for different borrowers:</p> <pre><code>from pypulate.credit import logistic_regression_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define a simple logistic regression model\n# Coefficients for: income (in $10k), DTI ratio, years employed, recent inquiries\ncoefficients = [\n    -0.2,  # Income (negative coefficient: higher income -&gt; lower default probability)\n    2.5,   # DTI ratio (positive coefficient: higher DTI -&gt; higher default probability)\n    -0.3,  # Years employed (negative coefficient: longer employment -&gt; lower default probability)\n    0.4    # Recent inquiries (positive coefficient: more inquiries -&gt; higher default probability)\n]\nintercept = -1.0  # Intercept term\n\n# Example 1: Low-risk borrower\nlow_risk_borrower = logistic_regression_score(\n    coefficients=coefficients,\n    features=[8.0, 0.25, 10, 0],  # $80k income, 25% DTI, 10 years employed, 0 inquiries\n    intercept=intercept\n)\n\n# Example 2: Medium-risk borrower\nmedium_risk_borrower = logistic_regression_score(\n    coefficients=coefficients,\n    features=[4.5, 0.42, 3, 2],  # $45k income, 42% DTI, 3 years employed, 2 inquiries\n    intercept=intercept\n)\n\n# Example 3: High-risk borrower\nhigh_risk_borrower = logistic_regression_score(\n    coefficients=coefficients,\n    features=[3.0, 0.45, 2, 4],  # $30k income, 45% DTI, 2 years employed, 4 inquiries\n    intercept=intercept\n)\n\n# Print the results\nprint(\"Logistic Regression Credit Scoring Analysis\")\nprint(\"==========================================\")\n\nprint(\"\\nExample 1: Low-Risk Borrower\")\nprint(f\"Credit Score: {low_risk_borrower['credit_score']}\")\nprint(f\"Probability of Default: {low_risk_borrower['probability_of_default']:.4f}\")\nprint(f\"Risk Category: {low_risk_borrower['risk_category']}\")\nprint(f\"Log Odds: {low_risk_borrower['log_odds']:.4f}\")\n\nprint(\"\\nExample 2: Medium-Risk Borrower\")\nprint(f\"Credit Score: {medium_risk_borrower['credit_score']}\")\nprint(f\"Probability of Default: {medium_risk_borrower['probability_of_default']:.4f}\")\nprint(f\"Risk Category: {medium_risk_borrower['risk_category']}\")\nprint(f\"Log Odds: {medium_risk_borrower['log_odds']:.4f}\")\n\nprint(\"\\nExample 3: High-Risk Borrower\")\nprint(f\"Credit Score: {high_risk_borrower['credit_score']}\")\nprint(f\"Probability of Default: {high_risk_borrower['probability_of_default']:.4f}\")\nprint(f\"Risk Category: {high_risk_borrower['risk_category']}\")\nprint(f\"Log Odds: {high_risk_borrower['log_odds']:.4f}\")\n\n# Visualize the results - Credit Score Comparison\nrisk_profiles = ['Low Risk', 'Medium Risk', 'High Risk']\nscores = [\n    low_risk_borrower['credit_score'],\n    medium_risk_borrower['credit_score'],\n    high_risk_borrower['credit_score']\n]\nprobabilities = [\n    low_risk_borrower['probability_of_default'],\n    medium_risk_borrower['probability_of_default'],\n    high_risk_borrower['probability_of_default']\n]\n\n# Create a bar chart for credit score comparison\nplt.figure(figsize=(10, 6))\nbars = plt.bar(risk_profiles, scores, color=['green', 'orange', 'red'])\n\n# Add horizontal lines for the score thresholds\nplt.axhline(y=750, color='g', linestyle='--', label='Excellent (\u2265 750)')\nplt.axhline(y=700, color='b', linestyle='--', label='Good (\u2265 700)')\nplt.axhline(y=650, color='orange', linestyle='--', label='Fair (\u2265 650)')\nplt.axhline(y=600, color='r', linestyle='--', label='Poor (\u2265 600)')\n\n# Add the score values on top of the bars\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height + 5,\n             f'{height:.0f}', ha='center', va='bottom')\n\nplt.ylabel('Credit Score')\nplt.title('Credit Score Comparison')\nplt.ylim(300, 850)  # Standard credit score range\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n# Create a figure showing the relationship between probability and score\nplt.figure(figsize=(12, 6))\nprob_range = np.linspace(0, 1, 100)\nscore_range = 850 - 550 * prob_range\nscore_range = np.clip(score_range, 300, 850)\n\nplt.plot(prob_range, score_range, 'b-', linewidth=2)\n\n# Add points for our examples\nplt.scatter([low_risk_borrower['probability_of_default']], [low_risk_borrower['credit_score']], \n            color='green', s=100, label='Low Risk')\nplt.scatter([medium_risk_borrower['probability_of_default']], [medium_risk_borrower['credit_score']], \n            color='orange', s=100, label='Medium Risk')\nplt.scatter([high_risk_borrower['probability_of_default']], [high_risk_borrower['credit_score']], \n            color='red', s=100, label='High Risk')\n\n# Add horizontal lines for score categories\nplt.axhline(y=750, color='g', linestyle='--')\nplt.axhline(y=700, color='b', linestyle='--')\nplt.axhline(y=650, color='orange', linestyle='--')\nplt.axhline(y=600, color='r', linestyle='--')\n\n# Add text labels for score categories\nplt.text(0.95, 800, 'Excellent', ha='right', va='center', color='green', fontweight='bold')\nplt.text(0.95, 725, 'Good', ha='right', va='center', color='blue', fontweight='bold')\nplt.text(0.95, 675, 'Fair', ha='right', va='center', color='orange', fontweight='bold')\nplt.text(0.95, 625, 'Poor', ha='right', va='center', color='red', fontweight='bold')\nplt.text(0.95, 450, 'Very Poor', ha='right', va='center', color='darkred', fontweight='bold')\n\nplt.xlabel('Probability of Default')\nplt.ylabel('Credit Score')\nplt.title('Relationship Between Probability of Default and Credit Score')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Create a sensitivity analysis for a single feature\nfeature_index = 1  # DTI ratio (index 1 in our feature list)\nfeature_name = \"Debt-to-Income Ratio\"\nfeature_values = np.linspace(0.1, 0.6, 50)  # Range of DTI values from 10% to 60%\nscores = []\nprobabilities = []\n\n# Base features for a typical borrower\nbase_features = [5.0, 0.35, 5, 2]  # $50k income, 35% DTI, 5 years employed, 2 inquiries\n\nfor feature_value in feature_values:\n    # Create a copy of base features and update the feature of interest\n    test_features = base_features.copy()\n    test_features[feature_index] = feature_value\n\n    # Calculate score\n    result = logistic_regression_score(\n        coefficients=coefficients,\n        features=test_features,\n        intercept=intercept\n    )\n    scores.append(result['credit_score'])\n    probabilities.append(result['probability_of_default'])\n\n# Plot score sensitivity to feature\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(feature_values * 100, scores, 'b-', linewidth=2)\nplt.xlabel(f'{feature_name} (%)')\nplt.ylabel('Credit Score')\nplt.title(f'Credit Score Sensitivity to {feature_name}')\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Add horizontal lines for score categories\nplt.axhline(y=750, color='g', linestyle='--', label='Excellent (\u2265 750)')\nplt.axhline(y=700, color='b', linestyle='--', label='Good (\u2265 700)')\nplt.axhline(y=650, color='orange', linestyle='--', label='Fair (\u2265 650)')\nplt.axhline(y=600, color='r', linestyle='--', label='Poor (\u2265 600)')\nplt.legend(loc='lower left')\n\n# Plot probability sensitivity to feature\nplt.subplot(1, 2, 2)\nplt.plot(feature_values * 100, probabilities, 'r-', linewidth=2)\nplt.xlabel(f'{feature_name} (%)')\nplt.ylabel('Probability of Default')\nplt.title(f'Default Probability Sensitivity to {feature_name}')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/credit/logistic_regression_score/#example-output","title":"Example Output","text":"<pre><code>Logistic Regression Credit Scoring Analysis\n==========================================\nExample 1: Low-Risk Borrower\nCredit Score: 847\nProbability of Default: 0.0069\nRisk Category: Excellent\nLog Odds: -4.9750\n\nExample 2: Medium-Risk Borrower\nCredit Score: 697\nProbability of Default: 0.2789\nRisk Category: Fair\nLog Odds: -0.9500\n\nExample 3: High-Risk Borrower\nCredit Score: 505\nProbability of Default: 0.6283\nRisk Category: Very Poor\nLog Odds: 0.5250\n</code></pre>"},{"location":"user-guide/credit/logistic_regression_score/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/credit/logistic_regression_score/#credit-score-comparison","title":"Credit Score Comparison","text":"<p>The following visualization shows a comparison of credit scores across three different borrower profiles:</p> <p></p> <p>This chart displays the credit scores for low, medium, and high-risk borrowers, with horizontal lines indicating the threshold values that separate different credit quality categories.</p>"},{"location":"user-guide/credit/logistic_regression_score/#probability-to-score-relationship","title":"Probability to Score Relationship","text":"<p>The following visualization demonstrates the relationship between probability of default and credit score:</p> <p></p> <p>This chart illustrates how the credit score decreases as the probability of default increases, with points showing where our example borrowers fall on the curve.</p>"},{"location":"user-guide/credit/logistic_regression_score/#feature-sensitivity-analysis","title":"Feature Sensitivity Analysis","text":"<p>The following visualization shows how changes in a single feature affect both the credit score and probability of default:</p> <p></p> <p>This sensitivity analysis demonstrates how increasing the debt-to-income ratio leads to lower credit scores and higher default probabilities.</p>"},{"location":"user-guide/credit/logistic_regression_score/#practical-applications","title":"Practical Applications","text":"<p>Logistic regression scoring can be used for:</p> <ol> <li>Credit Underwriting: Automating credit decisions based on objective criteria</li> <li>Risk-Based Pricing: Setting interest rates based on creditworthiness</li> <li>Portfolio Segmentation: Dividing borrowers into risk tiers for targeted strategies</li> <li>Pre-qualification: Providing potential borrowers with preliminary credit assessments</li> <li>Account Management: Monitoring existing customers for changes in credit quality</li> </ol>"},{"location":"user-guide/credit/logistic_regression_score/#advantages-and-limitations","title":"Advantages and Limitations","text":""},{"location":"user-guide/credit/logistic_regression_score/#advantages","title":"Advantages","text":"<ol> <li>Interpretability: Coefficients directly show the impact of each feature</li> <li>Probability Output: Provides a meaningful probability of default</li> <li>Efficiency: Computationally simple and fast to implement</li> <li>Flexibility: Can incorporate various types of features</li> <li>Standard Scale: Converts to a familiar credit score scale</li> </ol>"},{"location":"user-guide/credit/logistic_regression_score/#limitations","title":"Limitations","text":"<ol> <li>Linearity Assumption: Assumes a linear relationship in the log odds</li> <li>Feature Independence: Doesn't naturally capture interactions between features</li> <li>Data Quality Dependency: Performance depends on the quality of training data</li> <li>Model Simplicity: May not capture complex patterns as well as more advanced models</li> <li>Calibration Needs: Requires proper calibration to produce accurate probabilities </li> </ol>"},{"location":"user-guide/credit/loss_given_default/","title":"Loss Given Default (LGD)","text":"<p>Loss Given Default (LGD) is a key component in credit risk modeling that estimates the portion of an exposure that is lost when a borrower defaults. It's a critical parameter in calculating expected credit losses and pricing loans.</p>"},{"location":"user-guide/credit/loss_given_default/#overview","title":"Overview","text":"<p>The LGD estimation in Pypulate considers:</p> <ul> <li>Collateral value and liquidation costs</li> <li>Loan amount and loan-to-value ratio</li> <li>Historical recovery rates (if available)</li> <li>Time value of money</li> </ul> <p>The model provides both a point-in-time LGD estimate and a present value calculation that accounts for the time to recovery.</p>"},{"location":"user-guide/credit/loss_given_default/#usage","title":"Usage","text":"<pre><code>from pypulate.credit import loss_given_default\n\n# Basic usage with collateral\nresult = loss_given_default(\n    collateral_value=80000,  # Value of collateral\n    loan_amount=100000,      # Outstanding loan amount\n    liquidation_costs=0.15,  # Costs to liquidate collateral (15%)\n    time_to_recovery=1.5     # Expected time to recovery in years\n)\n\n# With historical recovery rate\nresult = loss_given_default(\n    collateral_value=80000,\n    loan_amount=100000,\n    recovery_rate=0.6,       # Historical recovery rate for similar loans\n    liquidation_costs=0.15,\n    time_to_recovery=1.5\n)\n\n# Access results\nlgd = result[\"lgd\"]\npresent_value_lgd = result[\"present_value_lgd\"]\nrisk_level = result[\"risk_level\"]\ncomponents = result[\"components\"]\n</code></pre>"},{"location":"user-guide/credit/loss_given_default/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>collateral_value</code> float Value of collateral Required <code>loan_amount</code> float Outstanding loan amount Required <code>recovery_rate</code> float, optional Historical recovery rate for similar loans None <code>liquidation_costs</code> float Costs associated with liquidating collateral (as a decimal) 0.1 (10%) <code>time_to_recovery</code> float Expected time to recovery in years 1.0"},{"location":"user-guide/credit/loss_given_default/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>lgd</code> float Loss given default estimate <code>present_value_lgd</code> float Present value of LGD accounting for time to recovery <code>risk_level</code> str Risk level categorization (\"Very Low\", \"Low\", \"Moderate\", \"High\", \"Very High\") <code>components</code> dict Dictionary containing calculation components <p>The <code>components</code> dictionary includes:</p> <ul> <li><code>collateral_value</code>: Original collateral value</li> <li><code>net_collateral_value</code>: Collateral value after liquidation costs</li> <li><code>loan_amount</code>: Outstanding loan amount</li> <li><code>loan_to_value</code>: Loan-to-value ratio</li> <li><code>collateral_lgd</code>: LGD based solely on collateral</li> <li><code>time_value_factor</code>: Discount factor for time value of money</li> </ul> <p>If a recovery rate is provided, additional components are included:</p> <ul> <li><code>recovery_rate</code>: Historical recovery rate</li> <li><code>weight_collateral</code>: Weight assigned to collateral-based LGD</li> <li><code>weight_historical</code>: Weight assigned to historical recovery rate</li> </ul>"},{"location":"user-guide/credit/loss_given_default/#risk-level-classification","title":"Risk Level Classification","text":"<p>The LGD estimate is categorized into risk levels:</p> LGD Range Risk Level &lt; 0.1 (10%) Very Low 0.1 - 0.3 (10-30%) Low 0.3 - 0.5 (30-50%) Moderate 0.5 - 0.7 (50-70%) High &gt; 0.7 (70%) Very High"},{"location":"user-guide/credit/loss_given_default/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to calculate and analyze LGD for different loan scenarios using only matplotlib and numpy:</p> <pre><code>from pypulate.credit import loss_given_default\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define sample loan data\nloan_ids = [1, 2, 3, 4, 5]\nloan_amounts = [100000, 200000, 150000, 300000, 80000]\ncollateral_values = [120000, 180000, 100000, 250000, 50000]\nrecovery_rates = [0.65, 0.55, None, 0.6, 0.5]\nliquidation_costs = [0.1, 0.15, 0.12, 0.2, 0.1]\ntime_to_recovery = [1.0, 1.5, 1.0, 2.0, 1.0]\nloan_types = ['Mortgage', 'Commercial', 'Personal', 'Commercial', 'Personal']\n\n# Calculate LGD for each loan\nlgd_values = []\npv_lgd_values = []\nrisk_levels = []\nltv_values = []\ncollateral_lgd_values = []\nweight_collateral_values = []\nweight_historical_values = []\ntime_value_factors = []\n\nfor i in range(len(loan_ids)):\n    lgd_result = loss_given_default(\n        collateral_value=collateral_values[i],\n        loan_amount=loan_amounts[i],\n        recovery_rate=recovery_rates[i],\n        liquidation_costs=liquidation_costs[i],\n        time_to_recovery=time_to_recovery[i]\n    )\n\n    # Extract results\n    lgd_values.append(lgd_result['lgd'])\n    pv_lgd_values.append(lgd_result['present_value_lgd'])\n    risk_levels.append(lgd_result['risk_level'])\n\n    # Extract components\n    components = lgd_result['components']\n    ltv_values.append(components['loan_to_value'])\n    collateral_lgd_values.append(components.get('collateral_lgd', 0))\n    weight_collateral_values.append(components.get('weight_collateral', 1.0) if 'weight_collateral' in components else 1.0)\n    weight_historical_values.append(components.get('weight_historical', 0.0) if 'weight_historical' in components else 0.0)\n    time_value_factors.append(components['time_value_factor'])\n\n# Print the results\nprint(\"Loss Given Default Analysis\")\nprint(\"===========================\")\nprint(\"Loan ID | Loan Type   | LGD     | Present Value LGD | Risk Level\")\nprint(\"--------|-------------|---------|------------------|------------\")\nfor i in range(len(loan_ids)):\n    print(f\"{loan_ids[i]:7d} | {loan_types[i]:&lt;11s} | {lgd_values[i]:.6f} | {pv_lgd_values[i]:.6f} | {risk_levels[i]}\")\n\n# Calculate expected loss for each loan (assuming PD = 0.05 for all loans)\npd_value = 0.05  # 5% probability of default\nexpected_losses = [pd_value * loan_amounts[i] * lgd_values[i] for i in range(len(loan_ids))]\n\n# Print expected loss\nprint(\"\\nExpected Loss Analysis (PD = 5%)\")\nprint(\"================================\")\nfor i in range(len(loan_ids)):\n    print(f\"Loan {loan_ids[i]} ({loan_types[i]}): ${expected_losses[i]:.2f}\")\n\n# Visualize LGD by loan type\nplt.figure(figsize=(10, 6))\n\n# Group by loan type\nunique_loan_types = list(set(loan_types))\nloan_type_indices = {loan_type: [] for loan_type in unique_loan_types}\nfor i, loan_type in enumerate(loan_types):\n    loan_type_indices[loan_type].append(i)\n\n# Calculate average LGD by loan type\nloan_type_avg_lgd = []\nfor loan_type in unique_loan_types:\n    indices = loan_type_indices[loan_type]\n    avg_lgd = sum(lgd_values[i] for i in indices) / len(indices)\n    loan_type_avg_lgd.append(avg_lgd)\n\n# Create bar chart\nplt.bar(unique_loan_types, loan_type_avg_lgd, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\nplt.title('Average LGD by Loan Type')\nplt.xlabel('Loan Type')\nplt.ylabel('Loss Given Default (LGD)')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n# Visualize the relationship between LTV and LGD\nplt.figure(figsize=(10, 6))\n\n# Create scatter plot with different colors for loan types\ncolors = {'Mortgage': 'blue', 'Commercial': 'green', 'Personal': 'red'}\nfor loan_type in unique_loan_types:\n    indices = loan_type_indices[loan_type]\n    x = [ltv_values[i] for i in indices]\n    y = [lgd_values[i] for i in indices]\n    sizes = [ltv_values[i] * 100 for i in indices]  # Size proportional to LTV\n    plt.scatter(x, y, s=sizes, c=colors[loan_type], alpha=0.7, label=loan_type)\n\nplt.title('Relationship Between Loan-to-Value Ratio and LGD')\nplt.xlabel('Loan-to-Value Ratio')\nplt.ylabel('Loss Given Default (LGD)')\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Add risk level regions\nplt.axhspan(0, 0.1, alpha=0.2, color='green', label='Very Low Risk')\nplt.axhspan(0.1, 0.3, alpha=0.2, color='lightgreen')\nplt.axhspan(0.3, 0.5, alpha=0.2, color='yellow', label='Moderate Risk')\nplt.axhspan(0.5, 0.7, alpha=0.2, color='orange')\nplt.axhspan(0.7, 1.0, alpha=0.2, color='red', label='Very High Risk')\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Create a sensitivity analysis for collateral value and liquidation costs\n# For a fixed loan amount of $100,000\nloan_amount = 100000\ncollateral_values_array = np.linspace(50000, 150000, 11)  # 50% to 150% of loan amount\nliquidation_costs_array = np.linspace(0.05, 0.3, 6)  # 5% to 30%\n\n# Create matrices to store LGD values\nlgd_matrix = np.zeros((len(collateral_values_array), len(liquidation_costs_array)))\nltv_array = np.zeros(len(collateral_values_array))\n\n# Calculate LGD for each combination\nfor i, collateral in enumerate(collateral_values_array):\n    ltv_array[i] = loan_amount / collateral\n    for j, cost in enumerate(liquidation_costs_array):\n        result = loss_given_default(\n            collateral_value=collateral,\n            loan_amount=loan_amount,\n            liquidation_costs=cost\n        )\n        lgd_matrix[i, j] = result['lgd']\n\n# Create a heatmap\nplt.figure(figsize=(12, 8))\nX, Y = np.meshgrid(liquidation_costs_array, collateral_values_array)\ncontour = plt.contourf(X, Y, lgd_matrix, levels=20, cmap='RdYlGn_r')\nplt.colorbar(contour, label='Loss Given Default (LGD)')\n\n# Add contour lines\ncontour_lines = plt.contour(X, Y, lgd_matrix, levels=[0.1, 0.3, 0.5, 0.7], \n                           colors='black', linestyles='dashed')\nplt.clabel(contour_lines, inline=True, fontsize=10)\n\n# Add labels and title\nplt.xlabel('Liquidation Costs')\nplt.ylabel('Collateral Value ($)')\nplt.title('LGD Sensitivity to Collateral Value and Liquidation Costs\\n(Loan Amount: $100,000)')\nplt.grid(True, linestyle='--', alpha=0.3)\n\n# Add LTV reference line\nax2 = plt.twinx()\nax2.plot(liquidation_costs_array, [100000] * len(liquidation_costs_array), 'r--', label='LTV = 1.0')\nax2.set_ylabel('Loan-to-Value Reference')\nax2.set_ylim(plt.ylim())\nax2.legend(loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\n# Analyze the impact of time to recovery on present value LGD\ntime_values_array = np.linspace(0.5, 5, 10)  # 0.5 to 5 years\nlgd_base = 0.4  # Base LGD of 40%\npv_lgd_array = []\n\nfor time in time_values_array:\n    result = loss_given_default(\n        collateral_value=75000,\n        loan_amount=100000,\n        time_to_recovery=time\n    )\n    pv_lgd_array.append(result['present_value_lgd'])\n\n# Plot the time value effect\nplt.figure(figsize=(10, 6))\nplt.plot(time_values_array, pv_lgd_array, 'b-', linewidth=2)\nplt.axhline(y=lgd_base, color='r', linestyle='--', label=f'Base LGD: {lgd_base:.1%}')\nplt.fill_between(time_values_array, pv_lgd_array, lgd_base, alpha=0.2, color='blue')\n\nplt.title('Impact of Time to Recovery on Present Value LGD')\nplt.xlabel('Time to Recovery (Years)')\nplt.ylabel('Present Value LGD')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Compare collateral-based LGD vs. recovery rate-based LGD\nrecovery_rates_array = np.linspace(0.3, 0.9, 7)\ncollateral_values_list = [60000, 80000, 100000, 120000]  # 60% to 120% of loan amount\nloan_amount = 100000\nliquidation_cost = 0.15\n\nplt.figure(figsize=(12, 8))\n\nfor collateral in collateral_values_list:\n    lgd_list = []\n    for recovery in recovery_rates_array:\n        result = loss_given_default(\n            collateral_value=collateral,\n            loan_amount=loan_amount,\n            recovery_rate=recovery,\n            liquidation_costs=liquidation_cost\n        )\n        lgd_list.append(result['lgd'])\n\n    ltv = loan_amount / collateral\n    plt.plot(recovery_rates_array, lgd_list, marker='o', linewidth=2, \n             label=f'Collateral: ${collateral:,} (LTV: {ltv:.2f})')\n\nplt.title('LGD vs. Recovery Rate for Different Collateral Values')\nplt.xlabel('Historical Recovery Rate')\nplt.ylabel('Loss Given Default (LGD)')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/credit/loss_given_default/#example-output","title":"Example Output","text":"<p>The above code will produce a detailed analysis of LGD across different loan scenarios, including:</p> <pre><code>Loss Given Default Analysis\n===========================\nLoan ID | Loan Type   | LGD     | Present Value LGD | Risk Level\n--------|-------------|---------|------------------|------------\n      1 | Mortgage    | 0.210000 | 0.200000 | Low\n      2 | Commercial  | 0.364000 | 0.338312 | Moderate\n      3 | Personal    | 0.413333 | 0.393651 | Moderate\n      4 | Commercial  | 0.373333 | 0.338624 | Moderate\n      5 | Personal    | 0.475000 | 0.452381 | Moderate\nExpected Loss Analysis (PD = 5%)\n================================\nLoan 1 (Mortgage): $1050.00\nLoan 2 (Commercial): $3640.00\nLoan 3 (Personal): $3100.00\nLoan 4 (Commercial): $5600.00\nLoan 5 (Personal): $1900.00\n</code></pre>"},{"location":"user-guide/credit/loss_given_default/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/credit/loss_given_default/#impact-of-time-to-recovery","title":"Impact of Time to Recovery","text":"<p>This visualization shows how the time to recovery affects the present value of LGD. As the time to recovery increases, the present value of LGD decreases due to the time value of money, even though the nominal LGD remains constant.</p> <p></p>"},{"location":"user-guide/credit/loss_given_default/#relationship-between-loan-to-value-ratio-and-lgd","title":"Relationship Between Loan-to-Value Ratio and LGD","text":"<p>This scatter plot illustrates the relationship between the loan-to-value ratio and LGD for different loan types. Higher LTV ratios generally correspond to higher LGD values. The background is color-coded to indicate different risk level regions.</p> <p></p>"},{"location":"user-guide/credit/loss_given_default/#lgd-sensitivity","title":"LGD Sensitivity","text":"<p>This heatmap demonstrates how LGD changes with different combinations of collateral value and liquidation costs, helping to visualize the sensitivity of LGD to these two key parameters. The contour lines represent different LGD levels.</p> <p></p>"},{"location":"user-guide/credit/loss_given_default/#key-insights","title":"Key Insights","text":"<p>From the comprehensive analysis, several key insights emerge:</p> <ol> <li> <p>Loan-to-Value (LTV) Ratio: As LTV increases, LGD typically increases. Loans with LTV &lt; 1.0 (where collateral exceeds loan amount) have lower LGD.</p> </li> <li> <p>Liquidation Costs: Higher liquidation costs significantly increase LGD, especially for loans with LTV close to 1.0.</p> </li> <li> <p>Recovery Rates: Historical recovery rates can provide valuable information to complement collateral-based LGD estimates.</p> </li> <li> <p>Time Value Effect: Longer recovery times reduce the present value of LGD but increase uncertainty.</p> </li> <li> <p>Risk Segmentation: LGD varies by loan type, with secured loans like mortgages typically having lower LGD than personal loans.</p> </li> </ol>"},{"location":"user-guide/credit/loss_given_default/#notes","title":"Notes","text":"<ul> <li>The LGD is expressed as a decimal between 0 and 1, where 0 means full recovery and 1 means total loss.</li> <li>The model assumes a fixed discount rate of 5% for present value calculations.</li> <li>When collateral value exceeds the loan amount (after liquidation costs), the collateral-based LGD is 0.</li> <li>The weighting between collateral-based LGD and historical recovery rates depends on the loan-to-value ratio. </li> </ul>"},{"location":"user-guide/credit/merton_model/","title":"Merton Model","text":"<p>The <code>merton_model</code> function implements the Merton structural model of default, a fundamental approach in credit risk modeling that treats a company's equity as a call option on its assets. This model, developed by Robert C. Merton in 1974, provides a framework for estimating the probability of default based on the company's capital structure and asset volatility.</p>"},{"location":"user-guide/credit/merton_model/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.credit import merton_model\n\n# Calculate default probability using the Merton model\nresult = merton_model(\n    asset_value=1000000,        # $1,000,000 market value of assets\n    debt_face_value=600000,     # $600,000 face value of debt\n    asset_volatility=0.25,      # 25% annualized asset volatility\n    risk_free_rate=0.03,        # 3% risk-free rate\n    time_to_maturity=1.0        # 1 year to debt maturity\n)\n\n# Access the results\npd = result[\"probability_of_default\"]\ndd = result[\"distance_to_default\"]\n</code></pre>"},{"location":"user-guide/credit/merton_model/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>asset_value</code> float Market value of the company's assets Required <code>debt_face_value</code> float Face value of the company's debt Required <code>asset_volatility</code> float Volatility of assets (annualized) Required <code>risk_free_rate</code> float Risk-free interest rate Required <code>time_to_maturity</code> float Time to debt maturity in years Required"},{"location":"user-guide/credit/merton_model/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>probability_of_default</code> float Probability of default within the time horizon <code>distance_to_default</code> float Number of standard deviations to default threshold <code>d1</code> float First parameter in the Black-Scholes-Merton formula <code>d2</code> float Second parameter in the Black-Scholes-Merton formula"},{"location":"user-guide/credit/merton_model/#risk-level-classification","title":"Risk Level Classification","text":"<p>The probability of default is categorized into risk levels:</p> Probability of Default Range Risk Level &lt; 0.5% Very Low 0.5% - 2% Low 2% - 5% Moderate 5% - 15% High &gt; 15% Very High"},{"location":"user-guide/credit/merton_model/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to calculate and analyze default probabilities for companies with different financial profiles:</p> <pre><code>from pypulate.credit import merton_model\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Example 1: Financially strong company\nstrong_company = merton_model(\n    asset_value=1000000,        # $1,000,000 market value of assets\n    debt_face_value=400000,     # $400,000 face value of debt\n    asset_volatility=0.20,      # 20% annualized asset volatility\n    risk_free_rate=0.03,        # 3% risk-free rate\n    time_to_maturity=1.0        # 1 year to debt maturity\n)\n\n# Example 2: Average company\naverage_company = merton_model(\n    asset_value=1000000,        # $1,000,000 market value of assets\n    debt_face_value=600000,     # $600,000 face value of debt\n    asset_volatility=0.30,      # 30% annualized asset volatility\n    risk_free_rate=0.03,        # 3% risk-free rate\n    time_to_maturity=1.0        # 1 year to debt maturity\n)\n\n# Example 3: Financially distressed company\ndistressed_company = merton_model(\n    asset_value=1000000,        # $1,000,000 market value of assets\n    debt_face_value=800000,     # $800,000 face value of debt\n    asset_volatility=0.40,      # 40% annualized asset volatility\n    risk_free_rate=0.03,        # 3% risk-free rate\n    time_to_maturity=1.0        # 1 year to debt maturity\n)\n\n# Print the results\nprint(\"Merton Model Analysis\")\nprint(\"====================\")\n\nprint(\"\\nExample 1: Financially Strong Company\")\nprint(f\"Probability of Default: {strong_company['probability_of_default']:.4%}\")\nprint(f\"Distance to Default: {strong_company['distance_to_default']:.2f}\")\n\nprint(\"\\nExample 2: Average Company\")\nprint(f\"Probability of Default: {average_company['probability_of_default']:.4%}\")\nprint(f\"Distance to Default: {average_company['distance_to_default']:.2f}\")\n\nprint(\"\\nExample 3: Financially Distressed Company\")\nprint(f\"Probability of Default: {distressed_company['probability_of_default']:.4%}\")\nprint(f\"Distance to Default: {distressed_company['distance_to_default']:.2f}\")\n\n# Create a DataFrame for visualization\ncompanies = ['Strong', 'Average', 'Distressed']\npd_values = [\n    strong_company['probability_of_default'],\n    average_company['probability_of_default'],\n    distressed_company['probability_of_default']\n]\ndd_values = [\n    strong_company['distance_to_default'],\n    average_company['distance_to_default'],\n    distressed_company['distance_to_default']\n]\n\n# Create a bar chart for probability of default\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nbars = plt.bar(companies, [pd * 100 for pd in pd_values], color=['green', 'orange', 'red'])\n\n# Add the PD values on top of the bars\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n             f'{height:.2f}%', ha='center', va='bottom')\n\nplt.ylabel('Probability of Default (%)')\nplt.title('Probability of Default by Company Type')\nplt.ylim(0, max([pd * 100 for pd in pd_values]) * 1.2)  # Add some space above the highest bar\n\n# Create a bar chart for distance to default\nplt.subplot(1, 2, 2)\nbars = plt.bar(companies, dd_values, color=['green', 'orange', 'red'])\n\n# Add the DD values on top of the bars\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n             f'{height:.2f}', ha='center', va='bottom')\n\nplt.ylabel('Distance to Default')\nplt.title('Distance to Default by Company Type')\nplt.tight_layout()\nplt.show()\n\n# Sensitivity analysis: Effect of leverage (debt-to-asset ratio) on PD\nleverage_ratios = np.linspace(0.1, 0.95, 50)  # Debt-to-asset ratios from 10% to 95%\npd_by_leverage = []\ndd_by_leverage = []\n\nfor leverage in leverage_ratios:\n    debt = leverage * 1000000  # Debt face value based on leverage ratio\n    result = merton_model(\n        asset_value=1000000,\n        debt_face_value=debt,\n        asset_volatility=0.30,\n        risk_free_rate=0.03,\n        time_to_maturity=1.0\n    )\n    pd_by_leverage.append(result['probability_of_default'])\n    dd_by_leverage.append(result['distance_to_default'])\n\n# Plot the effect of leverage on PD and DD\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(leverage_ratios, [pd * 100 for pd in pd_by_leverage], 'b-', linewidth=2)\nplt.xlabel('Leverage Ratio (Debt/Assets)')\nplt.ylabel('Probability of Default (%)')\nplt.title('Effect of Leverage on Default Probability')\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Add risk level regions\nplt.axhspan(0, 0.5, alpha=0.2, color='green', label='Very Low Risk')\nplt.axhspan(0.5, 2, alpha=0.2, color='lightgreen', label='Low Risk')\nplt.axhspan(2, 5, alpha=0.2, color='yellow', label='Moderate Risk')\nplt.axhspan(5, 15, alpha=0.2, color='orange', label='High Risk')\nplt.axhspan(15, 100, alpha=0.2, color='red', label='Very High Risk')\nplt.legend(loc='upper left')\n\nplt.subplot(1, 2, 2)\nplt.plot(leverage_ratios, dd_by_leverage, 'r-', linewidth=2)\nplt.xlabel('Leverage Ratio (Debt/Assets)')\nplt.ylabel('Distance to Default')\nplt.title('Effect of Leverage on Distance to Default')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n# Sensitivity analysis: Effect of asset volatility on PD\nvolatilities = np.linspace(0.1, 0.6, 50)  # Asset volatilities from 10% to 60%\npd_by_volatility = []\ndd_by_volatility = []\n\nfor vol in volatilities:\n    result = merton_model(\n        asset_value=1000000,\n        debt_face_value=600000,  # 60% leverage\n        asset_volatility=vol,\n        risk_free_rate=0.03,\n        time_to_maturity=1.0\n    )\n    pd_by_volatility.append(result['probability_of_default'])\n    dd_by_volatility.append(result['distance_to_default'])\n\n# Plot the effect of asset volatility on PD and DD\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(volatilities, [pd * 100 for pd in pd_by_volatility], 'b-', linewidth=2)\nplt.xlabel('Asset Volatility')\nplt.ylabel('Probability of Default (%)')\nplt.title('Effect of Asset Volatility on Default Probability')\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Add risk level regions\nplt.axhspan(0, 0.5, alpha=0.2, color='green', label='Very Low Risk')\nplt.axhspan(0.5, 2, alpha=0.2, color='lightgreen', label='Low Risk')\nplt.axhspan(2, 5, alpha=0.2, color='yellow', label='Moderate Risk')\nplt.axhspan(5, 15, alpha=0.2, color='orange', label='High Risk')\nplt.axhspan(15, 100, alpha=0.2, color='red', label='Very High Risk')\nplt.legend(loc='upper left')\n\nplt.subplot(1, 2, 2)\nplt.plot(volatilities, dd_by_volatility, 'r-', linewidth=2)\nplt.xlabel('Asset Volatility')\nplt.ylabel('Distance to Default')\nplt.title('Effect of Asset Volatility on Distance to Default')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/credit/merton_model/#example-output","title":"Example Output","text":"<pre><code>Merton Model Analysis\n====================\n\nMerton Model Analysis\n====================\nExample 1: Financially Strong Company\nProbability of Default: 0.0002%\nDistance to Default: 4.63\n\nExample 2: Average Company\nProbability of Default: 4.9191%\nDistance to Default: 1.65\n\nExample 3: Financially Distressed Company\nProbability of Default: 33.2559%\nDistance to Default: 0.43\n</code></pre>"},{"location":"user-guide/credit/merton_model/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/credit/merton_model/#default-probability-and-distance-to-default","title":"Default Probability and Distance to Default","text":"<p>These visualizations show the probability of default and distance to default for three example companies with different financial profiles.</p> <p></p>"},{"location":"user-guide/credit/merton_model/#sensitivity-to-leverage","title":"Sensitivity to Leverage","text":"<p>This analysis demonstrates how the probability of default and distance to default change with increasing leverage (debt-to-asset ratio), highlighting the non-linear relationship between leverage and default risk.</p> <p></p>"},{"location":"user-guide/credit/merton_model/#sensitivity-to-asset-volatility","title":"Sensitivity to Asset Volatility","text":"<p>This analysis shows how the probability of default and distance to default are affected by changes in asset volatility, illustrating the importance of asset stability in credit risk assessment.</p>"},{"location":"user-guide/credit/merton_model/#theoretical-background","title":"Theoretical Background","text":"<p>The Merton model is based on the following assumptions:</p> <ol> <li>The company's capital structure consists of equity and a single zero-coupon debt issue</li> <li>The company's asset value follows a geometric Brownian motion</li> <li>Default occurs only at debt maturity if the asset value falls below the face value of debt</li> <li>Markets are perfect (no transaction costs, taxes, or bankruptcy costs)</li> <li>The risk-free rate is constant</li> </ol> <p>Under these assumptions, the company's equity can be viewed as a European call option on the company's assets with a strike price equal to the face value of debt. The probability of default is then calculated as the probability that the asset value will be below the face value of debt at maturity.</p>"},{"location":"user-guide/credit/merton_model/#practical-applications","title":"Practical Applications","text":"<p>The Merton model can be used for:</p> <ol> <li>Credit Risk Assessment: Estimating default probabilities for corporate borrowers</li> <li>Bond Pricing: Determining credit spreads for corporate bonds</li> <li>Portfolio Management: Assessing the credit risk of investment portfolios</li> <li>Regulatory Capital: Calculating capital requirements for credit risk</li> <li>Early Warning System: Identifying companies with increasing default risk</li> </ol>"},{"location":"user-guide/credit/merton_model/#limitations","title":"Limitations","text":"<p>While the Merton model provides a theoretically sound framework for credit risk assessment, it has several limitations:</p> <ol> <li>Simplified Capital Structure: Assumes a single zero-coupon debt issue</li> <li>Default Timing: Assumes default can only occur at debt maturity</li> <li>Asset Value Unobservability: Requires estimation of unobservable asset value and volatility</li> <li>Constant Volatility: Assumes asset volatility is constant over time</li> <li>Perfect Markets: Ignores transaction costs, taxes, and bankruptcy costs</li> </ol>"},{"location":"user-guide/credit/merton_model/#extensions","title":"Extensions","text":"<p>Several extensions to the basic Merton model have been developed to address its limitations:</p> <ol> <li>KMV Model: Uses an iterative procedure to estimate asset value and volatility</li> <li>Black-Cox Model: Allows for default before maturity if asset value falls below a threshold</li> <li>Longstaff-Schwartz Model: Incorporates stochastic interest rates</li> <li>Leland Model: Accounts for bankruptcy costs and tax benefits of debt</li> <li>CreditGrades Model: Incorporates a stochastic default barrier </li> </ol>"},{"location":"user-guide/credit/scorecard/","title":"Credit Scorecard","text":"<p>The <code>create_scorecard</code> function implements a points-based credit scoring system, which is one of the most widely used approaches in the credit industry. This method assigns points to various borrower characteristics based on their predictive power for credit risk.</p>"},{"location":"user-guide/credit/scorecard/#what-is-a-credit-scorecard","title":"What is a Credit Scorecard?","text":"<p>A credit scorecard is a statistical model that: - Assigns points to different borrower characteristics - Combines these points into a total score - Categorizes applicants into risk segments based on their score - Provides transparency and interpretability in credit decisions</p> <p>Where: - \\(x_i\\) is the value of feature \\(i\\) - \\(\\text{offset}_i\\) is the reference point for feature \\(i\\) - \\(\\text{weight}_i\\) is the importance of feature \\(i\\) - \\(\\text{scaling factor}\\) adjusts the point scale</p>"},{"location":"user-guide/credit/scorecard/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.credit import create_scorecard\n\n# Define applicant features\nfeatures = {\n    \"age\": 35,\n    \"income\": 75000,\n    \"credit_history\": 0.8,\n    \"debt_ratio\": 0.3,\n    \"payment_history\": 0.95\n}\n\n# Define feature weights (importance)\nweights = {\n    \"age\": 0.5,\n    \"income\": 0.3,\n    \"credit_history\": 2.0,\n    \"debt_ratio\": -1.5,\n    \"payment_history\": 1.8\n}\n\n# Define offsets (reference points)\noffsets = {\n    \"age\": 25,\n    \"income\": 50000,\n    \"credit_history\": 0.5,\n    \"debt_ratio\": 0.4,\n    \"payment_history\": 0.7\n}\n\n# Create the scorecard\nresult = create_scorecard(\n    features=features,\n    weights=weights,\n    offsets=offsets,\n    scaling_factor=100.0,\n    base_score=600\n)\n\n# Access the results\ntotal_score = result[\"total_score\"]\nrisk_category = result[\"risk_category\"]\npoints_breakdown = result[\"points_breakdown\"]\nthresholds = result[\"thresholds\"]  # Dynamic thresholds based on scaling factor\n</code></pre>"},{"location":"user-guide/credit/scorecard/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>features</code> dict Dictionary of feature names and values Required <code>weights</code> dict Dictionary of feature names and weights Required <code>offsets</code> dict Dictionary of feature names and reference points {} <code>scaling_factor</code> float Controls the range of points 100.0 <code>base_score</code> float Starting point for the score calculation 600"},{"location":"user-guide/credit/scorecard/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>total_score</code> float The calculated credit score <code>risk_category</code> str Categorization based on the score <code>points_breakdown</code> dict Dictionary showing points contributed by each feature <code>thresholds</code> dict Dictionary of the adjusted thresholds used for risk categorization <p>The <code>points_breakdown</code> dictionary includes: - One key for each feature in the input <code>features</code> dictionary - The value is the points contributed by that feature to the total score</p> <p>The <code>thresholds</code> dictionary includes: - <code>excellent</code>: Threshold for \"Excellent\" risk category - <code>good</code>: Threshold for \"Good\" risk category - <code>fair</code>: Threshold for \"Fair\" risk category - <code>poor</code>: Threshold for \"Poor\" risk category</p>"},{"location":"user-guide/credit/scorecard/#risk-level-classification","title":"Risk Level Classification","text":"<p>The credit score is categorized into risk levels (using reference thresholds for scaling_factor=100.0):</p> Credit Score Range Risk Level \u2265 750 Excellent 700 - 749 Good 650 - 699 Fair 600 - 649 Poor &lt; 600 Very Poor <p>The thresholds are dynamically adjusted based on the scaling factor:</p> <ul> <li>Reference thresholds (for scaling_factor=100.0):</li> <li>Excellent: 750</li> <li>Good: 700</li> <li>Fair: 650</li> <li> <p>Poor: 600</p> </li> <li> <p>For different scaling factors, the thresholds are adjusted using:   <pre><code>adjusted_threshold = base_score + (reference_threshold - base_score) * (reference_scaling / scaling_factor)\n</code></pre></p> </li> </ul> <p>This formula ensures that: - With a smaller scaling factor (e.g., 20.0), thresholds are spread further apart from the base score - With a larger scaling factor (e.g., 500.0), thresholds are compressed closer to the base score</p> <p>This dynamic adjustment ensures that risk categories remain meaningful regardless of the scaling factor used.</p>"},{"location":"user-guide/credit/scorecard/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to create and use a credit scorecard for multiple applicants with different scaling factors:</p> <pre><code>from pypulate.credit import create_scorecard\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define applicant profiles\nweak_applicant = {\n    \"age\": 20,                  # Young, less stability\n    \"income\": 30000,            # Low income\n    \"credit_history\": 0.3,      # Poor credit record\n    \"debt_ratio\": 0.6,          # High debt\n    \"payment_history\": 0.5      # Frequent late payments\n}\n\nfair_applicant = {\n    \"age\": 40,                  # Mature, more stability\n    \"income\": 70000,            # Above-average income\n    \"credit_history\": 0.75,     # Decent credit record\n    \"debt_ratio\": 0.3,          # Below-average debt\n    \"payment_history\": 0.85     # Mostly on-time payments\n}\n\ngood_applicant = {\n    \"age\": 45,                  # Older, stable\n    \"income\": 90000,            # High income\n    \"credit_history\": 0.9,      # Excellent credit record\n    \"debt_ratio\": 0.15,         # Low debt\n    \"payment_history\": 0.95     # Nearly perfect payment record\n}\n\n# Define feature weights (importance)\nweights = {\n    \"age\": 0.5,                 # Moderate positive impact\n    \"income\": 0.3,              # Moderate positive impact\n    \"credit_history\": 2.0,      # Strong positive impact\n    \"debt_ratio\": -1.5,         # Strong negative impact\n    \"payment_history\": 1.8      # Strong positive impact\n}\n\n# Define offsets (reference points)\noffsets = {\n    \"age\": 25,                  # Reference age\n    \"income\": 50000,            # Reference income\n    \"credit_history\": 0.5,      # Reference credit history\n    \"debt_ratio\": 0.4,          # Reference debt ratio\n    \"payment_history\": 0.7      # Reference payment history\n}\n\n# Create scorecards with different scaling factors\nscaling_factors = [20.0, 100.0, 500.0]\nresults = {}\n\nfor scaling in scaling_factors:\n    results[scaling] = {\n        \"weak\": create_scorecard(\n            features=weak_applicant,\n            weights=weights,\n            offsets=offsets,\n            scaling_factor=scaling,\n            base_score=600\n        ),\n        \"fair\": create_scorecard(\n            features=fair_applicant,\n            weights=weights,\n            offsets=offsets,\n            scaling_factor=scaling,\n            base_score=600\n        ),\n        \"good\": create_scorecard(\n            features=good_applicant,\n            weights=weights,\n            offsets=offsets,\n            scaling_factor=scaling,\n            base_score=600\n        )\n    }\n\n# Print the results for the standard scaling factor (100.0)\nprint(\"Credit Scorecard Results (scaling_factor=100.0)\")\nprint(\"==============================================\")\n\nprint(\"\\nExample 1: Weak Applicant\")\nprint(f\"Total Score: {results[100.0]['weak']['total_score']:.2f}\")\nprint(f\"Risk Category: {results[100.0]['weak']['risk_category']}\")\nprint(\"Points Breakdown:\")\nfor feature, points in results[100.0]['weak']['points_breakdown'].items():\n    print(f\"  {feature}: {points:.2f} points\")\n\nprint(\"\\nExample 2: Fair Applicant\")\nprint(f\"Total Score: {results[100.0]['fair']['total_score']:.2f}\")\nprint(f\"Risk Category: {results[100.0]['fair']['risk_category']}\")\nprint(\"Points Breakdown:\")\nfor feature, points in results[100.0]['fair']['points_breakdown'].items():\n    print(f\"  {feature}: {points:.2f} points\")\n\nprint(\"\\nExample 3: Good Applicant\")\nprint(f\"Total Score: {results[100.0]['good']['total_score']:.2f}\")\nprint(f\"Risk Category: {results[100.0]['good']['risk_category']}\")\nprint(\"Points Breakdown:\")\nfor feature, points in results[100.0]['good']['points_breakdown'].items():\n    print(f\"  {feature}: {points:.2f} points\")\n\n# Print the thresholds for each scaling factor\nprint(\"\\nRisk Category Thresholds\")\nprint(\"=======================\")\nfor scaling in scaling_factors:\n    thresholds = results[scaling]['weak']['thresholds']\n    print(f\"\\nScaling Factor: {scaling}\")\n    for category, threshold in thresholds.items():\n        print(f\"  {category}: {threshold:.2f}\")\n\n# Visualize the results with different scaling factors\nplt.figure(figsize=(15, 8))\n\n# Use the original order of scaling factors\nscaling_factors = [20.0, 100.0, 500.0]\n\n# Create subplots for each scaling factor\nfor i, scaling in enumerate(scaling_factors):\n    plt.subplot(1, 3, i+1)\n\n    applicants = ['Weak', 'Fair', 'Good']\n    scores = [\n        results[scaling]['weak']['total_score'],\n        results[scaling]['fair']['total_score'],\n        results[scaling]['good']['total_score']\n    ]\n\n    # Create a bar chart\n    bars = plt.bar(applicants, scores, color=['red', 'orange', 'green'])\n\n    # Calculate the correct thresholds based on the scaling factor\n    reference_scaling = 100.0\n    reference_thresholds = {\n        \"Excellent\": 750,\n        \"Good\": 700,\n        \"Fair\": 650,\n        \"Poor\": 600\n    }\n\n    # Calculate adjustment factor and thresholds\n    adjustment_factor = reference_scaling / scaling\n    thresholds = {}\n    for category in reference_thresholds:\n        # Correct formula: multiply by adjustment factor instead of dividing\n        thresholds[category] = 600 + (reference_thresholds[category] - 600) * adjustment_factor\n\n    # Calculate the correct y-axis limits for this specific scaling factor\n    min_score = min(min(scores) * 0.9, thresholds['Poor'] * 0.9)  # 10% below the minimum score or threshold\n    max_score = max(max(scores) * 1.1, thresholds['Excellent'] * 1.1)  # 10% above the maximum score or threshold\n\n    # Set y-axis limits\n    plt.ylim(bottom=min_score, top=max_score)\n\n    # Add horizontal lines for the thresholds in the correct order\n    plt.axhline(y=thresholds['Poor'], color='r', linestyle='--', label='Poor')\n    plt.axhline(y=thresholds['Fair'], color='orange', linestyle='--', label='Fair')\n    plt.axhline(y=thresholds['Good'], color='y', linestyle='--', label='Good')\n    plt.axhline(y=thresholds['Excellent'], color='g', linestyle='--', label='Excellent')\n\n    # Add labels and title\n    plt.ylabel('Credit Score')\n    plt.title(f'Scaling Factor: {scaling}')\n\n    # Add the score values on top of the bars\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height + (max_score - min_score) * 0.02,\n                 f'{height:.1f}', ha='center', va='bottom', fontsize=8)\n\n    # Add legend to the first subplot only\n    if i == 0:\n        plt.legend(loc='upper left')\n\nplt.suptitle('Credit Score Comparison with Different Scaling Factors', fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n# Create a breakdown chart for points per feature with scaling_factor=100.0\nfeatures = list(results[100.0]['weak']['points_breakdown'].keys())\nweak_points = [results[100.0]['weak']['points_breakdown'][f] for f in features]\nfair_points = [results[100.0]['fair']['points_breakdown'][f] for f in features]\ngood_points = [results[100.0]['good']['points_breakdown'][f] for f in features]\n\nplt.figure(figsize=(12, 6))\nx = np.arange(len(features))\nwidth = 0.25\n\nplt.bar(x - width, weak_points, width, label='Weak Applicant', color='red')\nplt.bar(x, fair_points, width, label='Fair Applicant', color='orange')\nplt.bar(x + width, good_points, width, label='Good Applicant', color='green')\n\nplt.ylabel('Points')\nplt.title('Scorecard Points Breakdown by Feature (scaling_factor=100.0)')\nplt.xticks(x, features)\nplt.legend()\nplt.grid(axis='y')\nplt.tight_layout()\nplt.show()\n\n# Create a visualization showing how thresholds change with different scaling factors\nplt.figure(figsize=(10, 6))\n\n# Extract thresholds for each scaling factor\nscaling_values = np.linspace(10, 500, 100)  # Range of scaling factors from 10 to 500\nthreshold_values = {\n    'Excellent': [],\n    'Good': [],\n    'Fair': [],\n    'Poor': []\n}\n\n# Calculate thresholds for each scaling factor\nfor scaling in scaling_values:\n    # Use the same formula as in the create_scorecard function\n    reference_scaling = 100.0\n    reference_thresholds = {\n        \"Excellent\": 750,\n        \"Good\": 700,\n        \"Fair\": 650,\n        \"Poor\": 600\n    }\n    adjustment_factor = reference_scaling / scaling\n    for category in threshold_values.keys():\n        # Correct formula: multiply by adjustment factor instead of dividing\n        threshold = 600 + (reference_thresholds[category] - 600) * adjustment_factor\n        threshold_values[category].append(threshold)\n\n# Plot the threshold curves\nplt.plot(scaling_values, threshold_values['Excellent'], 'g-', label='Excellent')\nplt.plot(scaling_values, threshold_values['Good'], 'y-', label='Good')\nplt.plot(scaling_values, threshold_values['Fair'], 'orange', label='Fair')\nplt.plot(scaling_values, threshold_values['Poor'], 'r-', label='Poor')\n\n# Add markers for the specific scaling factors used in the example\nfor scaling in scaling_factors:\n    # Calculate thresholds for this scaling factor\n    adjustment_factor = reference_scaling / scaling\n    for category, color in zip(['Excellent', 'Good', 'Fair', 'Poor'], ['g', 'y', 'orange', 'r']):\n        threshold = 600 + (reference_thresholds[category] - 600) * adjustment_factor\n        plt.plot(scaling, threshold, 'o', color=color, markersize=8)\n\n# Add labels and title\nplt.xlabel('Scaling Factor')\nplt.ylabel('Threshold Value')\nplt.title('How Risk Category Thresholds Change with Scaling Factor')\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/credit/scorecard/#example-output","title":"Example Output","text":"<pre><code>Credit Scorecard Results (scaling_factor=100.0)\n==============================================\nExample 1: Weak Applicant\nTotal Score: 539.96\nRisk Category: Very Poor\nPoints Breakdown:\n  age: -0.03 points\n  income: -60.00 points\n  credit_history: -0.00 points\n  debt_ratio: -0.00 points\n  payment_history: -0.00 points\nExample 2: Fair Applicant\nTotal Score: 660.08\nRisk Category: Fair\nPoints Breakdown:\n  age: 0.07 points\n  income: 60.00 points\n  credit_history: 0.01 points\n  debt_ratio: 0.00 points\n  payment_history: 0.00 points\nExample 3: Good Applicant\nTotal Score: 720.12\nRisk Category: Good\nPoints Breakdown:\n  age: 0.10 points\n  income: 120.00 points\n  credit_history: 0.01 points\n  debt_ratio: 0.00 points\n  payment_history: 0.00 points\nRisk Category Thresholds\n=======================\nScaling Factor: 20.0\n  Excellent: 630.00\n  Good: 620.00\n  Fair: 610.00\n  Poor: 600.00\nScaling Factor: 100.0\n  Excellent: 750.00\n  Good: 700.00\n  Fair: 650.00\n  Poor: 600.00\nScaling Factor: 500.0\n  Excellent: 1350.00\n  Good: 1100.00\n  Fair: 850.00\n  Poor: 600.00\n</code></pre>"},{"location":"user-guide/credit/scorecard/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/credit/scorecard/#credit-score-comparison-with-different-scaling-factors","title":"Credit Score Comparison with Different Scaling Factors","text":"<p>This visualization shows how the same applicant profiles result in different scores based on the scaling factor used, while still maintaining the same risk categorization due to the dynamic thresholds.</p> <p></p>"},{"location":"user-guide/credit/scorecard/#points-breakdown-by-feature","title":"Points Breakdown by Feature","text":"<p>This visualization shows how each feature contributes to the total score for each applicant, highlighting the strengths and weaknesses of each profile.</p>"},{"location":"user-guide/credit/scorecard/#threshold-scaling-relationship","title":"Threshold Scaling Relationship","text":"<p>This visualization demonstrates how the risk category thresholds change as the scaling factor changes, showing the inverse relationship between scaling factor and threshold spread.</p> <p></p>"},{"location":"user-guide/credit/scorecard/#how-points-are-calculated","title":"How Points Are Calculated","text":"<p>Let's break down the calculation for the fair applicant with scaling_factor=100.0:</p> <ol> <li> <p>Age: (40 - 25) \u00d7 0.5 / 100.0 = 0.075 points</p> <ul> <li>15 years above reference age</li> <li>Moderate positive weight (0.5)</li> <li>Divided by scaling factor (100.0)</li> <li>Results in 0.075 additional points</li> </ul> </li> <li> <p>Income: (70000 - 50000) \u00d7 0.3 / 100.0 = 60.0 points</p> <ul> <li>$20,000 above reference income</li> <li>Moderate positive weight (0.3)</li> <li>Divided by scaling factor (100.0)</li> <li>Results in 60.0 additional points</li> </ul> </li> <li> <p>Credit History: (0.75 - 0.5) \u00d7 2.0 / 100.0 = 0.5 points</p> <ul> <li>0.25 above reference credit history</li> <li>Strong positive weight (2.0)</li> <li>Divided by scaling factor (100.0)</li> <li>Results in 0.5 additional points</li> </ul> </li> <li> <p>Debt Ratio: (0.3 - 0.4) \u00d7 -1.5 / 100.0 = 0.15 points</p> <ul> <li>0.1 below reference debt ratio</li> <li>Strong negative weight (-1.5)</li> <li>Divided by scaling factor (100.0)</li> <li>Being below reference with negative weight results in positive points</li> <li>Results in 0.15 additional points</li> </ul> </li> <li> <p>Payment History: (0.85 - 0.7) \u00d7 1.8 / 100.0 = 0.27 points</p> <ul> <li>0.15 above reference payment history</li> <li>Strong positive weight (1.8)</li> <li>Divided by scaling factor (100.0)</li> <li>Results in 0.27 additional points</li> </ul> </li> <li> <p>Base Score: 600 points</p> </li> <li> <p>Total Score: 600 + 0.075 + 60.0 + 0.5 + 0.15 + 0.27 = 660.995 points</p> </li> <li> <p>Risk Category: Fair (between 650 and 700)</p> <ul> <li>The thresholds are dynamically adjusted based on the scaling factor</li> <li>For scaling_factor=100.0, the thresholds are:<ul> <li>Excellent: 750</li> <li>Good: 700</li> <li>Fair: 650</li> <li>Poor: 600</li> </ul> </li> </ul> </li> </ol>"},{"location":"user-guide/credit/scorecard/#impact-of-scaling-factor","title":"Impact of Scaling Factor","text":"<p>The scaling factor significantly impacts the range of scores:</p> <ol> <li> <p>Small Scaling Factor (e.g., 20.0):</p> <ul> <li>Creates wider score ranges</li> <li>Points have more impact on the total score</li> <li>Thresholds are spread further from the base score</li> </ul> </li> <li> <p>Large Scaling Factor (e.g., 500.0):</p> <ul> <li>Creates narrower score ranges</li> <li>Points have less impact on the total score</li> <li>Thresholds are compressed closer to the base score</li> </ul> </li> <li> <p>Standard Scaling Factor (100.0):</p> <ul> <li>Provides a balanced approach</li> <li>Points have moderate impact on the total score</li> <li>Thresholds are at standard intervals from the base score</li> </ul> </li> </ol> <p>The dynamic threshold adjustment ensures that risk categories remain meaningful regardless of the scaling factor used.</p>"},{"location":"user-guide/credit/scorecard/#benefits-of-the-scorecard-approach","title":"Benefits of the Scorecard Approach","text":"<ol> <li>Transparency: Clear relationship between applicant characteristics and score</li> <li>Customizability: Weights and offsets can be adjusted based on business needs</li> <li>Interpretability: Easy to explain why an applicant received a particular score</li> <li>Consistency: Standardized approach to evaluating creditworthiness</li> <li>Flexibility: Can be applied to various lending contexts (mortgages, auto loans, credit cards)</li> </ol>"},{"location":"user-guide/credit/scorecard/#practical-applications","title":"Practical Applications","text":"<p>The scorecard approach can be used for:</p> <ol> <li>Consumer Lending: Evaluating loan applications</li> <li>Credit Card Approvals: Determining credit limits and interest rates</li> <li>Mortgage Underwriting: Assessing mortgage applicants</li> <li>Small Business Lending: Evaluating business loan applications</li> <li>Tenant Screening: Assessing potential renters</li> </ol>"},{"location":"user-guide/credit/scorecard/#best-practices","title":"Best Practices","text":"<ol> <li>Feature Selection: Choose features with strong predictive power</li> <li>Weight Calibration: Derive weights from statistical analysis of historical data</li> <li>Offset Selection: Set offsets based on population averages or policy considerations</li> <li>Regular Validation: Periodically validate the scorecard against actual outcomes</li> <li>Compliance Checks: Ensure the scorecard complies with fair lending regulations</li> </ol>"},{"location":"user-guide/credit/scoring_model_validation/","title":"Scoring Model Validation","text":"<p>The <code>scoring_model_validation</code> function provides a comprehensive set of metrics and visualizations to evaluate the performance of credit scoring models. This is a critical step in model development to ensure that your scoring system effectively discriminates between good and bad customers.</p>"},{"location":"user-guide/credit/scoring_model_validation/#purpose","title":"Purpose","text":"<p>Credit scoring model validation serves several key purposes:</p> <ol> <li>Assessing the model's discriminatory power</li> <li>Measuring the separation between good and bad customers</li> <li>Evaluating the predictive strength of the model</li> <li>Analyzing the relationship between scores and default rates</li> <li>Providing insights for setting appropriate cutoff thresholds</li> </ol>"},{"location":"user-guide/credit/scoring_model_validation/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.credit import scoring_model_validation\n\n# Validate a scoring model\nvalidation_results = scoring_model_validation(\n    predicted_scores=scores_array,     # Array of predicted scores\n    actual_defaults=defaults_array,    # Array of actual default outcomes (0/1)\n    score_bins=10                      # Number of score bins for analysis\n)\n\n# Access validation metrics\nauc = validation_results['auc']\ngini = validation_results['gini']\nks_statistic = validation_results['ks_statistic']\ninformation_value = validation_results['information_value']\nconcordance = validation_results['concordance']\n\n# Access ROC curve data\nfpr = validation_results['roc_curve']['fpr']\ntpr = validation_results['roc_curve']['tpr']\nthresholds = validation_results['roc_curve']['thresholds']\n\n# Access bin analysis\nbin_analysis = validation_results['bin_analysis']\n</code></pre>"},{"location":"user-guide/credit/scoring_model_validation/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>predicted_scores</code> array_like Array of predicted scores or probabilities Required <code>actual_defaults</code> array_like Array of actual default outcomes (0/1) Required <code>score_bins</code> int Number of score bins for analysis 10"},{"location":"user-guide/credit/scoring_model_validation/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>auc</code> float Area Under the ROC Curve <code>gini</code> float Gini coefficient (2*AUC - 1) <code>ks_statistic</code> float Kolmogorov-Smirnov statistic <code>information_value</code> float Information Value (IV) <code>concordance</code> float Concordance rate <code>roc_curve</code> dict Dictionary containing ROC curve data <code>bin_analysis</code> list List of dictionaries with bin-level statistics <p>The <code>roc_curve</code> dictionary includes: - <code>fpr</code>: False positive rates - <code>tpr</code>: True positive rates - <code>thresholds</code>: Threshold values</p> <p>The <code>bin_analysis</code> list contains dictionaries with the following keys for each bin: - <code>bin_number</code>: Bin number - <code>min_score</code>: Minimum score in the bin - <code>max_score</code>: Maximum score in the bin - <code>count</code>: Number of observations in the bin - <code>default_rate</code>: Default rate in the bin - <code>cumulative_good</code>: Cumulative percentage of good customers - <code>cumulative_bad</code>: Cumulative percentage of bad customers - <code>ks</code>: KS statistic at this bin</p>"},{"location":"user-guide/credit/scoring_model_validation/#risk-level-classification","title":"Risk Level Classification","text":"<p>The key validation metrics are categorized into performance levels:</p> Metric Range Performance Level AUC 0.5-0.6 Poor 0.6-0.7 Fair 0.7-0.8 Good 0.8-0.9 Very Good 0.9-1.0 Excellent Gini 0.0-0.2 Poor 0.2-0.4 Fair 0.4-0.6 Good 0.6-0.8 Very Good 0.8-1.0 Excellent KS Statistic 0.0-0.2 Poor 0.2-0.3 Fair 0.3-0.4 Good 0.4-0.5 Very Good &gt;0.5 Excellent Information Value &lt;0.02 Not Predictive 0.02-0.1 Weak 0.1-0.3 Medium 0.3-0.5 Strong &gt;0.5 Very Strong"},{"location":"user-guide/credit/scoring_model_validation/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to validate a credit scoring model:</p> <pre><code>from pypulate.credit import scoring_model_validation\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate synthetic data for demonstration\nnp.random.seed(42)\n\n# Sample size\nn_samples = 1000\n\n# Generate synthetic credit scores (higher score = better credit)\n# Good customers tend to have higher scores, but with more overlap with bad customers\ngood_scores = np.random.normal(650, 120, 800)\ngood_scores = np.clip(good_scores, 300, 850)  # Clip to typical credit score range\n\n# Bad customers tend to have lower scores, but with more overlap with good customers\nbad_scores = np.random.normal(580, 100, 200)\nbad_scores = np.clip(bad_scores, 300, 850)\n\n# Combine scores and create actual default labels (0 = good, 1 = bad)\npredicted_scores = np.concatenate([good_scores, bad_scores])\nactual_defaults = np.concatenate([np.zeros(800), np.ones(200)])\n\n# Shuffle the data\nindices = np.arange(n_samples)\nnp.random.shuffle(indices)\npredicted_scores = predicted_scores[indices]\nactual_defaults = actual_defaults[indices]\n\n# Validate the scoring model\nvalidation_results = scoring_model_validation(\n    predicted_scores=predicted_scores,\n    actual_defaults=actual_defaults,\n    score_bins=10  # Divide scores into 10 bins for analysis\n)\n\n# Print the validation metrics\nprint(\"Model Validation Results:\")\nprint(f\"AUC: {validation_results['auc']:.4f}\")\nprint(f\"Gini Coefficient: {validation_results['gini']:.4f}\")\nprint(f\"KS Statistic: {validation_results['ks_statistic']:.4f}\")\nprint(f\"Information Value: {validation_results['information_value']:.4f}\")\nprint(f\"Concordance: {validation_results['concordance']:.4f}\")\n\n# Print bin analysis\nprint(\"\\nBin Analysis:\")\nprint(\"Bin\\tScore Range\\t\\tCount\\tDefault Rate\")\nprint(\"-\" * 50)\nfor bin_info in validation_results['bin_analysis']:\n    print(f\"{bin_info['bin']}\\t{bin_info['min_score']:.0f}-{bin_info['max_score']:.0f}\\t\\t{bin_info['count']}\\t{bin_info['default_rate']:.2%}\")\n\n# Plot ROC curve\nplt.figure(figsize=(10, 6))\nplt.plot(validation_results['roc_curve']['fpr'], \n         validation_results['roc_curve']['tpr'], \n         label=f\"AUC = {validation_results['auc']:.4f}\")\nplt.plot([0, 1], [0, 1], 'k--', label='Random Model')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot default rates by score bin\nbins = [b['bin'] for b in validation_results['bin_analysis']]\ndefault_rates = [b['default_rate'] for b in validation_results['bin_analysis']]\n\nplt.figure(figsize=(10, 6))\nplt.bar(bins, default_rates)\nplt.xlabel('Score Bin (Higher = Better Score)')\nplt.ylabel('Default Rate')\nplt.title('Default Rate by Score Bin')\nplt.xticks(bins)\nplt.grid(axis='y')\nplt.show()\n</code></pre>"},{"location":"user-guide/credit/scoring_model_validation/#example-output","title":"Example Output","text":"<pre><code>Model Validation Results:\nAUC: -0.3565\nGini Coefficient: -1.7129\nKS Statistic: 0.2450\nInformation Value: 0.3722\nConcordance: 0.5805\nBin Analysis:\nBin Score Range     Count   Default Rate\n--------------------------------------------------\n1   300-492     100 27.00%\n2   492-536     100 37.00%\n3   536-574     100 22.00%\n4   574-604     100 28.00%\n5   604-638     100 22.00%\n6   638-664     100 19.00%\n7   664-696     100 12.00%\n8   696-732     100 16.00%\n9   732-782     100 13.00%\n10  782-850     100 4.00%\n</code></pre>"},{"location":"user-guide/credit/scoring_model_validation/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/credit/scoring_model_validation/#roc-curve","title":"ROC Curve","text":"<p>The ROC (Receiver Operating Characteristic) curve plots the True Positive Rate against the False Positive Rate at various threshold settings. The area under this curve (AUC) is a measure of the model's discriminatory power.</p>"},{"location":"user-guide/credit/scoring_model_validation/#default-rate-by-score-bin","title":"Default Rate by Score Bin","text":"<p>This visualization shows how default rates decrease as credit scores increase, which is a key indicator of a well-calibrated scoring model.</p> <p></p>"},{"location":"user-guide/credit/scoring_model_validation/#interpreting-the-results","title":"Interpreting the Results","text":"<p>In the example above:</p> <ol> <li> <p>AUC = 0.6845: The model has fair discriminatory power, correctly ranking good and bad customers 68.45% of the time.</p> </li> <li> <p>Gini = 0.3690: This indicates moderate predictive ability, typical of many real-world credit scoring models.</p> </li> <li> <p>KS Statistic = 0.2812: There is fair separation between the distributions of good and bad customers.</p> </li> <li> <p>Information Value = 0.4237: The model has strong predictive power, which is realistic for a credit scoring model in practice.</p> </li> <li> <p>Bin Analysis: Default rates decrease monotonically as scores increase, from 38% in the lowest bin to 2% in the highest bin. This indicates a well-calibrated model with a realistic gradient of risk.</p> </li> </ol>"},{"location":"user-guide/credit/scoring_model_validation/#business-applications","title":"Business Applications","text":"<p>The validation results can be used to:</p> <ol> <li> <p>Set Approval Thresholds: Based on the default rates in each bin, you can set appropriate approval, review, and rejection thresholds.</p> </li> <li> <p>Risk-Based Pricing: Assign different interest rates or terms based on score bins.</p> </li> <li> <p>Portfolio Segmentation: Divide your portfolio into risk segments for targeted management strategies.</p> </li> <li> <p>Model Refinement: Identify areas where the model could be improved.</p> </li> <li> <p>Regulatory Compliance: Demonstrate the statistical validity of your scoring model to regulators.</p> </li> </ol>"},{"location":"user-guide/credit/scoring_model_validation/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Split Sample Validation: Always validate on a holdout sample not used in model development.</p> </li> <li> <p>Regular Revalidation: Periodically revalidate your model to ensure it remains effective over time.</p> </li> <li> <p>Compare Multiple Models: Use these metrics to compare different scoring models.</p> </li> <li> <p>Consider Population Stability: Ensure the validation sample is representative of your target population.</p> </li> <li> <p>Look Beyond the Numbers: While these metrics are important, also consider business context and practical implications.</p> </li> </ol>"},{"location":"user-guide/credit/transition_matrix/","title":"Credit Rating Transition Matrix","text":"<p>The <code>transition_matrix</code> function calculates a credit rating transition matrix, which shows the probability of credit ratings migrating from one level to another over a specified time period. This is a fundamental tool in credit risk management for understanding rating stability and modeling future rating changes.</p>"},{"location":"user-guide/credit/transition_matrix/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.credit import transition_matrix\n\n# Calculate transition matrix from historical rating data\nresult = transition_matrix(\n    ratings_t0=['AAA', 'AA', 'A', 'BBB', 'BB', 'B', 'CCC'],  # Ratings at time 0\n    ratings_t1=['AA', 'A', 'BBB', 'BB', 'B', 'CCC', 'D']     # Ratings at time 1\n)\n\n# Access the results\nprob_matrix = result[\"probability_matrix\"]\ntrans_matrix = result[\"transition_matrix\"]\nratings = result[\"ratings\"]\n</code></pre>"},{"location":"user-guide/credit/transition_matrix/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>ratings_t0</code> array_like Array of credit ratings at the beginning of the period Required <code>ratings_t1</code> array_like Array of credit ratings at the end of the period Required"},{"location":"user-guide/credit/transition_matrix/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>transition_matrix</code> list of lists Count of transitions from each rating to each other rating <code>probability_matrix</code> list of lists Probability of transitioning from each rating to each other rating <code>ratings</code> list Unique ratings found in the input data"},{"location":"user-guide/credit/transition_matrix/#risk-level-classification","title":"Risk Level Classification","text":"<p>Credit ratings are typically categorized into risk levels:</p> Rating Category Risk Level AAA, AA Investment Grade - Very Low Risk A, BBB Investment Grade - Low to Moderate Risk BB, B Non-Investment Grade - Moderate to High Risk CCC, CC, C Non-Investment Grade - Very High Risk D Default"},{"location":"user-guide/credit/transition_matrix/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to calculate and analyze credit rating transitions:</p> <pre><code>from pypulate.credit import transition_matrix\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import Counter\n\n# Sample historical rating data\n# Let's create a dataset with 100 companies and their ratings over two periods\nnp.random.seed(42)  # For reproducibility\n\n# Define possible ratings\nratings = ['AAA', 'AA', 'A', 'BBB', 'BB', 'B', 'CCC', 'D']\n\n# Generate initial ratings with a distribution skewed toward investment grade\nweights = [0.05, 0.15, 0.25, 0.25, 0.15, 0.10, 0.05, 0.00]  # No defaults at t0\ninitial_ratings = np.random.choice(ratings, size=100, p=weights)\n\n# Create a simple model for rating transitions\n# Higher ratings are more stable, lower ratings have higher probability of downgrade\ndef generate_next_rating(current_rating):\n    current_idx = ratings.index(current_rating)\n\n    if current_rating == 'D':\n        return 'D'  # Default is an absorbing state\n\n    # Probability of staying at the same rating\n    stay_prob = 0.7 - 0.05 * current_idx  # Higher ratings are more stable\n\n    # Probability of upgrading (less likely for higher ratings)\n    if current_idx == 0:\n        upgrade_prob = 0  # AAA can't be upgraded\n    else:\n        upgrade_prob = 0.05 * (8 - current_idx) / 7  # More room for upgrade at lower ratings\n\n    # Probability of downgrading (more likely for lower ratings)\n    downgrade_prob = 1 - stay_prob - upgrade_prob\n\n    # Determine direction of movement\n    r = np.random.random()\n    if r &lt; stay_prob:\n        return current_rating\n    elif r &lt; stay_prob + upgrade_prob:\n        return ratings[current_idx - 1]  # Upgrade\n    else:\n        # For downgrades, allow for multi-notch downgrades for lower ratings\n        if current_idx &gt;= 5:  # B or lower\n            # Possible to skip directly to default\n            possible_downgrades = ratings[current_idx+1:]\n            return np.random.choice(possible_downgrades)\n        else:\n            return ratings[current_idx + 1]  # Single notch downgrade\n\n# Generate ratings at time 1\nfinal_ratings = [generate_next_rating(rating) for rating in initial_ratings]\n\n# Calculate the transition matrix\nresult = transition_matrix(initial_ratings, final_ratings)\n\n# Print the transition probability matrix\nprint(\"Credit Rating Transition Matrix (Probabilities)\")\nprint(\"==============================================\")\nprob_matrix = result['probability_matrix']\nunique_ratings = result['ratings']\n\n# Print the matrix with proper formatting\nprint(f\"{'':5}\", end=\"\")\nfor r in unique_ratings:\n    print(f\"{r:6}\", end=\"\")\nprint()\n\nfor i, row in enumerate(prob_matrix):\n    print(f\"{unique_ratings[i]:5}\", end=\"\")\n    for val in row:\n        print(f\"{val:.2f}  \", end=\"\")\n    print()\n\n# Calculate some statistics\ndowngrades = sum(1 for i, j in zip(initial_ratings, final_ratings) \n                if ratings.index(j) &gt; ratings.index(i))\nupgrades = sum(1 for i, j in zip(initial_ratings, final_ratings) \n              if ratings.index(j) &lt; ratings.index(i))\nsame = sum(1 for i, j in zip(initial_ratings, final_ratings) if i == j)\ndefaults = sum(1 for rating in final_ratings if rating == 'D')\n\nprint(\"\\nTransition Statistics:\")\nprint(f\"Total Entities: {len(initial_ratings)}\")\nprint(f\"Upgrades: {upgrades} ({upgrades/len(initial_ratings):.1%})\")\nprint(f\"Downgrades: {downgrades} ({downgrades/len(initial_ratings):.1%})\")\nprint(f\"Unchanged: {same} ({same/len(initial_ratings):.1%})\")\nprint(f\"Defaults: {defaults} ({defaults/len(initial_ratings):.1%})\")\n\n# Visualize the transition matrix as a heatmap using matplotlib\nplt.figure(figsize=(10, 8))\nplt.imshow(prob_matrix, cmap='YlGnBu', aspect='equal')\nplt.colorbar(label='Transition Probability')\n\n# Add text annotations to the heatmap\nfor i in range(len(unique_ratings)):\n    for j in range(len(unique_ratings)):\n        plt.text(j, i, f\"{prob_matrix[i][j]:.2f}\", \n                 ha=\"center\", va=\"center\", \n                 color=\"black\" if prob_matrix[i][j] &lt; 0.7 else \"white\")\n\n# Set ticks and labels\nplt.xticks(range(len(unique_ratings)), unique_ratings)\nplt.yticks(range(len(unique_ratings)), unique_ratings)\nplt.xlabel('Rating at Time 1')\nplt.ylabel('Rating at Time 0')\nplt.title('Credit Rating Transition Matrix')\nplt.tight_layout()\nplt.show()\n\n# Visualize rating distribution before and after\nplt.figure(figsize=(12, 6))\n\n# Count ratings at each time period\nt0_counts = {r: 0 for r in ratings}\nt1_counts = {r: 0 for r in ratings}\n\n# Update with actual counts\nfor r in initial_ratings:\n    t0_counts[r] += 1\nfor r in final_ratings:\n    t1_counts[r] += 1\n\n# Create bar chart\nx = np.arange(len(ratings))\nwidth = 0.35\n\nfig, ax = plt.subplots(figsize=(12, 6))\nax.bar(x - width/2, [t0_counts[r] for r in ratings], width, label='Time 0')\nax.bar(x + width/2, [t1_counts[r] for r in ratings], width, label='Time 1')\n\n# Add labels and legend\nax.set_xlabel('Credit Rating')\nax.set_ylabel('Number of Entities')\nax.set_title('Rating Distribution: Before vs After')\nax.set_xticks(x)\nax.set_xticklabels(ratings)\nax.legend()\nax.grid(axis='y', linestyle='--', alpha=0.7)\n\nplt.tight_layout()\nplt.show()\n\n# Calculate and visualize the default rates by initial rating\ndefault_rates = []\nfor rating in ratings[:-1]:  # Exclude 'D' as initial rating\n    entities_with_rating = sum(1 for r in initial_ratings if r == rating)\n    if entities_with_rating &gt; 0:\n        defaults_from_rating = sum(1 for i, j in zip(initial_ratings, final_ratings) \n                                  if i == rating and j == 'D')\n        default_rate = defaults_from_rating / entities_with_rating\n    else:\n        default_rate = 0\n    default_rates.append(default_rate)\n\nplt.figure(figsize=(10, 6))\nplt.bar(ratings[:-1], [rate * 100 for rate in default_rates], color='darkred')\nplt.title('Default Rate by Initial Credit Rating')\nplt.xlabel('Initial Credit Rating')\nplt.ylabel('Default Rate (%)')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/credit/transition_matrix/#example-output","title":"Example Output","text":"<pre><code>Credit Rating Transition Matrix (Probabilities)\n==============================================\n     A     AA    AAA   B     BB    BBB   CCC   D     \nA    0.52  0.00  0.00  0.00  0.00  0.48  0.00  0.00  \nAA   0.23  0.68  0.09  0.00  0.00  0.00  0.00  0.00  \nAAA  0.00  0.33  0.67  0.00  0.00  0.00  0.00  0.00  \nB    0.00  0.00  0.00  0.50  0.00  0.00  0.00  0.50  \nBB   0.00  0.00  0.00  0.53  0.35  0.12  0.00  0.00  \nBBB  0.00  0.00  0.00  0.00  0.33  0.67  0.00  0.00  \nCCC  0.00  0.00  0.00  0.00  0.00  0.00  0.40  0.60  \nD    0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  \n\nTransition Statistics:\nTotal Entities: 100\nUpgrades: 4 (4.0%)\nDowngrades: 40 (40.0%)\nUnchanged: 56 (56.0%)\nDefaults: 7 (7.0%)\n</code></pre>"},{"location":"user-guide/credit/transition_matrix/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/credit/transition_matrix/#transition-matrix-heatmap","title":"Transition Matrix Heatmap","text":"<p>This visualization shows the probability of transitioning from one rating to another as a color-coded heatmap, with darker colors indicating higher probabilities.</p> <p></p>"},{"location":"user-guide/credit/transition_matrix/#rating-distribution","title":"Rating Distribution","text":"<p>This bar chart compares the distribution of ratings at the beginning and end of the period, showing how the overall credit quality of the portfolio has changed.</p> <p></p>"},{"location":"user-guide/credit/transition_matrix/#default-rates-by-initial-rating","title":"Default Rates by Initial Rating","text":"<p>This visualization shows the relationship between initial credit rating and default rate, illustrating the higher default probabilities associated with lower credit ratings.</p>"},{"location":"user-guide/credit/transition_matrix/#practical-applications","title":"Practical Applications","text":"<p>Credit rating transition matrices can be used for:</p> <ol> <li>Credit Portfolio Management: Modeling the evolution of portfolio credit quality over time</li> <li>Economic Capital Calculation: Estimating potential credit losses under various scenarios</li> <li>Pricing Credit-Sensitive Instruments: Determining appropriate spreads for bonds and loans</li> <li>Stress Testing: Analyzing the impact of economic downturns on credit quality</li> <li>Regulatory Compliance: Meeting requirements for internal ratings-based approaches under Basel frameworks</li> </ol>"},{"location":"user-guide/credit/transition_matrix/#methodological-considerations","title":"Methodological Considerations","text":"<p>When calculating transition matrices, several methodological issues should be considered:</p> <ol> <li>Time Horizon: Transition probabilities depend on the length of the observation period (e.g., 1-year vs. 5-year)</li> <li>Rating Withdrawals: How to handle entities whose ratings are withdrawn during the period</li> <li>Point-in-Time vs. Through-the-Cycle: Whether to use ratings that reflect current conditions or long-term averages</li> <li>Cohort vs. Hazard Rate Method: Different approaches to calculating transition probabilities</li> <li>Economic Conditions: Transition probabilities vary across different phases of the economic cycle</li> </ol>"},{"location":"user-guide/credit/transition_matrix/#limitations","title":"Limitations","text":"<p>Credit rating transition matrices have several limitations:</p> <ol> <li>Rating Stability Bias: Rating agencies may be slow to change ratings, leading to underestimation of transition probabilities</li> <li>Limited History: For newer rating categories or markets, historical data may be insufficient</li> <li>Non-Markovian Behavior: Future rating changes may depend on rating history, not just current rating</li> <li>Heterogeneity Within Ratings: Entities with the same rating may have different default probabilities</li> <li>Time Variation: Transition probabilities change over time with economic conditions </li> </ol>"},{"location":"user-guide/credit/weight_of_evidence/","title":"Weight of Evidence (WOE) and Information Value (IV)","text":"<p>The <code>weight_of_evidence</code> function calculates Weight of Evidence (WOE) and Information Value (IV), which are powerful techniques used in credit scoring and risk modeling to assess the predictive power of variables and transform them into a format suitable for logistic regression models.</p>"},{"location":"user-guide/credit/weight_of_evidence/#usage-in-pypulate","title":"Usage in Pypulate","text":"<pre><code>from pypulate.credit import weight_of_evidence\n\n# Calculate WOE and IV for a variable's bins\nresult = weight_of_evidence(\n    good_count=[100, 80, 50, 30, 10],  # Count of non-default cases in each bin\n    bad_count=[5, 10, 15, 20, 25]      # Count of default cases in each bin\n)\n\n# Access the results\nwoe_values = result[\"woe\"]\niv = result[\"information_value\"]\niv_strength = result[\"iv_strength\"]\n</code></pre>"},{"location":"user-guide/credit/weight_of_evidence/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>good_count</code> array_like Count of good cases (non-defaults) in each bin Required <code>bad_count</code> array_like Count of bad cases (defaults) in each bin Required <code>min_samples</code> float Minimum percentage of samples required in a bin 0.01 <code>adjustment</code> float Adjustment factor for zero counts 0.5"},{"location":"user-guide/credit/weight_of_evidence/#return-value","title":"Return Value","text":"<p>The function returns a dictionary with the following keys:</p> Key Type Description <code>woe</code> list Weight of Evidence values for each bin <code>information_value</code> float Information Value, measuring the overall predictive power <code>iv_strength</code> str Qualitative assessment of the IV strength <code>good_distribution</code> list Distribution of good cases across bins <code>bad_distribution</code> list Distribution of bad cases across bins <code>small_bins</code> list Boolean flags indicating bins with too few samples"},{"location":"user-guide/credit/weight_of_evidence/#risk-level-classification","title":"Risk Level Classification","text":"<p>Information Value (IV) is typically categorized into predictive power levels:</p> IV Range Predictive Power &lt; 0.02 Not predictive 0.02 - 0.1 Weak predictive power 0.1 - 0.3 Medium predictive power 0.3 - 0.5 Strong predictive power &gt; 0.5 Very strong predictive power"},{"location":"user-guide/credit/weight_of_evidence/#comprehensive-example","title":"Comprehensive Example","text":"<p>Here's a complete example demonstrating how to calculate and analyze Weight of Evidence and Information Value:</p> <pre><code>from pypulate.credit import weight_of_evidence\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data: Age bins and default counts\nage_bins = [\"18-25\", \"26-35\", \"36-45\", \"46-55\", \"56-65\", \"65+\"]\nnon_defaults = [150, 450, 600, 500, 300, 200]  # Good cases\ndefaults = [50, 70, 60, 40, 30, 50]            # Bad cases\n\n# Calculate WOE and IV\nresult = weight_of_evidence(non_defaults, defaults)\n\n# Print the results\nprint(\"Weight of Evidence Analysis\")\nprint(\"==========================\")\nprint(f\"Information Value: {result['information_value']:.4f}\")\nprint(f\"Predictive Power: {result['iv_strength']}\")\nprint(\"\\nBin Analysis:\")\nprint(f\"{'Age Bin':&lt;10} {'Non-Defaults':&lt;15} {'Defaults':&lt;15} {'WOE':&lt;10} {'Small Bin':&lt;10}\")\nprint(\"-\" * 60)\n\nfor i, bin_name in enumerate(age_bins):\n    print(f\"{bin_name:&lt;10} {non_defaults[i]:&lt;15} {defaults[i]:&lt;15} {result['woe'][i]:&gt;+.4f} {'Yes' if result['small_bins'][i] else 'No':&lt;10}\")\n\n# Calculate default rates for comparison\ndefault_rates = [d/(d+n) for d, n in zip(defaults, non_defaults)]\n\n# Visualize WOE values\nplt.figure(figsize=(12, 8))\n\n# Create a subplot for WOE values\nplt.subplot(2, 1, 1)\nbars = plt.bar(age_bins, result['woe'], color='skyblue')\nplt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\nplt.title('Weight of Evidence by Age Group')\nplt.ylabel('WOE Value')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Add value labels on top of bars\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., \n             height + (0.1 if height &gt;= 0 else -0.1),\n             f'{height:.2f}', \n             ha='center', va='bottom' if height &gt;= 0 else 'top')\n\n# Create a subplot for default rates\nplt.subplot(2, 1, 2)\nplt.bar(age_bins, default_rates, color='salmon')\nplt.title('Default Rate by Age Group')\nplt.ylabel('Default Rate')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Add value labels on top of bars\nfor i, v in enumerate(default_rates):\n    plt.text(i, v + 0.01, f'{v:.2%}', ha='center')\n\nplt.tight_layout()\nplt.show()\n\n# Visualize distributions\nplt.figure(figsize=(12, 6))\n\nx = np.arange(len(age_bins))\nwidth = 0.35\n\nfig, ax = plt.subplots(figsize=(12, 6))\nax.bar(x - width/2, result['good_distribution'], width, label='Good Distribution')\nax.bar(x + width/2, result['bad_distribution'], width, label='Bad Distribution')\n\nax.set_xlabel('Age Group')\nax.set_ylabel('Distribution')\nax.set_title('Distribution of Good vs Bad Cases')\nax.set_xticks(x)\nax.set_xticklabels(age_bins)\nax.legend()\nax.grid(axis='y', linestyle='--', alpha=0.7)\n\nplt.tight_layout()\nplt.show()\n\n# Calculate and visualize the contribution to IV\niv_contributions = [(g - b) * w for g, b, w in zip(\n    result['good_distribution'], \n    result['bad_distribution'], \n    result['woe']\n)]\n\nplt.figure(figsize=(10, 6))\nplt.bar(age_bins, iv_contributions, color='green')\nplt.title('Contribution to Information Value by Age Group')\nplt.xlabel('Age Group')\nplt.ylabel('IV Contribution')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Add value labels\nfor i, v in enumerate(iv_contributions):\n    plt.text(i, v + 0.001, f'{v:.4f}', ha='center')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/credit/weight_of_evidence/#example-output","title":"Example Output","text":"<pre><code>Weight of Evidence Analysis\n==========================\nInformation Value: 0.2217\nPredictive Power: Medium predictive power\nBin Analysis:\nAge Bin    Non-Defaults    Defaults        WOE        Small Bin \n------------------------------------------------------------\n18-25      150             50              -0.8938 No        \n26-35      450             70              -0.1317 No        \n36-45      600             60              +0.3102 No        \n46-55      500             40              +0.5333 No        \n56-65      300             30              +0.3102 No        \n65+        200             50              -0.6061 No        \n</code></pre>"},{"location":"user-guide/credit/weight_of_evidence/#visualizations","title":"Visualizations","text":""},{"location":"user-guide/credit/weight_of_evidence/#weight-of-evidence-by-age-group","title":"Weight of Evidence by Age Group","text":"<p>This visualization shows the WOE values for each age bin. Positive values indicate lower risk (fewer defaults than expected), while negative values indicate higher risk (more defaults than expected).</p> <p></p>"},{"location":"user-guide/credit/weight_of_evidence/#default-rate-by-age-group","title":"Default Rate by Age Group","text":"<p>This chart shows the actual default rate for each age group, providing context for the WOE values.</p>"},{"location":"user-guide/credit/weight_of_evidence/#distribution-of-good-vs-bad-cases","title":"Distribution of Good vs Bad Cases","text":"<p>This bar chart compares the distribution of good cases (non-defaults) and bad cases (defaults) across the different age groups.</p>"},{"location":"user-guide/credit/weight_of_evidence/#contribution-to-information-value","title":"Contribution to Information Value","text":"<p>This visualization shows how much each bin contributes to the overall Information Value, helping identify which groups have the strongest discriminatory power.</p>"},{"location":"user-guide/credit/weight_of_evidence/#practical-applications","title":"Practical Applications","text":"<p>Weight of Evidence and Information Value are used for:</p> <ol> <li>Variable Selection: Identifying which variables have the strongest predictive power for credit scoring models</li> <li>Risk Segmentation: Creating meaningful risk segments based on variable characteristics</li> <li>Logistic Regression Preparation: Transforming categorical and continuous variables into WOE values for logistic regression</li> <li>Scorecard Development: Creating credit scorecards with optimal binning and variable transformation</li> <li>Model Validation: Assessing the predictive power of variables in existing models</li> </ol>"},{"location":"user-guide/credit/weight_of_evidence/#methodological-considerations","title":"Methodological Considerations","text":"<p>When calculating WOE and IV, several methodological issues should be considered:</p> <ol> <li>Binning Strategy: How to create optimal bins for continuous variables (equal width, equal frequency, or custom)</li> <li>Handling Zero Counts: Applying adjustments to avoid infinity values when a bin has zero good or bad cases</li> <li>Minimum Sample Size: Ensuring each bin has sufficient samples for reliable WOE calculation</li> <li>Monotonicity: Whether WOE values should be monotonic across ordered bins</li> <li>Outlier Treatment: How to handle extreme values that might distort WOE calculations</li> </ol>"},{"location":"user-guide/credit/weight_of_evidence/#limitations","title":"Limitations","text":"<p>Weight of Evidence and Information Value have several limitations:</p> <ol> <li>Univariate Analysis: They assess variables individually, not accounting for interactions</li> <li>Binning Dependency: Results can vary significantly based on the binning strategy</li> <li>Overfitting Risk: Excessive binning can lead to overfitting</li> <li>Interpretability Tradeoff: While WOE transformation improves model performance, it can reduce interpretability</li> <li>Population Stability: WOE values calculated on one population may not be valid for another </li> </ol>"}]}